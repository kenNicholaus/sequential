{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDeveloped by Sumbit, Inc for AI Trade Signals project. Please refrain from sharing the data/ideas/code used here with others.\\nPlease be careful especially with academics. They have used our ideas to publish papers and books in the past.\\nThis code and data are given to you for your learning only. It belongs to Sumbit Inc., Feel free to use it to enhance your skills in machine learning.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Developed by Sumbit, Inc for AI Trade Signals project. Please refrain from sharing the data/ideas/code used here with others.\n",
    "Please be careful especially with academics. They have used our ideas to publish papers and books in the past.\n",
    "This code and data are given to you for your learning only. It belongs to Sumbit Inc., Feel free to use it to enhance your skills in machine learning.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIf you are running locally then \\n1. reboot your local machine\\n2. create an environment called 'colab' using anaconda prompt\\nif you have a gpu\\nconda create -n colab python tensorflow-gpu \\nif not \\nconda create -n colab python tensorflow\\n3. to install jupyter notebook\\nconda install jupyter notebook\\n4. to go to the 'colab' environment\\nactivate colab\\n5. change file path to locate this notebook and then type 'jupyter notebook'\\n\\nIf you use colab\\n1. save the data file in your google drive\\n2. goto colab and start running the code\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you are running locally then \n",
    "1. reboot your local machine\n",
    "2. create an environment called 'colab' using anaconda prompt\n",
    "if you have a gpu\n",
    "conda create -n colab python tensorflow-gpu \n",
    "if not \n",
    "conda create -n colab python tensorflow\n",
    "3. to install jupyter notebook\n",
    "conda install jupyter notebook\n",
    "4. to go to the 'colab' environment\n",
    "activate colab\n",
    "5. change file path to locate this notebook and then type 'jupyter notebook'\n",
    "\n",
    "If you use colab\n",
    "1. save the data file in your google drive\n",
    "2. goto colab and start running the code\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install prominent libraries with specific versions. Change versions to the output of block 4\n",
    "#!pip install tensorflow==1.15.0\n",
    "#!pip install keras==2.2.4-tf\n",
    "#!pip install pandas== 0.25.1\n",
    "#!pip install sklearn==0.23.1\n",
    "#!pip install matplotlib==3.2.1\n",
    "#!pip install hyperas\n",
    "#!pip install hyperopt\n",
    "#!pip install yfinance\n",
    "#!pip install pandas_market_calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "sJnnN6xG_yaM",
    "outputId": "b6420ce8-7f05-440c-e2ce-479b5bc1cafc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function                                                                                                                                                                                                                                                                                                                              # from tensorflow.contrib.rnn import *import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, CuDNNGRU, Input, Activation, Flatten, BatchNormalization, Reshape,Concatenate,Bidirectional, TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, History\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import regularizers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import*\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adadelta\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, History, ReduceLROnPlateau \n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from collections import Counter\n",
    "import operator\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import hyperas\n",
    "import hyperopt\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperas import optim\n",
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from keras.layers import Conv1D, MaxPooling1D, ZeroPadding1D\n",
    "import pandas_market_calendars as mcal \n",
    "import datetime \n",
    "from datetime import timedelta  \n",
    "import yfinance as yf\n",
    "import glob \n",
    "import pickle\n",
    "import gc\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow.__version__ =  1.15.0\n",
      "sklearn.__version__ =  0.23.1\n",
      "numpy.__version__ =  1.19.1\n",
      "pandas.__version__ =  1.1.0\n",
      "matplotlib.__version__ =  3.2.2\n",
      "Last run date :  2020-08-14\n"
     ]
    }
   ],
   "source": [
    "#Get library versions\n",
    "print(\"tensorflow.__version__ = \", tf.__version__)\n",
    "#import tensorflow.python.keras\n",
    "#print(\"keras.__version__ = \", tensorflow.python.keras.__version__)\n",
    "import sklearn \n",
    "print(\"sklearn.__version__ = \", sklearn.__version__)\n",
    "print(\"numpy.__version__ = \", np.__version__)\n",
    "print(\"pandas.__version__ = \", pd.__version__)\n",
    "import matplotlib\n",
    "print(\"matplotlib.__version__ = \", matplotlib.__version__)\n",
    "now = datetime.datetime.now()\n",
    "today = now.strftime(\"%Y-%m-%d\")\n",
    "print (\"Last run date : \", today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "*****************************************************\n",
    "please make sure the model_name is the same as the notebook name\n",
    "*****************************************************\n",
    "'''\n",
    "model_name = '3.2P_CuDNNLSTM' # For hyperas, it is the notebook name (make sure the notebook resides in google drive if using colab)\n",
    "max_evals = 20 # number of hyperas evaluations for finding hyperparameters. More choices will require to higher evaluations\n",
    "early_stopping_count= 500 # stop training run if there is no improvement in accuracy after the choosen epoch count\n",
    "training_epochs=2000 # Note: hyperas epochs are different from training epochs\n",
    "training_verbose = 2 #choice 0, 1, 2 or False\n",
    "checkpointer_verbose = 2 #choice 0, 1, 2 or False\n",
    "call_reduce_verbose = 1 #choice 0, 1, 2 or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change data parameters to improve accuracy\n",
    "def data_params():\n",
    "    \"\"\"\n",
    "    Function passes global data parameters to hyperas optim.minimize method.\n",
    "    Please make ensure __pycache__ in your folder and and temp_model.py files are deleted \n",
    "    manually if you are making changes to these parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    batch_size= 512  # batch_size for training and evaluation\n",
    "    time_steps = 28   # Number of time steps to create in the dataset\n",
    "    rm_window =100     # Number of rolling mean columns for choosen ticker price\n",
    "    hyperas_epochs = 20 # number of epochs for each trials during hypears evaluation\n",
    "    forecast_period = 7 # can be 1 to 7 in a 7day model dataset(given)\n",
    "    \n",
    "    '''\n",
    "    ***********************************************************************************\n",
    "    check to see if using 'RobustScaler','Normalizer', 'MinMaxScaler', 'MaxAbsScaler', \n",
    "    'PowerTransformer', 'QuantileTransformer' instead of 'StandardScaler' improves accuracy\n",
    "    ***********************************************************************************\n",
    "    '''\n",
    "    x_scaler=StandardScaler()# change the pre-processing method to improve accuracy\n",
    "    y_scaler=StandardScaler()\n",
    "    \n",
    "    return batch_size, time_steps, rm_window, hyperas_epochs, forecast_period, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "***************************************************************************************************\n",
    "Please choose 'True' or 'False' parameter for search, train and starting epoch one \n",
    "*****************************************************************************************************\n",
    "'''\n",
    "\n",
    "best_model_search = True # 'True' to search for best model, 'False' to use already saved existing best model\n",
    "perform_training = True# 'True' to  train, 'False' to not to train after best_model search\n",
    "starting_epoch_one = True # 'True' to train from scratch(epoch 1) , 'False' to load the last checkpoint weights of previous training run\n",
    "evaluate_all_folds = True # evaluate the trained model for all the folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split():  \n",
    "    kfold = False # 'True' to implement kfold, 'False' to use train_test_split\n",
    "    train_size = 0.85  # use train_test_split between (0.7 to 0.9),make sure test is atleast of one batch_size\n",
    "    shuffle = True # 'True' to shuffle\n",
    "    split = 10 # number of splits in kfold\n",
    "    fold_number = 8 # (the choosen fold_number of the split becomes test data, rest becomes train data)\n",
    "    random_state= 54 # for random seed\n",
    "    return kfold, train_size, shuffle, split, fold_number,random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model exists but you can still search for a better model\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "you have choosen to search for best model and then train\n",
      "check point exists\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('best_model/'+model_name+'.h5'):\n",
    "    print('Best model exists but you can still search for a better model')\n",
    "    best_model_input_shape = load_model('best_model/'+model_name+'.h5').input_shape\n",
    "        \n",
    "else:\n",
    "    print('Best model doesnt exist, changing best_model variable to True')\n",
    "    best_model_search= True\n",
    "    \n",
    "\n",
    "if best_model_search and perform_training:\n",
    "    print('you have choosen to search for best model and then train')\n",
    "elif best_model_search and not perform_training:\n",
    "    print('you have choosen to search for best model only')\n",
    "elif not best_model_search and perform_training:\n",
    "    print('you have choosen to train using already saved best model')\n",
    "    print('For training use best model batch_size:', best_model_input_shape[0])\n",
    "    print('For training use best model time steps:', best_model_input_shape[1])\n",
    "    print('For training use best model features:', best_model_input_shape[2])\n",
    "elif not best_model_search and not perform_training:\n",
    "     print('you have choosen to not to search or train. Therefore the model will not run')  \n",
    "\n",
    "if os.path.exists('train_model/checkpoint/'+model_name+'.ckpt'):\n",
    "    print('check point exists')\n",
    "    if os.path.exists('train_metrics/'+model_name+'.csv'):\n",
    "        train_metrics = pd.read_csv('train_metrics/'+model_name+'.csv', index_col=0)\n",
    "        print('\\n','Results of last run checkpoint:')\n",
    "        print(train_metrics.tail(1))       \n",
    "else:\n",
    "    print(\"check point doesn't exist, starting training from the epoch 1\")\n",
    "    starting_epoch_one = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random seed to generate reproduceable results\n",
    "_, _, _, _, _, random_state = data_split()\n",
    "from numpy.random import seed\n",
    "seed(random_state)\n",
    "try:\n",
    "    tf.compat.v1.set_random_seed(random_state)\n",
    "except:\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(random_state)\n",
    "random.seed(random_state)\n",
    "os.environ['PYTHONHASHSEED']=str(1)\n",
    "os.environ['TF_KERAS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they do not exist\n",
    "def build_path(dirName):\n",
    "    try:\n",
    "        os.makedirs(dirName)    \n",
    "        print(\"Directory \" , dirName ,  \" created \")\n",
    "    except:\n",
    "        print(\"Directory \" , dirName ,  \" already exists\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6158193534031571540\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7125503181\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8646262434642142101\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# to check if GPU is getting used locally.....you need to see CPU as well as GPU in the output\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection():\n",
    "    '''\n",
    "    Function reads the dataset and prepares inputs and labels\n",
    "    '''\n",
    "    ticker = '^GSPC'  # ticker to classify as buy or sell\n",
    "    #features\n",
    "    stocks = ['A', 'AAL', 'AAP', 'AAPL', 'ABBV', 'ABC', 'ABMD', 'ABT', 'ACN', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADS', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AGN', 'AIG', 'AIV', 'AIZ', 'AJG', 'AKAM', 'ALB', 'ALGN', 'ALK', 'ALL', 'ALLE', 'ALXN', 'AMAT', 'AMCR', 'AMD', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN', 'ANET', 'ANSS', 'ANTM', 'AON', 'AOS', 'APA', 'APD', 'APH', 'APTV', 'ARE', 'ARNC', 'ATO', 'ATVI', 'AVB', 'AVGO', 'AVY', 'AWK', 'AXP', 'AZO', 'BA', 'BAC', 'BAX', 'BBY', 'BDX', 'BEN', 'BF-B', 'BIIB', 'BK', 'BKNG', 'BKR', 'BLK', 'BLL', 'BMY', 'BR', 'BRK-B', 'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT', 'CB', 'CBOE', 'CBRE', 'CCI', 'CCL', 'CDNS', 'CDW', 'CE', 'CERN', 'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX', 'CMA', 'CMCSA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF', 'COG', 'COO', 'COP', 'COST', 'COTY', 'CPB', 'CPRI', 'CPRT', 'CRM', 'CSCO', 'CSX', 'CTAS', 'CTL', 'CTSH', 'CTVA', 'CTXS', 'CVS', 'CVX', 'CXO', 'D', 'DAL', 'DD', 'DE', 'DFS', 'DG', 'DGX', 'DHI', 'DHR', 'DIA', 'DIS', 'DISCA', 'DISCK', 'DISH', 'DLR', 'DLTR', 'DOV', 'DOW', 'DRE', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DX-Y.NYB', 'DXC', 'EA', 'EBAY', 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'EOG', 'EQIX', 'EQR', 'ES', 'ESS', 'ETFC', 'ETN', 'ETR', 'EVRG', 'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG', 'FAST', 'FB', 'FBHS', 'FCX', 'FDX', 'FE', 'FFIV', 'FIS', 'FISV', 'FITB', 'FLIR', 'FLS', 'FLT', 'FMC', 'FOX', 'FOXA', 'FRC', 'FRT', 'FTI', 'FTNT', 'FTV', 'GD', 'GE', 'GILD', 'GIS', 'GL', 'GLW', 'GM', 'GOOG', 'GPC', 'GPN', 'GPS', 'GRMN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HBI', 'HCA', 'HD', 'HES', 'HFC', 'HIG', 'HII', 'HLT', 'HOG', 'HOLX', 'HON', 'HP', 'HPE', 'HPQ', 'HRB', 'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IDXX', 'IEX', 'IFF', 'ILMN', 'INCY', 'INFO', 'INTC', 'INTU', 'IP', 'IPG', 'IPGP', 'IQV', 'IR', 'IRM', 'ISRG', 'IT', 'ITW', 'IVZ', 'J', 'JBHT', 'JCI', 'JKHY', 'JNJ', 'JNPR', 'JPM', 'JWN', 'K', 'KEY', 'KEYS', 'KHC', 'KIM', 'KLAC', 'KMB', 'KMI', 'KMX', 'KO', 'KR', 'KSS', 'KSU', 'L', 'LB', 'LDOS', 'LEG', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW', 'LRCX', 'LUV', 'LVS', 'LW', 'LYB', 'LYV', 'M', 'MA', 'MAA', 'MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDLZ', 'MDT', 'MET', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS', 'MPC', 'MRK', 'MRO', 'MS', 'MSCI', 'MSFT', 'MSI', 'MTB', 'MTD', 'MU', 'MXIM', 'MYL', 'NBL', 'NCLH', 'NDAQ', 'NEE', 'NEM', 'NFLX', 'NI', 'NKE', 'NLOK', 'NLSN', 'NOC', 'NOV', 'NOW', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVDA', 'NVR', 'NWL', 'NWS', 'NWSA', 'O', 'ODFL', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT', 'PCAR', 'PEAK', 'PEG', 'PEP', 'PFE', 'PFG', 'PG', 'PGR', 'PH', 'PHM', 'PKG', 'PKI', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'PXD', 'PYPL', 'QCOM', 'QRVO', 'RCL', 'RE', 'REG', 'REGN', 'RF', 'RHI', 'RJF', 'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTN', 'SBAC', 'SBUX', 'SCHW', 'SEE', 'SHW', 'SIVB', 'SJM', 'SLB', 'SLG', 'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SPY', 'SRE', 'STE', 'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYF', 'SYK', 'SYY', 'T', 'TAP', 'TDG', 'TEL', 'TFC', 'TFX', 'TGT', 'TIF', 'TJX', 'TMO', 'TMUS', 'TPR', 'TROW', 'TRV', 'TSCO', 'TSN', 'TTWO', 'TWTR', 'TXN', 'TXT', 'UA', 'UAA', 'UAL', 'UDR', 'UHS', 'ULTA', 'UNH', 'UNM', 'UNP', 'UPS', 'URI', 'USB', 'UTX', 'V', 'VAR', 'VFC', 'VIAC', 'VLO', 'VMC', 'VNO', 'VRSK', 'VRSN', 'VRTX', 'VTR', 'VZ', 'WAB', 'WAT', 'WBA', 'WCG', 'WDC', 'WEC', 'WELL', 'WFC', 'WHR', 'WLTW', 'WM', 'WMB', 'WMT', 'WRB', 'WRK', 'WU', 'WY', 'WYNN', 'XEC', 'XEL', 'XLB', 'XLE', 'XLF', 'XLI', 'XLK', 'XLNX', 'XLP', 'XLU', 'XLV', 'XLY', 'XOM', 'XRAY', 'XRT', 'XRX', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZION', 'ZTS', '^GSPC', '^IXIC', '^NDX', '^SOX', '^TNX', '^VIX', '^VVIX', '^VXN', '^VXO', '^VXV']\n",
    "    short_interest = ['Trading Volume']\n",
    "    nytimes = ['neg', 'neu', 'pos']\n",
    "    economic = ['Dividend', 'Earnings', 'CPI', 'Long Interest Rate', 'Real Price', 'Real Dividend', 'Real Earnings', 'Cyclically Adjusted PE Ratio', 'ECRI USLEADING LEVEL', 'ECRI USLEADING GROWTH', 'ECRI USLAGGING LEVEL', 'ECRI USLAGGING GROWTH', 'ECRI_USCOIN LEVEL', 'ECRI_USCOIN GROWTH', 'University of Michigan Consumer Survey, Index of Consumer Sentiment', 'Bullish', 'Neutral', 'Bearish', 'Bullish 8-Week Mov Avg', 'Bull-Bear Spread', 'Bullish Average', 'Bullish Average + St. Dev', 'Bullish Average - St. Dev', 'Stock Market Confidence Indices - United States One Year Index Data - Individual', 'Stock Market Confidence Indices - United States One Year Index Data - Institutional', 'Stock Market Confidence Indices - United States Buy on Dips Confidence Index Data - Individual', 'Stock Market Confidence Indices - United States Buy on Dips Confidence Index Data - Institutional', 'Stock Market Confidence Indices - United States Crash Confidence Index Data - Individual', 'Stock Market Confidence Indices - United States Crash Confidence Index Data - Institutional', 'Stock Market Confidence Indices - United States Valuation Index Data - Individual', 'Stock Market Confidence Indices - United States Valuation Index Data - Institutional', 'Zillow Home Value Index (Metro): Zillow Rental Index - All Homes - United States', 'Zillow Home Value Index (Metro): Turnover - All Homes - United States', 'Zillow Home Value Index (Metro): Price To Rent Ratio - All Homes - United States', 'Zillow Home Value Index (Metro): Median Listing Price - All Homes - United States', 'Zillow Home Value Index (Metro):  Home Sold As Foreclosure - Ratio - All Homes - United States', 'Historical Housing Market Data - Real Home Price Index', 'Cost Index', 'U.S. Population (Millions)', 'Long Rate', 'Historical Housing Market Data - Nominal Home Price Index', 'Historical Housing Market Data - Consumer Price Index (Quarterly)', '30-Year Conventional Mortgage Rate', 'House Price Index - US National', '15-Year Fix Rate', '15-Year Fix Rate Fees & Points', '5-Year Adjustable Rate', '5-Year Adjustable Rate Fees & Points', '1-Year Adjustable Rate', '1-Year Adjustable Rate Fees & Points', '30-Year Fixed Rate Mortgage Average in the United States', 'Federal Funds Rate', '90 day T Bill Rate', 'One Year Maturity Treasury Yield', 'Ten Year Maturity Treasury Yield', 'Freddie Mac Commitment Fixed Rate Mortgages', 'Freddie Mac Commitment ARMs', 'Prime Rate', 'Total Housing Starts', 'Single Family Housing Starts', 'Multi-Family Housing Starts', 'New Single-Family Sales', 'Existing Single-Family Home Sales', '1-Year CD: National Rate of Banks', '5-Year CD: National Rate of Banks', 'Interest Checking Accounts: National Rate of Banks', 'Money Market Accounts: National Rate of Banks', '1-Month AA Financial Commercial Paper Rate', '3-Month AA Financial Commercial Paper Rate', '1-Month AA Nonfinancial Commercial Paper Rate', '3-Month AA Nonfinancial Commercial Paper Rate', 'Effective Federal Funds Rate', '10-Year Treasury Inflation-Indexed Security, Constant Maturity', '20-Year Treasury Inflation-Indexed Security, Constant Maturity', '30-Year Treasury Inflation-Indexed Security, Constant Maturity', '5-Year Treasury Inflation-Indexed Security, Constant Maturity', '7-Year Treasury Inflation-Indexed Security, Constant Maturity', '10-Year Treasury Constant Maturity Rate', '2-Year Treasury Constant Maturity Rate', '20-Year Treasury Constant Maturity Rate', '3-Year Treasury Constant Maturity Rate', '30-Year Treasury Constant Maturity Rate', '5-Year Treasury Constant Maturity Rate', '7-Year Treasury Constant Maturity Rate', 'Bank Prime Loan Rate', '1-Year Treasury Bill: Secondary Market Rate', '3-Month Treasury Bill: Secondary Market Rate', '4-Week Treasury Bill: Secondary Market Rate', '6-Month Treasury Bill: Secondary Market Rate', 'Overnight London Interbank Offered Rate (LIBOR), based on U.S. Dollar', 'Finance Rate on Personal Loans at Commercial Banks, 24 Month Loan', 'Commercial Bank Interest Rate on Credit Card Plans, All Accounts', 'RR3_TBILL_PGDP_2', 'RR3_TBILL_PGDP_3', 'RR3_TBILL_PGDP_4', 'RR3_TBILL_PGDP_5', 'RR2_TBILL_CCPI_2', 'RR2_TBILL_CCPI_3', 'RR2_TBILL_CCPI_4', 'RR2_TBILL_CCPI_5', 'Gross domestic product', 'Personal consumption expenditures', ':Goods', '::Durable goods', '::Nondurable goods', ':Services', 'Gross private domestic investment', 'Net exports of goods and services', 'Government consumption expenditures and gross investment', '::Gross domestic income (GDI)', '::Gross national product', '::Gross national income', '::Disposable personal income', 'Personal income', ':Compensation of employees', '::Wages and salaries', ':Population (midperiod; thousands)', '::Disposable personal income; current dollars', 'Services', ':Household consumption expenditures (for services)', '::Housing and utilities', '::Health care', '::Transportation services', '::Recreation services', '::Food services and accommodations', '::Financial services and insurance', '::Other services', 'Corporate profits with inventory valuation and capital consumption adjustments', 'Consumer Price Index - USA', 'United States Gross National Savings, % of GDP', 'United States GDP at Constant Prices, % change', 'Nominal Potential Gross Domestic Product', 'Industrial Production: Durable Goods: Other durable goods', 'FRED: Projection of General government net lending/borrowing for United States', 'FRED: Real Potential Gross Domestic Product', 'FRED: Real Gross Domestic Product', 'FRED: Gross Domestic Product', 'FRBP: GDPplus GDP Growth', 'Inflation YOY - USA', 'Consumer Price Index for All Urban Consumers: All Items', 'PRCCPI1', 'PRCCPI2', 'PRCCPI3', 'PRCCPI4', 'PRCCPI5', 'PRCCPI6', 'PRCCPI7', 'PRCCPI8', 'PRCCPI9', 'PRCCPI10', 'PRCCPI11', 'PRCCPI12', 'PRCCPI13', 'PRCCPI14', 'PRCCPI15', 'PRCCPI16', 'PRCCPI17', 'PRCCPI18', 'PRCCPI19', 'PRCCPI20', 'Mean CPI1', 'Mean CPI2', 'Mean CPI3', 'Mean CPI4', 'Mean CPI5', 'Mean CPI6', 'Mean CPIA', 'Mean CPIB', 'Mean CPIC', 'Median CPI1', 'Median CPI2', 'Median CPI3', 'Median CPI4', 'Median CPI5', 'Median CPI6', 'Median CPIA', 'Median CPIB', 'Median CPIC', 'CORECPI1', 'CORECPI2', 'CORECPI3', 'CORECPI4', 'CORECPI5', 'CORECPI6', 'CORECPIA', 'CORECPIB', 'CORECPIC', 'Median CORECPI1', 'Median CORECPI2', 'Median CORECPI3', 'Median CORECPI4', 'Median CORECPI5', 'Median CORECPI6', 'Median CORECPIA', 'Median CORECPIB', 'Median CORECPIC', 'Employment; Openings; All industries; Rate (Percent) - National', 'Employment; Gross Job Losses; All industries; Rate (Percent) - National', 'Employment - All employees, quarterly averages, seasonally adjusted, thousands; Total nonfarm industry', 'Employment - All employees, thousands; Total private industry', 'Employment - All employees, thousands; Total nonfarm industry', 'Hires for Total nonfarm in Total US Region', 'Layoffs and discharges for Total nonfarm in Total US Region', 'Total separations for Total nonfarm in Total US Region', 'Hires percent for Total private in Total US Region', 'Job openings for Total private in Total US Region', 'Job openings percent for Total private in Total US Region', 'Total separations for Total private in Total US Region', 'Hires for Construction in Total US Region', 'Total separations for Construction in Total US Region', 'Hires for Manufacturing in Total US Region', 'Hires for Government in Total US Region', 'Total separations for Government in Total US Region', 'US Forecast Unemployment Rate, % of Total Labor Force', 'ISM Manufacturing Employment Index', 'ISM Non-Manufacturing Employment Index', 'FRED Forecast Natural Rate of Unemployment (Short-Term)', 'FRED Forecast Natural Rate of Unemployment (Long-Term)', 'ADP_Total private', 'ADP_Goods producing', 'Service providing', 'ADP_Goods Producing Industries_1-19', 'ADP_Goods Producing Industries_20-49', 'ADP_Goods Producing Industries_1-49', 'ADP_Goods Producing Industries_50-499', 'ADP_Goods Producing Industries_500+', 'ADP_Goods Producing Industries_500-999', 'ADP_Goods Producing Industries_1,000+', 'ADP_Construction', 'ADP_Natural resources & mining', 'Manufacturing', 'ADP_Prof.l/business services', 'ADP_Professional services', 'ADP_Total Non-Farm Private_1-19', 'ADP_Total Non-Farm Private_20-49', 'ADP_Total Non-Farm Private_1-49', 'ADP_Total Non-Farm Private_50-499', 'ADP_Total Non-Farm Private_500+', 'ADP_Total Non-Farm Private_500-999', '1000+', 'ADP_Private Service_1-19', 'ADP_Private Service_20-49', 'ADP_Private Service_1-49', 'ADP_Private Service_50-499', 'ADP_Private Service_500+', 'ADP_Private Service_500-999', 'ADP_Private Service_1,000+', 'Hand to Mouth-Manufacturing Buying Policy: Capital Expenditures', 'Average Days-Manufacturing Buying Policy: Capital Expenditures', 'Hand to Mouth-Manufacturing Buying Policy: MRO Supplies', 'Average Days-Manufacturing Buying Policy: MRO Supplies', 'Manufacturing Backlog of Orders Index', \"Manufacturing Customers' Inventories Index\", 'Manufacturing Supplier Deliveries Index', 'Manufacturing New Export Orders Index', 'Manufacturing Imports Index', 'Manufacturing Inventories Index', 'Manufacturing New Orders Index', 'Manufacturing Prices Index', 'Manufacturing Production Index', 'Non-Manufacturing Backlog of Orders Index', 'Non-Manufacturing Business Activity Index', 'Non-Manufacturing Supplier Deliveries Index', 'Non-Manufacturing New Export Orders Index', 'Non-Manufacturing Imports Index', 'Non-Manufacturing Inventories Index', 'Non-Manufacturing Inventory Sentiment Index', 'Non-Manufacturing New Orders Index', 'NMI Non-Manufacturing Index', 'Non-Manufacturing Prices Index', 'Gold Prices (Daily) - Currency USD', 'Milk, Nonfat dry, Chicago', 'Crude Oil Futures, Continuous Contract #1 (CL1) (Front Month)', 'UK Natural Gas Futures, Continuous Contract', 'Natural Gas (Henry Hub) Physical Futures, Continuous Contract #1 (NG1) (Front Month)', 'STEO price forecast, Monthly', 'US All Grades Conventional Gas Price', 'Copper Prices', 'Aluminum Prices', 'Silver Price: London Fixing', 'Seasonal Factors for the Empire State Manufacturing Survey - Future General Business Conditions - Percent Reporting Increase', 'Empire State Manufacturing Index - Forecasted Inventories - Diffusion Index (Seasonally Adjusted)', 'Seasonal Factors for the Empire State Manufacturing Survey - Current New Orders - Percent Reporting Decrease', 'Seasonal Factors for the Empire State Manufacturing Survey - Current New Orders - Percent Reporting Increase', 'Empire State Manufacturing Index - Forecasted Shipments - Diffusion Index (Seasonally Adjusted)', 'Empire State Manufacturing Index - Forecasted Unfilled Orders - Diffusion Index (Seasonally Adjusted)', 'Philly Fed Seasonally Adjusted Diffusion Indexes - Future New Orders', 'Philly Fed Seasonally Adjusted Diffusion Indexes - Future Shipments', 'Philly Fed Seasonally Adjusted Diffusion Indexes - Future Unfilled Orders', 'Future New Orders;Percent Reporting Increases for FRB - Philadelphia District']\n",
    "    pattern = ['weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'year_1971', 'year_1972', 'year_1973', 'year_1974', 'year_1975', 'year_1976', 'year_1977', 'year_1978', 'year_1979', 'year_1980', 'year_1981', 'year_1982', 'year_1983', 'year_1984', 'year_1985', 'year_1986', 'year_1987', 'year_1988', 'year_1989', 'year_1990', 'year_1991', 'year_1992', 'year_1993', 'year_1994', 'year_1995', 'year_1996', 'year_1997', 'year_1998', 'year_1999', 'year_2000', 'year_2001', 'year_2002', 'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007', 'year_2008', 'year_2009', 'year_2010', 'year_2011', 'year_2012', 'year_2013', 'year_2014', 'year_2015', 'year_2016', 'year_2017', 'year_2018', 'year_2019', 'year_2020', 'quarter_2', 'quarter_3', 'quarter_4', 'days_of_month_2', 'days_of_month_3', 'days_of_month_4', 'days_of_month_5', 'days_of_month_6', 'days_of_month_7', 'days_of_month_8', 'days_of_month_9', 'days_of_month_10', 'days_of_month_11', 'days_of_month_12', 'days_of_month_13', 'days_of_month_14', 'days_of_month_15', 'days_of_month_16', 'days_of_month_17', 'days_of_month_18', 'days_of_month_19', 'days_of_month_20', 'days_of_month_21', 'days_of_month_22', 'days_of_month_23', 'days_of_month_24', 'days_of_month_25', 'days_of_month_26', 'days_of_month_27', 'days_of_month_28', 'days_of_month_29', 'days_of_month_30', 'days_of_month_31', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12', 'early_close']\n",
    "    derivatives = ['CALLS', 'PUTS', 'TOTAL_PC', 'P/C Ratio']\n",
    "    fama = ['ff_0.45', 'ff_-0.32', 'ff_-0.08', 'ff_0.009000000000000001', 'ff_-0.67', 'ff_0.0', 'ff_-0.32.1', 'ff_-0.01', 'ff_0.15', 'ff_0.012']\n",
    "    correlations = ['corr^GSPC_A', 'corr^GSPC_AAL', 'corr^GSPC_AAP', 'corr^GSPC_AAPL', 'corr^GSPC_ABBV', 'corr^GSPC_ABC', 'corr^GSPC_ABMD', 'corr^GSPC_ABT', 'corr^GSPC_ACN', 'corr^GSPC_ADBE', 'corr^GSPC_ADI', 'corr^GSPC_ADM', 'corr^GSPC_ADP', 'corr^GSPC_ADS', 'corr^GSPC_ADSK', 'corr^GSPC_AEE', 'corr^GSPC_AEP', 'corr^GSPC_AES', 'corr^GSPC_AFL', 'corr^GSPC_AGN', 'corr^GSPC_AIG', 'corr^GSPC_AIV', 'corr^GSPC_AIZ', 'corr^GSPC_AJG', 'corr^GSPC_AKAM', 'corr^GSPC_ALB', 'corr^GSPC_ALGN', 'corr^GSPC_ALK', 'corr^GSPC_ALL', 'corr^GSPC_ALLE', 'corr^GSPC_ALXN', 'corr^GSPC_AMAT', 'corr^GSPC_AMCR', 'corr^GSPC_AMD', 'corr^GSPC_AME', 'corr^GSPC_AMG', 'corr^GSPC_AMGN', 'corr^GSPC_AMP', 'corr^GSPC_AMT', 'corr^GSPC_AMZN', 'corr^GSPC_ANET', 'corr^GSPC_ANSS', 'corr^GSPC_ANTM', 'corr^GSPC_AON', 'corr^GSPC_AOS', 'corr^GSPC_APA', 'corr^GSPC_APD', 'corr^GSPC_APH', 'corr^GSPC_APTV', 'corr^GSPC_ARE', 'corr^GSPC_ARNC', 'corr^GSPC_ATO', 'corr^GSPC_ATVI', 'corr^GSPC_AVB', 'corr^GSPC_AVGO', 'corr^GSPC_AVY', 'corr^GSPC_AWK', 'corr^GSPC_AXP', 'corr^GSPC_AZO', 'corr^GSPC_BA', 'corr^GSPC_BAC', 'corr^GSPC_BAX', 'corr^GSPC_BBY', 'corr^GSPC_BDX', 'corr^GSPC_BEN', 'corr^GSPC_BF-B', 'corr^GSPC_BIIB', 'corr^GSPC_BK', 'corr^GSPC_BKNG', 'corr^GSPC_BKR', 'corr^GSPC_BLK', 'corr^GSPC_BLL', 'corr^GSPC_BMY', 'corr^GSPC_BR', 'corr^GSPC_BRK-B', 'corr^GSPC_BSX', 'corr^GSPC_BWA', 'corr^GSPC_BXP', 'corr^GSPC_C', 'corr^GSPC_CAG', 'corr^GSPC_CAH', 'corr^GSPC_CAT', 'corr^GSPC_CB', 'corr^GSPC_CBOE', 'corr^GSPC_CBRE', 'corr^GSPC_CCI', 'corr^GSPC_CCL', 'corr^GSPC_CDNS', 'corr^GSPC_CDW', 'corr^GSPC_CE', 'corr^GSPC_CERN', 'corr^GSPC_CF', 'corr^GSPC_CFG', 'corr^GSPC_CHD', 'corr^GSPC_CHRW', 'corr^GSPC_CHTR', 'corr^GSPC_CI', 'corr^GSPC_CINF', 'corr^GSPC_CL', 'corr^GSPC_CLX', 'corr^GSPC_CMA', 'corr^GSPC_CMCSA', 'corr^GSPC_CME', 'corr^GSPC_CMG', 'corr^GSPC_CMI', 'corr^GSPC_CMS', 'corr^GSPC_CNC', 'corr^GSPC_CNP', 'corr^GSPC_COF', 'corr^GSPC_COG', 'corr^GSPC_COO', 'corr^GSPC_COP', 'corr^GSPC_COST', 'corr^GSPC_COTY', 'corr^GSPC_CPB', 'corr^GSPC_CPRI', 'corr^GSPC_CPRT', 'corr^GSPC_CRM', 'corr^GSPC_CSCO', 'corr^GSPC_CSX', 'corr^GSPC_CTAS', 'corr^GSPC_CTL', 'corr^GSPC_CTSH', 'corr^GSPC_CTVA', 'corr^GSPC_CTXS', 'corr^GSPC_CVS', 'corr^GSPC_CVX', 'corr^GSPC_CXO', 'corr^GSPC_D', 'corr^GSPC_DAL', 'corr^GSPC_DD', 'corr^GSPC_DE', 'corr^GSPC_DFS', 'corr^GSPC_DG', 'corr^GSPC_DGX', 'corr^GSPC_DHI', 'corr^GSPC_DHR', 'corr^GSPC_DIA', 'corr^GSPC_DIS', 'corr^GSPC_DISCA', 'corr^GSPC_DISCK', 'corr^GSPC_DISH', 'corr^GSPC_DLR', 'corr^GSPC_DLTR', 'corr^GSPC_DOV', 'corr^GSPC_DOW', 'corr^GSPC_DRE', 'corr^GSPC_DRI', 'corr^GSPC_DTE', 'corr^GSPC_DUK', 'corr^GSPC_DVA', 'corr^GSPC_DVN', 'corr^GSPC_DX-Y.NYB', 'corr^GSPC_DXC', 'corr^GSPC_EA', 'corr^GSPC_EBAY', 'corr^GSPC_ECL', 'corr^GSPC_ED', 'corr^GSPC_EFX', 'corr^GSPC_EIX', 'corr^GSPC_EL', 'corr^GSPC_EMN', 'corr^GSPC_EMR', 'corr^GSPC_EOG', 'corr^GSPC_EQIX', 'corr^GSPC_EQR', 'corr^GSPC_ES', 'corr^GSPC_ESS', 'corr^GSPC_ETFC', 'corr^GSPC_ETN', 'corr^GSPC_ETR', 'corr^GSPC_EVRG', 'corr^GSPC_EW', 'corr^GSPC_EXC', 'corr^GSPC_EXPD', 'corr^GSPC_EXPE', 'corr^GSPC_EXR', 'corr^GSPC_F', 'corr^GSPC_FANG', 'corr^GSPC_FAST', 'corr^GSPC_FB', 'corr^GSPC_FBHS', 'corr^GSPC_FCX', 'corr^GSPC_FDX', 'corr^GSPC_FE', 'corr^GSPC_FFIV', 'corr^GSPC_FIS', 'corr^GSPC_FISV', 'corr^GSPC_FITB', 'corr^GSPC_FLIR', 'corr^GSPC_FLS', 'corr^GSPC_FLT', 'corr^GSPC_FMC', 'corr^GSPC_FOX', 'corr^GSPC_FOXA', 'corr^GSPC_FRC', 'corr^GSPC_FRT', 'corr^GSPC_FTI', 'corr^GSPC_FTNT', 'corr^GSPC_FTV', 'corr^GSPC_GD', 'corr^GSPC_GE', 'corr^GSPC_GILD', 'corr^GSPC_GIS', 'corr^GSPC_GL', 'corr^GSPC_GLW', 'corr^GSPC_GM', 'corr^GSPC_GOOG', 'corr^GSPC_GPC', 'corr^GSPC_GPN', 'corr^GSPC_GPS', 'corr^GSPC_GRMN', 'corr^GSPC_GS', 'corr^GSPC_GWW', 'corr^GSPC_HAL', 'corr^GSPC_HAS', 'corr^GSPC_HBAN', 'corr^GSPC_HBI', 'corr^GSPC_HCA', 'corr^GSPC_HD', 'corr^GSPC_HES', 'corr^GSPC_HFC', 'corr^GSPC_HIG', 'corr^GSPC_HII', 'corr^GSPC_HLT', 'corr^GSPC_HOG', 'corr^GSPC_HOLX', 'corr^GSPC_HON', 'corr^GSPC_HP', 'corr^GSPC_HPE', 'corr^GSPC_HPQ', 'corr^GSPC_HRB', 'corr^GSPC_HRL', 'corr^GSPC_HSIC', 'corr^GSPC_HST', 'corr^GSPC_HSY', 'corr^GSPC_HUM', 'corr^GSPC_IBM', 'corr^GSPC_ICE', 'corr^GSPC_IDXX', 'corr^GSPC_IEX', 'corr^GSPC_IFF', 'corr^GSPC_ILMN', 'corr^GSPC_INCY', 'corr^GSPC_INFO', 'corr^GSPC_INTC', 'corr^GSPC_INTU', 'corr^GSPC_IP', 'corr^GSPC_IPG', 'corr^GSPC_IPGP', 'corr^GSPC_IQV', 'corr^GSPC_IR', 'corr^GSPC_IRM', 'corr^GSPC_ISRG', 'corr^GSPC_IT', 'corr^GSPC_ITW', 'corr^GSPC_IVZ', 'corr^GSPC_JBHT', 'corr^GSPC_JCI', 'corr^GSPC_JEC', 'corr^GSPC_JKHY', 'corr^GSPC_JNJ', 'corr^GSPC_JNPR', 'corr^GSPC_JPM', 'corr^GSPC_JWN', 'corr^GSPC_K', 'corr^GSPC_KEY', 'corr^GSPC_KEYS', 'corr^GSPC_KHC', 'corr^GSPC_KIM', 'corr^GSPC_KLAC', 'corr^GSPC_KMB', 'corr^GSPC_KMI', 'corr^GSPC_KMX', 'corr^GSPC_KO', 'corr^GSPC_KR', 'corr^GSPC_KSS', 'corr^GSPC_KSU', 'corr^GSPC_L', 'corr^GSPC_LB', 'corr^GSPC_LDOS', 'corr^GSPC_LEG', 'corr^GSPC_LEN', 'corr^GSPC_LH', 'corr^GSPC_LHX', 'corr^GSPC_LIN', 'corr^GSPC_LKQ', 'corr^GSPC_LLY', 'corr^GSPC_LMT', 'corr^GSPC_LNC', 'corr^GSPC_LNT', 'corr^GSPC_LOW', 'corr^GSPC_LRCX', 'corr^GSPC_LUV', 'corr^GSPC_LVS', 'corr^GSPC_LW', 'corr^GSPC_LYB', 'corr^GSPC_M', 'corr^GSPC_MA', 'corr^GSPC_MAA', 'corr^GSPC_MAC', 'corr^GSPC_MAR', 'corr^GSPC_MAS', 'corr^GSPC_MCD', 'corr^GSPC_MCHP', 'corr^GSPC_MCK', 'corr^GSPC_MCO', 'corr^GSPC_MDLZ', 'corr^GSPC_MDT', 'corr^GSPC_MET', 'corr^GSPC_MGM', 'corr^GSPC_MHK', 'corr^GSPC_MKC', 'corr^GSPC_MKTX', 'corr^GSPC_MLM', 'corr^GSPC_MMC', 'corr^GSPC_MMM', 'corr^GSPC_MNST', 'corr^GSPC_MO', 'corr^GSPC_MOS', 'corr^GSPC_MPC', 'corr^GSPC_MRK', 'corr^GSPC_MRO', 'corr^GSPC_MS', 'corr^GSPC_MSCI', 'corr^GSPC_MSFT', 'corr^GSPC_MSI', 'corr^GSPC_MTB', 'corr^GSPC_MTD', 'corr^GSPC_MU', 'corr^GSPC_MXIM', 'corr^GSPC_MYL', 'corr^GSPC_NBL', 'corr^GSPC_NCLH', 'corr^GSPC_NDAQ', 'corr^GSPC_NEE', 'corr^GSPC_NEM', 'corr^GSPC_NFLX', 'corr^GSPC_NI', 'corr^GSPC_NKE', 'corr^GSPC_NLOK', 'corr^GSPC_NLSN', 'corr^GSPC_NOC', 'corr^GSPC_NOV', 'corr^GSPC_NOW', 'corr^GSPC_NRG', 'corr^GSPC_NSC', 'corr^GSPC_NTAP', 'corr^GSPC_NTRS', 'corr^GSPC_NUE', 'corr^GSPC_NVDA', 'corr^GSPC_NVR', 'corr^GSPC_NWL', 'corr^GSPC_NWS', 'corr^GSPC_NWSA', 'corr^GSPC_O', 'corr^GSPC_ODFL', 'corr^GSPC_OKE', 'corr^GSPC_OMC', 'corr^GSPC_ORCL', 'corr^GSPC_ORLY', 'corr^GSPC_OXY', 'corr^GSPC_PAYX', 'corr^GSPC_PBCT', 'corr^GSPC_PCAR', 'corr^GSPC_PEAK', 'corr^GSPC_PEG', 'corr^GSPC_PEP', 'corr^GSPC_PFE', 'corr^GSPC_PFG', 'corr^GSPC_PG', 'corr^GSPC_PGR', 'corr^GSPC_PH', 'corr^GSPC_PHM', 'corr^GSPC_PKG', 'corr^GSPC_PKI', 'corr^GSPC_PLD', 'corr^GSPC_PM', 'corr^GSPC_PNC', 'corr^GSPC_PNR', 'corr^GSPC_PNW', 'corr^GSPC_PPG', 'corr^GSPC_PPL', 'corr^GSPC_PRGO', 'corr^GSPC_PRU', 'corr^GSPC_PSA', 'corr^GSPC_PSX', 'corr^GSPC_PVH', 'corr^GSPC_PWR', 'corr^GSPC_PXD', 'corr^GSPC_PYPL', 'corr^GSPC_QCOM', 'corr^GSPC_QRVO', 'corr^GSPC_RCL', 'corr^GSPC_RE', 'corr^GSPC_REG', 'corr^GSPC_REGN', 'corr^GSPC_RF', 'corr^GSPC_RHI', 'corr^GSPC_RJF', 'corr^GSPC_RL', 'corr^GSPC_RMD', 'corr^GSPC_ROK', 'corr^GSPC_ROL', 'corr^GSPC_ROP', 'corr^GSPC_ROST', 'corr^GSPC_RSG', 'corr^GSPC_RTN', 'corr^GSPC_SBAC', 'corr^GSPC_SBUX', 'corr^GSPC_SCHW', 'corr^GSPC_SEE', 'corr^GSPC_SHW', 'corr^GSPC_SIVB', 'corr^GSPC_SJM', 'corr^GSPC_SLB', 'corr^GSPC_SLG', 'corr^GSPC_SNA', 'corr^GSPC_SNPS', 'corr^GSPC_SO', 'corr^GSPC_SPG', 'corr^GSPC_SPGI', 'corr^GSPC_SPY', 'corr^GSPC_SRE', 'corr^GSPC_STT', 'corr^GSPC_STX', 'corr^GSPC_STZ', 'corr^GSPC_SWK', 'corr^GSPC_SWKS', 'corr^GSPC_SYF', 'corr^GSPC_SYK', 'corr^GSPC_SYY', 'corr^GSPC_T', 'corr^GSPC_TAP', 'corr^GSPC_TDG', 'corr^GSPC_TEL', 'corr^GSPC_TFC', 'corr^GSPC_TFX', 'corr^GSPC_TGT', 'corr^GSPC_TIF', 'corr^GSPC_TJX', 'corr^GSPC_TMO', 'corr^GSPC_TMUS', 'corr^GSPC_TPR', 'corr^GSPC_TRIP', 'corr^GSPC_TROW', 'corr^GSPC_TRV', 'corr^GSPC_TSCO', 'corr^GSPC_TSN', 'corr^GSPC_TTWO', 'corr^GSPC_TWTR', 'corr^GSPC_TXN', 'corr^GSPC_TXT', 'corr^GSPC_UA', 'corr^GSPC_UAA', 'corr^GSPC_UAL', 'corr^GSPC_UDR', 'corr^GSPC_UHS', 'corr^GSPC_ULTA', 'corr^GSPC_UNH', 'corr^GSPC_UNM', 'corr^GSPC_UNP', 'corr^GSPC_UPS', 'corr^GSPC_URI', 'corr^GSPC_USB', 'corr^GSPC_UTX', 'corr^GSPC_V', 'corr^GSPC_VAR', 'corr^GSPC_VFC', 'corr^GSPC_VIAC', 'corr^GSPC_VLO', 'corr^GSPC_VMC', 'corr^GSPC_VNO', 'corr^GSPC_VRSK', 'corr^GSPC_VRSN', 'corr^GSPC_VRTX', 'corr^GSPC_VTR', 'corr^GSPC_VZ', 'corr^GSPC_WAB', 'corr^GSPC_WAT', 'corr^GSPC_WBA', 'corr^GSPC_WCG', 'corr^GSPC_WDC', 'corr^GSPC_WEC', 'corr^GSPC_WELL', 'corr^GSPC_WFC', 'corr^GSPC_WHR', 'corr^GSPC_WLTW', 'corr^GSPC_WM', 'corr^GSPC_WMB', 'corr^GSPC_WMT', 'corr^GSPC_WRB', 'corr^GSPC_WRK', 'corr^GSPC_WU', 'corr^GSPC_WY', 'corr^GSPC_WYNN', 'corr^GSPC_XEC', 'corr^GSPC_XEL', 'corr^GSPC_XLB', 'corr^GSPC_XLE', 'corr^GSPC_XLF', 'corr^GSPC_XLI', 'corr^GSPC_XLK', 'corr^GSPC_XLNX', 'corr^GSPC_XLP', 'corr^GSPC_XLU', 'corr^GSPC_XLV', 'corr^GSPC_XLY', 'corr^GSPC_XOM', 'corr^GSPC_XRAY', 'corr^GSPC_XRT', 'corr^GSPC_XRX', 'corr^GSPC_XYL', 'corr^GSPC_YUM', 'corr^GSPC_ZBH', 'corr^GSPC_ZION', 'corr^GSPC_ZTS', 'corr^GSPC_^IXIC', 'corr^GSPC_^NDX', 'corr^GSPC_^SOX', 'corr^GSPC_^TNX', 'corr^GSPC_^VIX', 'corr^GSPC_^VVIX', 'corr^GSPC_^VXN', 'corr^GSPC_^VXO', 'corr^GSPC_^VXV']\n",
    "    technicals = ['50Minus200_dayMA', '15_dayMA', 'Bol_upper_band', 'Bol_lower_band', 'percent_b', 'Advance_Decline_ratio', '1AD', '2ADD', '3ADOSC', '4ADX', '5ADXR', '6APO', '7AROON DOWN', '7AROON UP', '8AROONOSC', '9ASIN_ADJCLOSE', '10ASIN_HIGH', '11ASIN_LOW', '12ATR', '13AVGPRICE', '14UPPER BAND', '14MIDDLE BAND', '14LOWER BAND', '15rm_return', '15upperband_return', '15lowerband_return', '16BETA', '17BETAMARKET', '18BOP', '19CCI', '20CDL2CROWS', '21CDL3BLACKCROWS', '22CDL3INSIDE', '23CDL3LINESTRIKE', '24CDL3OUTSIDE', '25CDL3STARSINSOUTH', '26CDL3WHITESOLDIERS', '27CDLABANDONEDBABY', '28CDLADVANCEBLOCK', '29CDLBELTHOLD', '30CDLBREAKAWAY0', '31CDLCLOSINGMARUBOZU', '32CDLCONCEALBABYSWALL', '33CDLCOUNTERATTACK', '34CDLDARKCLOUDCOVER', '35CDLDOJI', '36CDLDOJISTAR', '37CDLDRAGONFLYDOJI', '38CDLENGULFING', '39CDLEVENINGDOJISTAR', '40CDLEVENINGSTAR', '41CDLGAPSIDESIDEWHITE', '42CDLGRAVESTONEDOJI', '43CDLHAMMER', '44CDLHANGINGMAN', '45CDLHARAMI', '46CDLHARAMICROSS', '47CDLHIGHWAVE', '48CDLHIKKAKE', '49CDLHIKKAKEMOD', '50CDLHOMINGPIGEON', '51CDLIDENTICAL3CROWS0', '52CDLINNECK', '53CDLINVERTEDHAMMER', '54CDLKICKING', '55CDLKICKINGBYLENGTH', '56CDLLADDERBOTTOM', '57CDLLONGLEGGEDDOJI', '58CDLLONGLINE', '59CDLMARUBOZU', '60CDLMATCHINGLOW', '61CDLMATHOLD', '62CDLMORNINGDOJISTAR', '63CDLMORNINGSTAR', '64CDLONNECK', '65CDLPIERCING', '66CDLRICKSHAWMAN', '67CDLRISEFALL3METHODS', '68CDLSEPARATINGLINES', '69CDLSHOOTINGSTAR', '70CDLSHORTLINE', '71CDLSPINNINGTOP', '72CDLSTALLEDPATTERN', '73CDLSTICKSANDWICH', '74CDLTAKURI', '75CDLTASUKIGAP', '76CDLTHRUSTING', '77CDLTRISTAR', '78CDLUNIQUE3RIVER', '79CDLUPSIDEGAP2CROWS', '80CDLXSIDEGAP3METHODS', '81CMO', '82CORREL', '83CORRELMARKET', '84COSH_ADJCLOSE', '85COSH_HIGH', '86COSH_LOW', '87COS_ADJCLOSE', '88COS_HIGH', '89COS_LOW', '90DEMA', '91DIV', '92DX', '93EMA', '94daily_returns', '94excess_returns', '95EXP', '96HIGHLOW', '97HT_DCPERIOD', '98HT_DCPHASE', '99INPHASE', '99QUADRATURE', '100SINE', '100LEADSINE', '101HT_TRENDLINE', '102HT_TRENDMODE', '103KAMA', '104LINEARREG_AD', '105LINEARREG_ADJCLOSE', '106LINEARREG_ADOSC', '107LINEARREG_ADX', '108LINEARREG_ADXR', '109LINEARREG_ANGLE_AD', '110LINEARREG_ANGLE_ADJCLOSE', '111LINEARREG_ANGLE_ADOSC', '112LINEARREG_ANGLE_ADX', '113LINEARREG_ANGLE_ADXR', '114LINEARREG_ANGLE_APO', '115LINEARREG_ANGLE_AROONOSC', '116LINEARREG_ANGLE_AROON_DOWN', '117LINEARREG_ANGLE_AROON_UP', '118LINEARREG_ANGLE_ATR', '119LINEARREG_ANGLE_BOP', '120LINEARREG_ANGLE_CCI', '121LINEARREG_ANGLE_CMO', '122LINEARREG_ANGLE_DEMA', '123LINEARREG_ANGLE_DX', '124LINEARREG_ANGLE_FAMA', '125LINEARREG_ANGLE_FASTD', '126LINEARREG_ANGLE_FASTDR', '127LINEARREG_ANGLE_FASTK', '128LINEARREG_ANGLE_FASTKR', '129LINEARREG_ANGLE_HIGH', '130LINEARREG_ANGLE_HT_DCPERIOD', '131LINEARREG_ANGLE_HT_DCPHASE', '132LINEARREG_ANGLE_HT_TRENDLINE', '133LINEARREG_ANGLE_INPHASE', '134LINEARREG_ANGLE_KAMA', '135LINEARREG_ANGLE_LEADSINE', '136LINEARREG_ANGLE_LOW', '137LINEARREG_ANGLE_LOWER_BAND', '138LINEARREG_ANGLE_MAMA', '139LINEARREG_ANGLE_MFI', '140LINEARREG_ANGLE_MIDDLE_BAND', '141LINEARREG_ANGLE_MINUS_DI', '142LINEARREG_ANGLE_MINUS_DM', '143LINEARREG_ANGLE_MOM', '144LINEARREG_ANGLE_NATR', '145LINEARREG_ANGLE_OBV', '146LINEARREG_ANGLE_PLUS_DI', '147LINEARREG_ANGLE_PLUS_DM', '148LINEARREG_ANGLE_PPO', '149LINEARREG_ANGLE_QUADRATURE', '150LINEARREG_ANGLE_ROC', '151LINEARREG_ANGLE_ROCP', '152LINEARREG_ANGLE_ROCR', '153LINEARREG_ANGLE_ROCR100', '154LINEARREG_ANGLE_RSI', '155LINEARREG_ANGLE_SAR', '156LINEARREG_ANGLE_SAREXT', '157LINEARREG_ANGLE_SINE', '158LINEARREG_ANGLE_SLOWD', '159LINEARREG_ANGLE_SLOWK', '160LINEARREG_ANGLE_T3', '161LINEARREG_ANGLE_TEMA', '162LINEARREG_ANGLE_TRANGE', '163LINEARREG_ANGLE_TRIMA', '164LINEARREG_ANGLE_TRIX', '165LINEARREG_ANGLE_ULTOSC', '166LINEARREG_ANGLE_UPPER_BAND', '167LINEARREG_ANGLE_WILLR', '168LINEARREG_ANGLE_WMA', '169LINEARREG_APO', '170LINEARREG_AROONOSC', '171LINEARREG_AROON_DOWN', '172LINEARREG_AROON_UP', '173LINEARREG_ATR', '174LINEARREG_BOP', '175LINEARREG_CCI', '176LINEARREG_CMO', '177LINEARREG_DEMA', '178LINEARREG_DX', '179LINEARREG_FAMA', '180LINEARREG_FASTD', '181LINEARREG_FASTDR', '182LINEARREG_FASTK', '183LINEARREG_FASTKR', '184LINEARREG_HIGH', '185LINEARREG_HT_DCPERIOD', '186LINEARREG_HT_DCPHASE', '187LINEARREG_HT_TRENDLINE', '188LINEARREG_INPHASE', '189LINEARREG_INTERCEPT_AD', '190LINEARREG_INTERCEPT_ADJCLOSE', '191LINEARREG_INTERCEPT_ADOSC', '192LINEARREG_INTERCEPT_ADX', '193LINEARREG_INTERCEPT_ADXR', '194LINEARREG_INTERCEPT_APO', '195LINEARREG_INTERCEPT_AROONOSC', '196LINEARREG_INTERCEPT_AROON_DOWN', '197LINEARREG_INTERCEPT_AROON_UP', '198LINEARREG_INTERCEPT_ATR', '199LINEARREG_INTERCEPT_BOP', '200LINEARREG_INTERCEPT_CCI', '201LINEARREG_INTERCEPT_CMO', '202LINEARREG_INTERCEPT_DEMA', '203LINEARREG_INTERCEPT_DX', '204LINEARREG_INTERCEPT_FAMA', '205LINEARREG_INTERCEPT_FASTD', '206LINEARREG_INTERCEPT_FASTDR', '207LINEARREG_INTERCEPT_FASTK', '208LINEARREG_INTERCEPT_FASTKR', '209LINEARREG_INTERCEPT_HIGH', '210LINEARREG_INTERCEPT_HT_DCPERIOD', '211LINEARREG_INTERCEPT_HT_DCPHASE', '212LINEARREG_INTERCEPT_HT_TRENDLINE', '213LINEARREG_INTERCEPT_INPHASE', '214LINEARREG_INTERCEPT_KAMA', '215LINEARREG_INTERCEPT_LEADSINE', '216LINEARREG_INTERCEPT_LOW', '217LINEARREG_INTERCEPT_LOWER_BAND', '218LINEARREG_INTERCEPT_MAMA', '219LINEARREG_INTERCEPT_MFI', '220LINEARREG_INTERCEPT_MIDDLE_BAND', '221LINEARREG_INTERCEPT_MINUS_DI', '222LINEARREG_INTERCEPT_MINUS_DM', '223LINEARREG_INTERCEPT_MOM', '224LINEARREG_INTERCEPT_NATR', '225LINEARREG_INTERCEPT_OBV', '226LINEARREG_INTERCEPT_PLUS_DI', '227LINEARREG_INTERCEPT_PLUS_DM', '228LINEARREG_INTERCEPT_PPO', '229LINEARREG_INTERCEPT_QUADRATURE', '230LINEARREG_INTERCEPT_ROC', '231LINEARREG_INTERCEPT_ROCP', '232LINEARREG_INTERCEPT_ROCR', '233LINEARREG_INTERCEPT_ROCR100', '234LINEARREG_INTERCEPT_RSI', '235LINEARREG_INTERCEPT_SAR', '236LINEARREG_INTERCEPT_SAREXT', '237LINEARREG_INTERCEPT_SINE', '238LINEARREG_INTERCEPT_SLOWD', '239LINEARREG_INTERCEPT_SLOWK', '240LINEARREG_INTERCEPT_T3', '241LINEARREG_INTERCEPT_TEMA', '242LINEARREG_INTERCEPT_TRANGE', '243LINEARREG_INTERCEPT_TRIMA', '244LINEARREG_INTERCEPT_TRIX', '245LINEARREG_INTERCEPT_ULTOSC', '246LINEARREG_INTERCEPT_UPPER_BAND', '247LINEARREG_INTERCEPT_WILLR', '248LINEARREG_INTERCEPT_WMA', '249LINEARREG_KAMA', '250LINEARREG_LEADSINE', '251LINEARREG_LOW', '252LINEARREG_LOWER_BAND', '253LINEARREG_MAMA', '254LINEARREG_MFI', '255LINEARREG_MIDDLE_BAND', '256LINEARREG_MINUS_DI', '257LINEARREG_MINUS_DM', '258LINEARREG_MOM', '259LINEARREG_NATR', '260LINEARREG_OBV', '261LINEARREG_PLUS_DI', '262LINEARREG_PLUS_DM', '263LINEARREG_PPO', '264LINEARREG_QUADRATURE', '265LINEARREG_ROC', '266LINEARREG_ROCP', '267LINEARREG_ROCR', '268LINEARREG_ROCR100', '269LINEARREG_RSI', '270LINEARREG_SAR', '271LINEARREG_SAREXT', '272LINEARREG_SINE', '273LINEARREG_SLOPE_AD', '274LINEARREG_SLOPE_ADJCLOSE', '275LINEARREG_SLOPE_ADOSC', '276LINEARREG_SLOPE_ADX', '277LINEARREG_SLOPE_ADXR', '278LINEARREG_SLOPE_APO', '279LINEARREG_SLOPE_AROONOSC', '280LINEARREG_SLOPE_AROON_DOWN', '281LINEARREG_SLOPE_AROON_UP', '282LINEARREG_SLOPE_ATR', '283LINEARREG_SLOPE_BOP', '284LINEARREG_SLOPE_CCI', '285LINEARREG_SLOPE_CMO', '286LINEARREG_SLOPE_DEMA', '287LINEARREG_SLOPE_DX', '288LINEARREG_SLOPE_FAMA', '289LINEARREG_SLOPE_FASTD', '290LINEARREG_SLOPE_FASTDR', '291LINEARREG_SLOPE_FASTK', '292LINEARREG_SLOPE_FASTKR', '293LINEARREG_SLOPE_HIGH', '294LINEARREG_SLOPE_HT_DCPERIOD', '295LINEARREG_SLOPE_HT_DCPHASE', '296LINEARREG_SLOPE_HT_TRENDLINE', '297LINEARREG_SLOPE_INPHASE', '298LINEARREG_SLOPE_KAMA', '299LINEARREG_SLOPE_LEADSINE', '300LINEARREG_SLOPE_LOW', '301LINEARREG_SLOPE_LOWER_BAND', '302LINEARREG_SLOPE_MAMA', '303LINEARREG_SLOPE_MFI', '304LINEARREG_SLOPE_MIDDLE_BAND', '305LINEARREG_SLOPE_MINUS_DI', '306LINEARREG_SLOPE_MINUS_DM', '307LINEARREG_SLOPE_MOM', '308LINEARREG_SLOPE_NATR', '309LINEARREG_SLOPE_OBV', '310LINEARREG_SLOPE_PLUS_DI', '311LINEARREG_SLOPE_PLUS_DM', '312LINEARREG_SLOPE_PPO', '313LINEARREG_SLOPE_QUADRATURE', '314LINEARREG_SLOPE_ROC', '315LINEARREG_SLOPE_ROCP', '316LINEARREG_SLOPE_ROCR', '317LINEARREG_SLOPE_ROCR100', '318LINEARREG_SLOPE_RSI', '319LINEARREG_SLOPE_SAR', '320LINEARREG_SLOPE_SAREXT', '321LINEARREG_SLOPE_SINE', '322LINEARREG_SLOPE_SLOWD', '323LINEARREG_SLOPE_SLOWK', '324LINEARREG_SLOPE_T3', '325LINEARREG_SLOPE_TEMA', '326LINEARREG_SLOPE_TRANGE', '327LINEARREG_SLOPE_TRIMA', '328LINEARREG_SLOPE_TRIX', '329LINEARREG_SLOPE_ULTOSC', '330LINEARREG_SLOPE_UPPER_BAND', '331LINEARREG_SLOPE_WILLR', '332LINEARREG_SLOPE_WMA', '333LINEARREG_SLOWD', '334LINEARREG_SLOWK', '335LINEARREG_T3', '336LINEARREG_TEMA', '337LINEARREG_TRANGE', '338LINEARREG_TRIMA', '339LINEARREG_TRIX', '340LINEARREG_ULTOSC', '341LINEARREG_UPPER_BAND', '342LINEARREG_WILLR', '343LINEARREG_WMA', '344LN_ADJCLOSE', '345LN_HIGH', '346LN_LOW', '347LOG10_ADJCLOSE', '348LOG10_HIGH', '349LOG10_LOW', '350MACD', '350MACD SIGNAL', '350MACD HIST', '351MACD', '351MACD SIGNAL', '351MACD HIST', '352MACD', '352MACD SIGNAL', '352MACD HIST', '353MAMA', '353FAMA', '354MEDPRICE', '355MFI', '356MIDPOINT', '357MIDPRICE', '358MIN', '358MAX', '359MINIDX', '359MAXIDX', '360MINUS_DI', '361MINUS_DM', '362MOM', '363MULT', '364NATR', '365OBV', '366PLUS_DI', '367PLUS_DM', '368PPO', '369^GSPC_rm_2', '369^GSPC_rm_3', '369^GSPC_rm_4', '369^GSPC_rm_5', '369^GSPC_rm_6', '369^GSPC_rm_7', '369^GSPC_rm_8', '369^GSPC_rm_9', '369^GSPC_rm_10', '369^GSPC_rm_11', '369^GSPC_rm_12', '369^GSPC_rm_13', '369^GSPC_rm_14', '369^GSPC_rm_15', '369^GSPC_rm_16', '369^GSPC_rm_17', '369^GSPC_rm_18', '369^GSPC_rm_19', '369^GSPC_rm_20', '369^GSPC_rm_21', '369^GSPC_rm_22', '369^GSPC_rm_23', '369^GSPC_rm_24', '369^GSPC_rm_25', '369^GSPC_rm_26', '369^GSPC_rm_27', '369^GSPC_rm_28', '369^GSPC_rm_29', '369^GSPC_rm_30', '370ROC', '371ROCP', '372ROCR', '373ROCR100', '374RSI', '375SAR', '376SAREXT', '377SINH_ADJCLOSE', '378SINH_HIGH', '379SINH_LOW', '380SIN_ADJCLOSE', '381SIN_HIGH', '382SIN_LOW', '383SMA_25', '383SMA_50', '383SMA_200', '384SQRT_ADJCLOSE', '385SQRT_HIGH', '386SQRT_LOW', '387STDDEV1_AD', '388STDDEV1_ADJCLOSE', '389STDDEV1_ADOSC', '390STDDEV1_ADX', '391STDDEV1_ADXR', '392STDDEV1_APO', '393STDDEV1_AROONOSC', '394STDDEV1_AROON_DOWN', '395STDDEV1_AROON_UP', '396STDDEV1_ATR', '397STDDEV1_BOP', '398STDDEV1_CCI', '399STDDEV1_CMO', '400STDDEV1_DEMA', '401STDDEV1_DX', '402STDDEV1_FAMA', '403STDDEV1_FASTD', '404STDDEV1_FASTDR', '405STDDEV1_FASTK', '406STDDEV1_FASTKR', '407STDDEV1_HIGH', '408STDDEV1_HT_DCPERIOD', '409STDDEV1_HT_DCPHASE', '410STDDEV1_HT_TRENDLINE', '411STDDEV1_INPHASE', '412STDDEV1_KAMA', '413STDDEV1_LEADSINE', '414STDDEV1_LOW', '415STDDEV1_LOWER_BAND', '416STDDEV1_MAMA', '417STDDEV1_MFI', '418STDDEV1_MIDDLE_BAND', '419STDDEV1_MINUS_DI', '420STDDEV1_MINUS_DM', '421STDDEV1_MOM', '422STDDEV1_NATR', '423STDDEV1_OBV', '424STDDEV1_PLUS_DI', '425STDDEV1_PLUS_DM', '426STDDEV1_PPO', '427STDDEV1_QUADRATURE', '428STDDEV1_ROC', '429STDDEV1_ROCP', '430STDDEV1_ROCR', '431STDDEV1_ROCR100', '432STDDEV1_RSI', '433STDDEV1_SAR', '434STDDEV1_SAREXT', '435STDDEV1_SINE', '436STDDEV1_SLOWD', '437STDDEV1_SLOWK', '438STDDEV1_T3', '439STDDEV1_TEMA', '440STDDEV1_TRANGE', '441STDDEV1_TRIMA', '442STDDEV1_TRIX', '443STDDEV1_ULTOSC', '444STDDEV1_UPPER_BAND', '445STDDEV1_WILLR', '446STDDEV1_WMA', '447STDDEV2_AD', '448STDDEV2_ADJCLOSE', '449STDDEV2_ADOSC', '450STDDEV2_ADX', '451STDDEV2_ADXR', '452STDDEV2_APO', '453STDDEV2_AROONOSC', '454STDDEV2_AROON_DOWN', '455STDDEV2_AROON_UP', '456STDDEV2_ATR', '457STDDEV2_BOP', '458STDDEV2_CCI', '459STDDEV2_CMO', '460STDDEV2_DEMA', '461STDDEV2_DX', '462STDDEV2_FAMA', '463STDDEV2_FASTD', '464STDDEV2_FASTDR', '465STDDEV2_FASTK', '466STDDEV2_FASTKR', '467STDDEV2_HIGH', '468STDDEV2_HT_DCPERIOD', '469STDDEV2_HT_DCPHASE', '470STDDEV2_HT_TRENDLINE', '471STDDEV2_INPHASE', '472STDDEV2_KAMA', '473STDDEV2_LEADSINE', '474STDDEV2_LOW', '475STDDEV2_LOWER_BAND', '476STDDEV2_MAMA', '477STDDEV2_MFI', '478STDDEV2_MIDDLE_BAND', '479STDDEV2_MINUS_DI', '480STDDEV2_MINUS_DM', '481STDDEV2_MOM', '482STDDEV2_NATR', '483STDDEV2_OBV', '484STDDEV2_PLUS_DI', '485STDDEV2_PLUS_DM', '486STDDEV2_PPO', '487STDDEV2_QUADRATURE', '488STDDEV2_ROC', '489STDDEV2_ROCP', '490STDDEV2_ROCR', '491STDDEV2_ROCR100', '492STDDEV2_RSI', '493STDDEV2_SAR', '494STDDEV2_SAREXT', '495STDDEV2_SINE', '496STDDEV2_SLOWD', '497STDDEV2_SLOWK', '498STDDEV2_T3', '499STDDEV2_TEMA', '500STDDEV2_TRANGE', '501STDDEV2_TRIMA', '502STDDEV2_TRIX', '503STDDEV2_ULTOSC', '504STDDEV2_UPPER_BAND', '505STDDEV2_WILLR', '506STDDEV2_WMA', '507SLOWK', '507SLOWD', '508FASTK', '508FASTD', '509FASTKR', '509FASTDR', '510SUB', '511SUM', '512T3', '513TANH_ADJCLOSE', '514TANH_HIGH', '515TANH_LOW', '516TAN_ADJCLOSE', '517TAN_HIGH', '518TAN_LOW', '519TEMA', '520TRANGE', '521TRIMA', '522TRIX', '523TSF_AD', '524TSF_ADJCLOSE', '525TSF_ADOSC', '526TSF_ADX', '527TSF_ADXR', '528TSF_APO', '529TSF_AROONOSC', '530TSF_AROON_DOWN', '531TSF_AROON_UP', '532TSF_ATR', '533TSF_BOP', '534TSF_CCI', '535TSF_CMO', '536TSF_DEMA', '537TSF_DX', '538TSF_FAMA', '539TSF_FASTD', '540TSF_FASTDR', '541TSF_FASTK', '542TSF_FASTKR', '543TSF_HIGH', '544TSF_HT_DCPERIOD', '545TSF_HT_DCPHASE', '546TSF_HT_TRENDLINE', '547TSF_INPHASE', '548TSF_KAMA', '549TSF_LEADSINE', '550TSF_LOW', '551TSF_LOWER_BAND', '552TSF_MAMA', '553TSF_MFI', '554TSF_MIDDLE_BAND', '555TSF_MINUS_DI', '556TSF_MINUS_DM', '557TSF_MOM', '558TSF_NATR', '559TSF_OBV', '560TSF_PLUS_DI', '561TSF_PLUS_DM', '562TSF_PPO', '563TSF_QUADRATURE', '564TSF_ROC', '565TSF_ROCP', '566TSF_ROCR', '567TSF_ROCR100', '568TSF_RSI', '569TSF_SAR', '570TSF_SAREXT', '571TSF_SINE', '572TSF_SLOWD', '573TSF_SLOWK', '574TSF_T3', '575TSF_TEMA', '576TSF_TRANGE', '577TSF_TRIMA', '578TSF_TRIX', '579TSF_ULTOSC', '580TSF_UPPER_BAND', '581TSF_WILLR', '582TSF_WMA', '583TYPPRICE', '584ULTOSC', '585VAR1_AD', '586VAR1_ADJCLOSE', '587VAR1_ADOSC', '588VAR1_ADX', '589VAR1_ADXR', '590VAR1_APO', '591VAR1_AROONOSC', '592VAR1_AROON_DOWN', '593VAR1_AROON_UP', '594VAR1_ATR', '595VAR1_BOP', '596VAR1_CCI', '597VAR1_CMO', '598VAR1_DEMA', '599VAR1_DX', '600VAR1_FAMA', '601VAR1_FASTD', '602VAR1_FASTDR', '603VAR1_FASTK', '604VAR1_FASTKR', '605VAR1_HIGH', '606VAR1_HT_DCPERIOD', '607VAR1_HT_DCPHASE', '608VAR1_HT_TRENDLINE', '609VAR1_INPHASE', '610VAR1_KAMA', '611VAR1_LEADSINE', '612VAR1_LOW', '613VAR1_LOWER_BAND', '614VAR1_MAMA', '615VAR1_MFI', '616VAR1_MIDDLE_BAND', '617VAR1_MINUS_DI', '618VAR1_MINUS_DM', '619VAR1_MOM', '620VAR1_NATR', '621VAR1_OBV', '622VAR1_PLUS_DI', '623VAR1_PLUS_DM', '624VAR1_PPO', '625VAR1_QUADRATURE', '626VAR1_ROC', '627VAR1_ROCP', '628VAR1_ROCR', '629VAR1_ROCR100', '630VAR1_RSI', '631VAR1_SAR', '632VAR1_SAREXT', '633VAR1_SINE', '634VAR1_SLOWD', '635VAR1_SLOWK', '636VAR1_T3', '637VAR1_TEMA', '638VAR1_TRANGE', '639VAR1_TRIMA', '640VAR1_TRIX', '641VAR1_ULTOSC', '642VAR1_UPPER_BAND', '643VAR1_WILLR', '644VAR1_WMA', '645VAR2_AD', '646VAR2_ADJCLOSE', '647VAR2_ADOSC', '648VAR2_ADX', '649VAR2_ADXR', '650VAR2_APO', '651VAR2_AROONOSC', '652VAR2_AROON_DOWN', '653VAR2_AROON_UP', '654VAR2_ATR', '655VAR2_BOP', '656VAR2_CCI', '657VAR2_CMO', '658VAR2_DEMA', '659VAR2_DX', '660VAR2_FAMA', '661VAR2_FASTD', '662VAR2_FASTDR', '663VAR2_FASTK', '664VAR2_FASTKR', '665VAR2_HIGH', '666VAR2_HT_DCPERIOD', '667VAR2_HT_DCPHASE', '668VAR2_HT_TRENDLINE', '669VAR2_INPHASE', '670VAR2_KAMA', '671VAR2_LEADSINE', '672VAR2_LOW', '673VAR2_LOWER_BAND', '674VAR2_MAMA', '675VAR2_MFI', '676VAR2_MIDDLE_BAND', '677VAR2_MINUS_DI', '678VAR2_MINUS_DM', '679VAR2_MOM', '680VAR2_NATR', '681VAR2_OBV', '682VAR2_PLUS_DI', '683VAR2_PLUS_DM', '684VAR2_PPO', '685VAR2_QUADRATURE', '686VAR2_ROC', '687VAR2_ROCP', '688VAR2_ROCR', '689VAR2_ROCR100', '690VAR2_RSI', '691VAR2_SAR', '692VAR2_SAREXT', '693VAR2_SINE', '694VAR2_SLOWD', '695VAR2_SLOWK', '696VAR2_T3', '697VAR2_TEMA', '698VAR2_TRANGE', '699VAR2_TRIMA', '700VAR2_TRIX', '701VAR2_ULTOSC', '702VAR2_UPPER_BAND', '703VAR2_WILLR', '704VAR2_WMA', '705WCLPRICE', '706WILLR', '707WMA']\n",
    "    dtw = ['dtw_^GSPC_A', 'dtw_^GSPC_AAL', 'dtw_^GSPC_AAP', 'dtw_AAPL_^GSPC', 'dtw_ABBV_^GSPC', 'dtw_ABC_^GSPC', 'dtw_ABMD_^GSPC', 'dtw_ABT_^GSPC', 'dtw_ACN_^GSPC', 'dtw_ADBE_^GSPC', 'dtw_ADI_^GSPC', 'dtw_ADM_^GSPC', 'dtw_ADP_^GSPC', 'dtw_ADS_^GSPC', 'dtw_ADSK_^GSPC', 'dtw_AEE_^GSPC', 'dtw_AEP_^GSPC', 'dtw_AES_^GSPC', 'dtw_AFL_^GSPC', 'dtw_AGN_^GSPC', 'dtw_AIG_^GSPC', 'dtw_AIV_^GSPC', 'dtw_AIZ_^GSPC', 'dtw_AJG_^GSPC', 'dtw_AKAM_^GSPC', 'dtw_ALB_^GSPC', 'dtw_ALGN_^GSPC', 'dtw_ALK_^GSPC', 'dtw_ALL_^GSPC', 'dtw_ALLE_^GSPC', 'dtw_ALXN_^GSPC', 'dtw_AMAT_^GSPC', 'dtw_AMCR_^GSPC', 'dtw_AMD_^GSPC', 'dtw_AME_^GSPC', 'dtw_AMGN_^GSPC', 'dtw_AMP_^GSPC', 'dtw_AMT_^GSPC', 'dtw_AMZN_^GSPC', 'dtw_ANET_^GSPC', 'dtw_ANSS_^GSPC', 'dtw_ANTM_^GSPC', 'dtw_AON_^GSPC', 'dtw_AOS_^GSPC', 'dtw_APA_^GSPC', 'dtw_APD_^GSPC', 'dtw_APH_^GSPC', 'dtw_APTV_^GSPC', 'dtw_ARE_^GSPC', 'dtw_ARNC_^GSPC', 'dtw_ATO_^GSPC', 'dtw_ATVI_^GSPC', 'dtw_AVB_^GSPC', 'dtw_AVGO_^GSPC', 'dtw_AVY_^GSPC', 'dtw_AWK_^GSPC', 'dtw_AXP_^GSPC', 'dtw_AZO_^GSPC', 'dtw_BA_^GSPC', 'dtw_BAC_^GSPC', 'dtw_BAX_^GSPC', 'dtw_BBY_^GSPC', 'dtw_BDX_^GSPC', 'dtw_BEN_^GSPC', 'dtw_BF-B_^GSPC', 'dtw_BIIB_^GSPC', 'dtw_BK_^GSPC', 'dtw_BKNG_^GSPC', 'dtw_BKR_^GSPC', 'dtw_BLK_^GSPC', 'dtw_BLL_^GSPC', 'dtw_BMY_^GSPC', 'dtw_BR_^GSPC', 'dtw_BRK-B_^GSPC', 'dtw_BSX_^GSPC', 'dtw_BWA_^GSPC', 'dtw_BXP_^GSPC', 'dtw_C_^GSPC', 'dtw_CAG_^GSPC', 'dtw_CAH_^GSPC', 'dtw_CAT_^GSPC', 'dtw_CB_^GSPC', 'dtw_CBOE_^GSPC', 'dtw_CBRE_^GSPC', 'dtw_CCI_^GSPC', 'dtw_CCL_^GSPC', 'dtw_CDNS_^GSPC', 'dtw_CDW_^GSPC', 'dtw_CE_^GSPC', 'dtw_CERN_^GSPC', 'dtw_CF_^GSPC', 'dtw_CFG_^GSPC', 'dtw_CHD_^GSPC', 'dtw_CHRW_^GSPC', 'dtw_CHTR_^GSPC', 'dtw_CI_^GSPC', 'dtw_CINF_^GSPC', 'dtw_CL_^GSPC', 'dtw_CLX_^GSPC', 'dtw_CMA_^GSPC', 'dtw_CMCSA_^GSPC', 'dtw_CME_^GSPC', 'dtw_CMG_^GSPC', 'dtw_CMI_^GSPC', 'dtw_CMS_^GSPC', 'dtw_CNC_^GSPC', 'dtw_CNP_^GSPC', 'dtw_COF_^GSPC', 'dtw_COG_^GSPC', 'dtw_COO_^GSPC', 'dtw_COP_^GSPC', 'dtw_COST_^GSPC', 'dtw_COTY_^GSPC', 'dtw_CPB_^GSPC', 'dtw_CPRI_^GSPC', 'dtw_CPRT_^GSPC', 'dtw_CRM_^GSPC', 'dtw_CSCO_^GSPC', 'dtw_CSX_^GSPC', 'dtw_CTAS_^GSPC', 'dtw_CTL_^GSPC', 'dtw_CTSH_^GSPC', 'dtw_CTVA_^GSPC', 'dtw_CTXS_^GSPC', 'dtw_CVS_^GSPC', 'dtw_CVX_^GSPC', 'dtw_CXO_^GSPC', 'dtw_D_^GSPC', 'dtw_DAL_^GSPC', 'dtw_DD_^GSPC', 'dtw_DE_^GSPC', 'dtw_DFS_^GSPC', 'dtw_DG_^GSPC', 'dtw_DGX_^GSPC', 'dtw_DHI_^GSPC', 'dtw_DHR_^GSPC', 'dtw_DIA_^GSPC', 'dtw_DIS_^GSPC', 'dtw_DISCA_^GSPC', 'dtw_DISCK_^GSPC', 'dtw_DISH_^GSPC', 'dtw_DLR_^GSPC', 'dtw_DLTR_^GSPC', 'dtw_DOV_^GSPC', 'dtw_DOW_^GSPC', 'dtw_DRE_^GSPC', 'dtw_DRI_^GSPC', 'dtw_DTE_^GSPC', 'dtw_DUK_^GSPC', 'dtw_DVA_^GSPC', 'dtw_DVN_^GSPC', 'dtw_DX-Y.NYB_^GSPC', 'dtw_DXC_^GSPC', 'dtw_EA_^GSPC', 'dtw_EBAY_^GSPC', 'dtw_ECL_^GSPC', 'dtw_ED_^GSPC', 'dtw_EFX_^GSPC', 'dtw_EIX_^GSPC', 'dtw_EL_^GSPC', 'dtw_EMN_^GSPC', 'dtw_EMR_^GSPC', 'dtw_EOG_^GSPC', 'dtw_EQIX_^GSPC', 'dtw_EQR_^GSPC', 'dtw_ES_^GSPC', 'dtw_ESS_^GSPC', 'dtw_ETFC_^GSPC', 'dtw_ETN_^GSPC', 'dtw_ETR_^GSPC', 'dtw_EVRG_^GSPC', 'dtw_EW_^GSPC', 'dtw_EXC_^GSPC', 'dtw_EXPD_^GSPC', 'dtw_EXPE_^GSPC', 'dtw_EXR_^GSPC', 'dtw_F_^GSPC', 'dtw_FANG_^GSPC', 'dtw_FAST_^GSPC', 'dtw_FB_^GSPC', 'dtw_FBHS_^GSPC', 'dtw_FCX_^GSPC', 'dtw_FDX_^GSPC', 'dtw_FE_^GSPC', 'dtw_FFIV_^GSPC', 'dtw_FIS_^GSPC', 'dtw_FISV_^GSPC', 'dtw_FITB_^GSPC', 'dtw_FLIR_^GSPC', 'dtw_FLS_^GSPC', 'dtw_FLT_^GSPC', 'dtw_FMC_^GSPC', 'dtw_FOX_^GSPC', 'dtw_FOXA_^GSPC', 'dtw_FRC_^GSPC', 'dtw_FRT_^GSPC', 'dtw_FTI_^GSPC', 'dtw_FTNT_^GSPC', 'dtw_FTV_^GSPC', 'dtw_GD_^GSPC', 'dtw_GE_^GSPC', 'dtw_GILD_^GSPC', 'dtw_GIS_^GSPC', 'dtw_GL_^GSPC', 'dtw_GLW_^GSPC', 'dtw_GM_^GSPC', 'dtw_GOOG_^GSPC', 'dtw_GPC_^GSPC', 'dtw_GPN_^GSPC', 'dtw_GPS_^GSPC', 'dtw_GRMN_^GSPC', 'dtw_GS_^GSPC', 'dtw_GWW_^GSPC', 'dtw_HAL_^GSPC', 'dtw_HAS_^GSPC', 'dtw_HBAN_^GSPC', 'dtw_HBI_^GSPC', 'dtw_HCA_^GSPC', 'dtw_HD_^GSPC', 'dtw_HES_^GSPC', 'dtw_HFC_^GSPC', 'dtw_HIG_^GSPC', 'dtw_HII_^GSPC', 'dtw_HLT_^GSPC', 'dtw_HOG_^GSPC', 'dtw_HOLX_^GSPC', 'dtw_HON_^GSPC', 'dtw_HP_^GSPC', 'dtw_HPE_^GSPC', 'dtw_HPQ_^GSPC', 'dtw_HRB_^GSPC', 'dtw_HRL_^GSPC', 'dtw_HSIC_^GSPC', 'dtw_HST_^GSPC', 'dtw_HSY_^GSPC', 'dtw_HUM_^GSPC', 'dtw_IBM_^GSPC', 'dtw_ICE_^GSPC', 'dtw_IDXX_^GSPC', 'dtw_IEX_^GSPC', 'dtw_IFF_^GSPC', 'dtw_ILMN_^GSPC', 'dtw_INCY_^GSPC', 'dtw_INFO_^GSPC', 'dtw_INTC_^GSPC', 'dtw_INTU_^GSPC', 'dtw_IP_^GSPC', 'dtw_IPG_^GSPC', 'dtw_IPGP_^GSPC', 'dtw_IQV_^GSPC', 'dtw_IR_^GSPC', 'dtw_IRM_^GSPC', 'dtw_ISRG_^GSPC', 'dtw_IT_^GSPC', 'dtw_ITW_^GSPC', 'dtw_IVZ_^GSPC', 'dtw_J_^GSPC', 'dtw_JBHT_^GSPC', 'dtw_JCI_^GSPC', 'dtw_JKHY_^GSPC', 'dtw_JNJ_^GSPC', 'dtw_JNPR_^GSPC', 'dtw_JPM_^GSPC', 'dtw_JWN_^GSPC', 'dtw_K_^GSPC', 'dtw_KEY_^GSPC', 'dtw_KEYS_^GSPC', 'dtw_KHC_^GSPC', 'dtw_KIM_^GSPC', 'dtw_KLAC_^GSPC', 'dtw_KMB_^GSPC', 'dtw_KMI_^GSPC', 'dtw_KMX_^GSPC', 'dtw_KO_^GSPC', 'dtw_KR_^GSPC', 'dtw_KSS_^GSPC', 'dtw_KSU_^GSPC', 'dtw_L_^GSPC', 'dtw_LB_^GSPC', 'dtw_LDOS_^GSPC', 'dtw_LEG_^GSPC', 'dtw_LEN_^GSPC', 'dtw_LH_^GSPC', 'dtw_LHX_^GSPC', 'dtw_LIN_^GSPC', 'dtw_LKQ_^GSPC', 'dtw_LLY_^GSPC', 'dtw_LMT_^GSPC', 'dtw_LNC_^GSPC', 'dtw_LNT_^GSPC', 'dtw_LOW_^GSPC', 'dtw_LRCX_^GSPC', 'dtw_LUV_^GSPC', 'dtw_LVS_^GSPC', 'dtw_LW_^GSPC', 'dtw_LYB_^GSPC', 'dtw_LYV_^GSPC', 'dtw_M_^GSPC', 'dtw_MA_^GSPC', 'dtw_MAA_^GSPC', 'dtw_MAR_^GSPC', 'dtw_MAS_^GSPC', 'dtw_MCD_^GSPC', 'dtw_MCHP_^GSPC', 'dtw_MCK_^GSPC', 'dtw_MCO_^GSPC', 'dtw_MDLZ_^GSPC', 'dtw_MDT_^GSPC', 'dtw_MET_^GSPC', 'dtw_MGM_^GSPC', 'dtw_MHK_^GSPC', 'dtw_MKC_^GSPC', 'dtw_MKTX_^GSPC', 'dtw_MLM_^GSPC', 'dtw_MMC_^GSPC', 'dtw_MMM_^GSPC', 'dtw_MNST_^GSPC', 'dtw_MO_^GSPC', 'dtw_MOS_^GSPC', 'dtw_MPC_^GSPC', 'dtw_MRK_^GSPC', 'dtw_MRO_^GSPC', 'dtw_MS_^GSPC', 'dtw_MSCI_^GSPC', 'dtw_MSFT_^GSPC', 'dtw_MSI_^GSPC', 'dtw_MTB_^GSPC', 'dtw_MTD_^GSPC', 'dtw_MU_^GSPC', 'dtw_MXIM_^GSPC', 'dtw_MYL_^GSPC', 'dtw_NBL_^GSPC', 'dtw_NCLH_^GSPC', 'dtw_NDAQ_^GSPC', 'dtw_NEE_^GSPC', 'dtw_NEM_^GSPC', 'dtw_NFLX_^GSPC', 'dtw_NI_^GSPC', 'dtw_NKE_^GSPC', 'dtw_NLOK_^GSPC', 'dtw_NLSN_^GSPC', 'dtw_NOC_^GSPC', 'dtw_NOV_^GSPC', 'dtw_NOW_^GSPC', 'dtw_NRG_^GSPC', 'dtw_NSC_^GSPC', 'dtw_NTAP_^GSPC', 'dtw_NTRS_^GSPC', 'dtw_NUE_^GSPC', 'dtw_NVDA_^GSPC', 'dtw_NVR_^GSPC', 'dtw_NWL_^GSPC', 'dtw_NWS_^GSPC', 'dtw_NWSA_^GSPC', 'dtw_O_^GSPC', 'dtw_ODFL_^GSPC', 'dtw_OKE_^GSPC', 'dtw_OMC_^GSPC', 'dtw_ORCL_^GSPC', 'dtw_ORLY_^GSPC', 'dtw_OXY_^GSPC', 'dtw_PAYX_^GSPC', 'dtw_PBCT_^GSPC', 'dtw_PCAR_^GSPC', 'dtw_PEAK_^GSPC', 'dtw_PEG_^GSPC', 'dtw_PEP_^GSPC', 'dtw_PFE_^GSPC', 'dtw_PFG_^GSPC', 'dtw_PG_^GSPC', 'dtw_PGR_^GSPC', 'dtw_PH_^GSPC', 'dtw_PHM_^GSPC', 'dtw_PKG_^GSPC', 'dtw_PKI_^GSPC', 'dtw_PLD_^GSPC', 'dtw_PM_^GSPC', 'dtw_PNC_^GSPC', 'dtw_PNR_^GSPC', 'dtw_PNW_^GSPC', 'dtw_PPG_^GSPC', 'dtw_PPL_^GSPC', 'dtw_PRGO_^GSPC', 'dtw_PRU_^GSPC', 'dtw_PSA_^GSPC', 'dtw_PSX_^GSPC', 'dtw_PVH_^GSPC', 'dtw_PWR_^GSPC', 'dtw_PXD_^GSPC', 'dtw_PYPL_^GSPC', 'dtw_QCOM_^GSPC', 'dtw_QRVO_^GSPC', 'dtw_RCL_^GSPC', 'dtw_RE_^GSPC', 'dtw_REG_^GSPC', 'dtw_REGN_^GSPC', 'dtw_RF_^GSPC', 'dtw_RHI_^GSPC', 'dtw_RJF_^GSPC', 'dtw_RL_^GSPC', 'dtw_RMD_^GSPC', 'dtw_ROK_^GSPC', 'dtw_ROL_^GSPC', 'dtw_ROP_^GSPC', 'dtw_ROST_^GSPC', 'dtw_RSG_^GSPC', 'dtw_RTN_^GSPC', 'dtw_SBAC_^GSPC', 'dtw_SBUX_^GSPC', 'dtw_SCHW_^GSPC', 'dtw_SEE_^GSPC', 'dtw_SHW_^GSPC', 'dtw_SIVB_^GSPC', 'dtw_SJM_^GSPC', 'dtw_SLB_^GSPC', 'dtw_SLG_^GSPC', 'dtw_SNA_^GSPC', 'dtw_SNPS_^GSPC', 'dtw_SO_^GSPC', 'dtw_SPG_^GSPC', 'dtw_SPGI_^GSPC', 'dtw_SPY_^GSPC', 'dtw_SRE_^GSPC', 'dtw_STE_^GSPC', 'dtw_STT_^GSPC', 'dtw_STX_^GSPC', 'dtw_STZ_^GSPC', 'dtw_SWK_^GSPC', 'dtw_SWKS_^GSPC', 'dtw_SYF_^GSPC', 'dtw_SYK_^GSPC', 'dtw_SYY_^GSPC', 'dtw_T_^GSPC', 'dtw_TAP_^GSPC', 'dtw_TDG_^GSPC', 'dtw_TEL_^GSPC', 'dtw_TFC_^GSPC', 'dtw_TFX_^GSPC', 'dtw_TGT_^GSPC', 'dtw_TIF_^GSPC', 'dtw_TJX_^GSPC', 'dtw_TMO_^GSPC', 'dtw_TMUS_^GSPC', 'dtw_TPR_^GSPC', 'dtw_TROW_^GSPC', 'dtw_TRV_^GSPC', 'dtw_TSCO_^GSPC', 'dtw_TSN_^GSPC', 'dtw_TTWO_^GSPC', 'dtw_TWTR_^GSPC', 'dtw_TXN_^GSPC', 'dtw_TXT_^GSPC', 'dtw_UA_^GSPC', 'dtw_UAA_^GSPC', 'dtw_UAL_^GSPC', 'dtw_UDR_^GSPC', 'dtw_UHS_^GSPC', 'dtw_ULTA_^GSPC', 'dtw_UNH_^GSPC', 'dtw_UNM_^GSPC', 'dtw_UNP_^GSPC', 'dtw_UPS_^GSPC', 'dtw_URI_^GSPC', 'dtw_USB_^GSPC', 'dtw_UTX_^GSPC', 'dtw_V_^GSPC', 'dtw_VAR_^GSPC', 'dtw_VFC_^GSPC', 'dtw_VIAC_^GSPC', 'dtw_VLO_^GSPC', 'dtw_VMC_^GSPC', 'dtw_VNO_^GSPC', 'dtw_VRSK_^GSPC', 'dtw_VRSN_^GSPC', 'dtw_VRTX_^GSPC', 'dtw_VTR_^GSPC', 'dtw_VZ_^GSPC', 'dtw_WAB_^GSPC', 'dtw_WAT_^GSPC', 'dtw_WBA_^GSPC', 'dtw_WCG_^GSPC', 'dtw_WDC_^GSPC', 'dtw_WEC_^GSPC', 'dtw_WELL_^GSPC', 'dtw_WFC_^GSPC', 'dtw_WHR_^GSPC', 'dtw_WLTW_^GSPC', 'dtw_WM_^GSPC', 'dtw_WMB_^GSPC', 'dtw_WMT_^GSPC', 'dtw_WRB_^GSPC', 'dtw_WRK_^GSPC', 'dtw_WU_^GSPC', 'dtw_WY_^GSPC', 'dtw_WYNN_^GSPC', 'dtw_XEC_^GSPC', 'dtw_XEL_^GSPC', 'dtw_XLB_^GSPC', 'dtw_XLE_^GSPC', 'dtw_XLF_^GSPC', 'dtw_XLI_^GSPC', 'dtw_XLK_^GSPC', 'dtw_XLNX_^GSPC', 'dtw_XLP_^GSPC', 'dtw_XLU_^GSPC', 'dtw_XLV_^GSPC', 'dtw_XLY_^GSPC', 'dtw_XOM_^GSPC', 'dtw_XRAY_^GSPC', 'dtw_XRT_^GSPC', 'dtw_XRX_^GSPC', 'dtw_XYL_^GSPC', 'dtw_YUM_^GSPC', 'dtw_ZBH_^GSPC', 'dtw_ZBRA_^GSPC', 'dtw_ZION_^GSPC', 'dtw_ZTS_^GSPC', 'dtw_^GSPC_^GSPC', 'dtw_^IXIC_^GSPC', 'dtw_^NDX_^GSPC', 'dtw_^SOX_^GSPC', 'dtw_^TNX_^GSPC', 'dtw_^VIX_^GSPC', 'dtw_^VVIX_^GSPC', 'dtw_^VXN_^GSPC', 'dtw_^VXO_^GSPC', 'dtw_^VXV_^GSPC', 'dtw_^GSPC_to_all']\n",
    "    target_label = ['^GSPC_7d_target']\n",
    "    \n",
    "    df=pd.read_csv('../^GSPC_7_days_0_return_dtw.csv', index_col = 0, parse_dates = True)#read data\n",
    "    \n",
    "    \n",
    "    y = df[[ticker]]  # select target label to predict\n",
    "    '''\n",
    "    ************************************************************************************\n",
    "    Please select the features to be included in the training data in df = df[stocks+...+..+..+]\n",
    "    ************************************************************************************\n",
    "    '''\n",
    "    # add the list name with the '+' sign, stocks are prerequisite for calculating rolling mean\n",
    "    #selected_features = stocks+correlations+technicals+pattern+economic+fama+derivatives+dtw\n",
    "    \n",
    "    selected_features = stocks+correlations+pattern+economic+fama+derivatives+dtw\n",
    "    \n",
    "    '''\n",
    "    ************************************************************************************\n",
    "    '''\n",
    "    df = df[selected_features]\n",
    "    remove_columns=([i for i in df.columns if df[i].isnull().any()]) # find Nan columns\n",
    "    #print('removed columns: ', remove_columns)\n",
    "    df.drop(remove_columns, axis=1, inplace=True) # drop columns with NaN\n",
    "    \n",
    "   \n",
    "    \n",
    "    try: #to prevent error from not choosing the 'stocks' list for the dataframe(no ticker column)\n",
    "        for a in range(2,rm_window+1):\n",
    "            df[ticker+'rm_'+str(a)] = df[ticker].rolling(window=rm_window,center=False).mean()\n",
    "        #print('rolling mean columns added: ', rm_window)\n",
    "        df = df[rm_window:] # slice to remove top rows with Nan generated due to rolling mean calc\n",
    "        y = y[rm_window:] # slice to remove top rows with Nan generated due to rolling mean calc\n",
    "    except:\n",
    "        print('no stock ticker column selected. Therefore no rolling mean column added or something went wrong')\n",
    "        pass\n",
    "    \n",
    "\n",
    "    return df, y, ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    '''\n",
    "      Function applies percent change and prepares the data for machine learning model\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    batch_size, time_steps, rm_window, hyperas_epochs, forecast_period, x_scaler, y_scaler = data_params()\n",
    "    df, y, ticker = feature_selection()\n",
    "    kfold, train_size, shuffle, split, fold_number,random_state = data_split()\n",
    "    \n",
    "    last_date = df.index[-1]\n",
    "\n",
    "    # add percent change\n",
    "    #df=df.pct_change()\n",
    "    #df=df.replace([np.inf, -np.inf],np.nan) \n",
    "    #df.fillna(0, inplace=True)\n",
    "    #df.isnull().any().mean()\n",
    "    y=y.shift(-forecast_period)\n",
    "    #x=df.values\n",
    "    #y=y.values\n",
    "    \n",
    "    # apply preprocessing \n",
    "    x = x_scaler.fit_transform(df)\n",
    "    y = y_scaler.fit_transform(y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    # apply time steps\n",
    "    def create_dataset(X, y, time_steps=1):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            v = X[i:(i + time_steps)]\n",
    "            Xs.append(v)\n",
    "            ys.append(y[i + time_steps])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "    x, y = create_dataset(x, y, time_steps)\n",
    "    # forecast periods and removing last7 rows (for the target being NaN for 7 day model)\n",
    "    \n",
    "    \n",
    "    x_predict = x[-batch_size:]\n",
    "   \n",
    "    \n",
    "    x = x[:-forecast_period]\n",
    "    y =  y[:-forecast_period]\n",
    "    \n",
    "   \n",
    "    \n",
    "    #implementation of fold\n",
    "    if kfold:\n",
    "        count = 0\n",
    "        fd = KFold(n_splits=split, random_state=random_state, shuffle=shuffle)\n",
    "\n",
    "        for train_index, test_index in fd.split(x):\n",
    "            count +=1\n",
    "            if count == fold_number: \n",
    "                x_train, x_test,y_train, y_test  = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "                del x, y, train_index, test_index\n",
    "                print('Prepated data for a kfold split of ', split, ' with the fold number ', fold_number)\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test=train_test_split(x,y, train_size=train_size, random_state=random_state, shuffle=shuffle)\n",
    "        print('Prepared data with a train_test_split of ', train_size) \n",
    "        \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "    \n",
    "\n",
    "    # adjustment for batch_size\n",
    "    train_start = x_train.shape[0]%batch_size\n",
    "    test_start = x_test.shape[0]%batch_size\n",
    "    x_train = x_train[train_start:]\n",
    "    y_train = y_train[train_start:]\n",
    "    x_test = x_test[test_start:]\n",
    "    y_test = y_test[test_start:]\n",
    "    \n",
    "            \n",
    "    return x_train, y_train, x_test, y_test, batch_size, time_steps, x_predict, last_date, ticker, x_scaler, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc8hDOIbADgQ"
   },
   "outputs": [],
   "source": [
    "def create_cudnnlstm_model(x_train, y_train, x_test, y_test, batch_size):\n",
    "    '''\n",
    "    Function creates and compiles machine learning model\n",
    "    '''\n",
    "    activation_choice = {{choice(['relu', 'tanh', 'selu', 'elu'])}}\n",
    "   \n",
    "    loss_choice = {{choice(['mean_squared_error', 'mean_absolute_error', \\\n",
    "                            'mean_absolute_percentage_error', 'mean_squared_logarithmic_error', \\\n",
    "                             'huber_loss'])}}\n",
    "    optimizer_choice = {{choice(['sgd','rmsprop','adam', 'adadelta'])}}\n",
    "    units = {{choice([64, 128, 254, 512])}}\n",
    "    units1 = {{choice([16, 32,64, 128])}}\n",
    "    units2 = {{choice([8, 16, 32,64])}}\n",
    "    l1_k_regularizer_choice = {{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}\n",
    "    l2_k_regularizer_choice = {{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}\n",
    "    b_regularizer_choice = {{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}\n",
    "    a_regularizer_choice = {{choice([0.01,0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1])}}\n",
    "    kernel_initializer = {{choice(['glorot_normal', 'glorot_uniform', 'VarianceScaling', 'truncated_normal', 'RandomNormal'])}}\n",
    "    dropout_choice = {{uniform(0, 1)}}\n",
    "    dropout_choice1 = {{uniform(0, 0.3)}}\n",
    "    \n",
    "    model = Sequential()\n",
    "   \n",
    "    model.add(CuDNNLSTM(units = units ,return_sequences = True,stateful = True, go_backwards = True, \n",
    "                        batch_input_shape = (batch_size,x_train.shape[1], x_train.shape[2]), \n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        kernel_regularizer=regularizers.l2(l2_k_regularizer_choice),\n",
    "                        bias_regularizer=regularizers.l2(b_regularizer_choice)\n",
    "                        ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_choice))\n",
    "\n",
    "    \n",
    "    model.add(CuDNNLSTM(units = units1,return_sequences = False, stateful = True, go_backwards = True, \n",
    "                        kernel_regularizer=regularizers.l2(l2_k_regularizer_choice),\n",
    "                        bias_regularizer=regularizers.l2(b_regularizer_choice)\n",
    "                       ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_choice))\n",
    "    \n",
    "   \n",
    "    if {{choice(['no_dense', 'yes_dense'])}} == 'yes_dense': \n",
    "        model.add(Dense(units = units2, activation = activation_choice,\n",
    "                        kernel_regularizer=regularizers.l2(l2_k_regularizer_choice),\n",
    "                        bias_regularizer=regularizers.l2(b_regularizer_choice)\n",
    "                       ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_choice1)) \n",
    "   \n",
    "              \n",
    "\n",
    "    #Add the output layer\n",
    "    model.add(Dense(units=1))\n",
    "    model.add(Activation('linear'))\n",
    "    #model.summary()\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.95, mode='auto', patience=3, min_lr=0.000001, \n",
    "                                              verbose=False)\n",
    "    \n",
    "    model.compile(loss=loss_choice, optimizer = optimizer_choice, metrics = ['mse','mae','mape'])\n",
    "   \n",
    "   \n",
    "    history = model.fit(x_train, y_train,  batch_size=batch_size, validation_data=(x_test, y_test), \n",
    "                        epochs=hyperas_epochs, verbose=False,  callbacks=[reduce_lr])\n",
    "    \n",
    "    \n",
    "    validation_mse = np.amin(history.history['val_mse'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Loss: ', history.history['loss'])\n",
    "    print('Val loss: ', history.history['val_loss'])\n",
    "    print('Mse: ', history.history['mse'])  \n",
    "    print('Val_mse: ', history.history['val_mse'])  \n",
    "    print('Mae: ', history.history['mae'])  \n",
    "    print('Val_mae: ', history.history['mae'])  \n",
    "    print('Mape: ', history.history['mape'])  \n",
    "    print('Val_mape: ', history.history['val_mape'])  \n",
    "    \n",
    "    print('Lr: ', history.history['lr'])  \n",
    "    print('*'*30+'Next Evaluation Results'+'*'*30)\n",
    "        \n",
    "    return {'loss': validation_mse, 'status': STATUS_OK, 'model': model}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callback(early_stopping_count):\n",
    "    '''\n",
    "    Function returns callbacks for training the model\n",
    "    '''\n",
    "    build_path('train_model')\n",
    "    build_path('train_model/checkpoint')\n",
    "    build_path(\"logs\")\n",
    "    build_path('train_metrics')\n",
    "    \n",
    "    \n",
    "    call_reduce = ReduceLROnPlateau(monitor='val_mse', factor=0.95, mode='min', patience=3, min_lr=0.000001, \n",
    "                                             verbose=call_reduce_verbose)\n",
    "    callback_es = tf.keras.callbacks.EarlyStopping(monitor='val_mse', patience=early_stopping_count)\n",
    "    \n",
    "    save_best = ModelCheckpoint('train_model/'+model_name+\".h5\", save_best_only=True, monitor='val_mse', mode='min', verbose=checkpointer_verbose)\n",
    "    \n",
    "    save_ckpt = ModelCheckpoint('train_model/checkpoint/'+model_name+'.ckpt', save_weights_only=True, \n",
    "                                  verbose=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 10:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "    callback_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)    \n",
    "    #callback_pl = tf.keras.callbacks.ProgbarLogger(count_mode=\"samples\")\n",
    "    callback_csv = tf.keras.callbacks.CSVLogger(filename = 'train_metrics/'+model_name+\".csv\")\n",
    "    \n",
    "    return save_best, save_ckpt, call_reduce, callback_lr, callback_es, callback_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_history(history):\n",
    "    '''\n",
    "    Function charts the training history for loss, accuracy and learning rate\n",
    "    '''\n",
    "    print(history.history.keys())\n",
    "    keys = ['loss', 'mse', 'mae', 'mape']\n",
    "    \n",
    "    for i in keys:\n",
    "        plt.plot(history.history[i])\n",
    "        plt.plot(history.history['val_'+i])\n",
    "        plt.title('model '+i)\n",
    "        plt.ylabel(i.upper())\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='best')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # summarize history for learning rate\n",
    "    plt.plot(history.history['lr'])\n",
    "    plt.title('learning rate progress')\n",
    "    plt.ylabel('learning rate')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['learning rate'], loc='best')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, x_test, y_test, batch_size, forecast_period, last_date, y_scaler):\n",
    "    forecast_last_batch =  model.predict(x_predict, batch_size=batch_size)\n",
    "    forecast_last_batch =  y_scaler.inverse_transform(forecast_last_batch)\n",
    "    future_prediction = forecast_last_batch[-forecast_period:]\n",
    "    nyse = mcal.get_calendar('NYSE') # NYSE Calendar\n",
    "    schedule_nyse = nyse.schedule(last_date, today)[1:(forecast_period+2)]\n",
    "    prediction_days = list(pd.to_datetime(schedule_nyse.market_close.dt.to_pydatetime()).strftime('%Y-%m-%d'))    \n",
    "    data = yf.download(ticker, start=prediction_days[0], end=prediction_days[-1])[\"Adj Close\"]\n",
    "    data=pd.DataFrame(data)\n",
    "    data.rename(columns = {'Adj Close':'actual'}, inplace = True)\n",
    "    data['prediction']=future_prediction\n",
    "    data['difference']=data['actual']-data['prediction']\n",
    "    build_path('prediction')\n",
    "    data.to_csv('prediction/'+model_name+'.csv')\n",
    "    print('*'*80)\n",
    "    print(data)\n",
    "    data.plot(title='Prediction Accuracy')\n",
    "    print('*'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    model = load_model('best_model/'+model_name+'.h5')\n",
    "    print('*'*30+'BEST MODEL SUMMARY'+'*'*30)\n",
    "    model.summary()\n",
    "    \n",
    "    f = open('best_model/'+model_name+'_best_run.pickle', 'rb')# read best_run parameters from a pickle file\n",
    "    best_run = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    if not starting_epoch_one:\n",
    "        model.load_weights('train_model/checkpoint/'+model_name+'.ckpt')\n",
    "        print('Loaded weights from the latest checkpoint')\n",
    "        \n",
    "    model.compile(loss=best_run['loss_choice'],optimizer=best_run['optimizer_choice'], metrics = ['mse','mae','mape'])\n",
    "    \n",
    "    \n",
    "    save_best, save_ckpt, call_reduce, callback_lr, callback_es, callback_csv = get_callback(early_stopping_count)\n",
    "    tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "    callbacks_list = [save_best, save_ckpt, call_reduce, callback_es, callback_csv, tensorboard]\n",
    "    \n",
    "    \n",
    "    history = model.fit(x_train, y_train,batch_size=batch_size, epochs=training_epochs,\n",
    "                        verbose=training_verbose, validation_data=(x_test, y_test), callbacks=callbacks_list)\n",
    "        \n",
    "    \n",
    "    model = load_model('train_model/'+model_name+'.h5')\n",
    "\n",
    "    val_loss, val_mse, val_mae, val_mape = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print('val_loss: ', val_loss, ' val_mse: ',val_mse, ' val_mae: ',val_mae,  ' val_mape: ',val_mape)\n",
    "    draw_history(history)\n",
    "    prediction(model,x_test, y_test, batch_size, forecast_period, last_date, y_scaler)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HcxtusN4AK8y",
    "outputId": "43d151c2-cf3b-4ab8-e95f-7c48cef1da49",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data with a train_test_split of  0.85\n",
      "  0%|                                                                           | 0/20 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3313: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loss:                                                                                                                  \n",
      "[485.9803955078125, 414.75904388427733, 393.9103515625, 342.3159881591797, 325.40953674316404, 323.4288818359375, 321.93494720458983, 303.58861236572267, 284.57505264282224, 278.44033279418943, 294.18279724121095, 260.119921875, 277.7036277770996, 256.9703422546387, 239.364453125, 245.2772331237793, 227.49541473388672, 244.81956405639647, 215.2033836364746, 212.07696456909179]\n",
      "Val loss:                                                                                                              \n",
      "[654.2720031738281, 505.5592346191406, 398.4322916666667, 784.0983479817709, 668.1209208170573, 403.8869934082031, 578.5261637369791, 431.52989196777344, 690.1323954264323, 508.74009704589844, 408.34368387858075, 314.65253194173175, 404.54345703125, 266.222407023112, 298.01877848307294, 166.94737243652344, 226.8248494466146, 235.79412333170572, 179.31102498372397, 222.03307088216147]\n",
      "Mse:                                                                                                                   \n",
      "[1.5457572, 0.5741332, 0.51557255, 0.49042076, 0.48702723, 0.4531215, 0.44770807, 0.44790325, 0.40614444, 0.39383656, 0.37514254, 0.3842751, 0.3805747, 0.35204303, 0.3491132, 0.33924142, 0.33678415, 0.32741085, 0.33111867, 0.31089002]\n",
      "Val_mse:                                                                                                               \n",
      "[0.1998923420906067, 0.1259458214044571, 0.13891148567199707, 0.13616515696048737, 0.1211400032043457, 0.10796526819467545, 0.12218678742647171, 0.12830233573913574, 0.11094788461923599, 0.11143442988395691, 0.10420346260070801, 0.15106801688671112, 0.15065279603004456, 0.11357999593019485, 0.09837403148412704, 0.10839033871889114, 0.08394620567560196, 0.09463048726320267, 0.09588924050331116, 0.10943376272916794]\n",
      "Mae:                                                                                                                   \n",
      "[0.89655846, 0.5874764, 0.54490745, 0.5260573, 0.5206609, 0.5070387, 0.49015865, 0.49255028, 0.4589246, 0.45342875, 0.44414347, 0.44252214, 0.4408968, 0.43171057, 0.41803437, 0.41029936, 0.40634292, 0.39381427, 0.40074533, 0.3859443]\n",
      "Val_mae:                                                                                                               \n",
      "[0.89655846, 0.5874764, 0.54490745, 0.5260573, 0.5206609, 0.5070387, 0.49015865, 0.49255028, 0.4589246, 0.45342875, 0.44414347, 0.44252214, 0.4408968, 0.43171057, 0.41803437, 0.41029936, 0.40634292, 0.39381427, 0.40074533, 0.3859443]\n",
      "Mape:                                                                                                                  \n",
      "[313.75015, 239.73174, 219.63437, 169.9895, 155.87222, 157.13632, 159.07584, 143.93565, 127.95023, 125.15906, 144.21643, 113.526405, 134.53677, 116.96448, 102.428406, 111.48257, 96.8071, 116.97886, 89.8889, 89.37685]\n",
      "Val_mape:                                                                                                              \n",
      "[479.67431640625, 330.6614685058594, 225.10023498535156, 613.0720825195312, 500.2732849121094, 239.36805725097656, 417.5065002441406, 273.3537292480469, 535.2431030273438, 357.2210388183594, 260.1084899902344, 169.9041748046875, 263.1206359863281, 127.80660247802734, 162.70143127441406, 34.8049430847168, 97.65995025634766, 109.32025146484375, 55.3354377746582, 100.74810791015625]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.00095, 0.00095, 0.00095, 0.0009025, 0.0009025, 0.0009025, 0.0009025, 0.0009025, 0.0009025, 0.0009025, 0.0009025, 0.0009025, 0.0009025, 0.000857375]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[49.2462194442749, 30.119433975219728, 20.525782299041747, 15.304246473312379, 12.57686381340027, 11.090647411346435, 10.202120637893676, 9.56580982208252, 9.039675521850587, 8.575456190109254, 8.13499619960785, 7.717058706283569, 7.328792214393616, 6.925394749641418, 6.559954881668091, 6.183741044998169, 5.837525749206543, 5.486329245567322, 5.170908355712891, 4.836506485939026]\n",
      "Val loss:                                                                                                              \n",
      "[36.748504638671875, 24.246477127075195, 17.39643096923828, 13.680818875630697, 11.786788940429688, 10.725934346516928, 9.925070762634277, 9.428805351257324, 8.893792470296225, 8.440296173095703, 8.024381001790365, 7.592765649159749, 7.238510608673096, 6.833899021148682, 6.5011067390441895, 6.06367842356364, 5.751525243123372, 5.396274248758952, 5.066228866577148, 4.68507194519043]\n",
      "Mse:                                                                                                                   \n",
      "[1.5199507, 1.2718008, 1.3565644, 1.4147902, 1.5898408, 1.5713084, 1.5095468, 1.6912054, 1.7843863, 1.6689785, 1.5380342, 1.5542448, 1.4298521, 1.4455795, 1.3597072, 1.2151163, 1.2008674, 1.0734584, 1.0784959, 0.89512384]\n",
      "Val_mse:                                                                                                               \n",
      "[0.5858791470527649, 0.6478586196899414, 0.9230647087097168, 0.840472936630249, 0.9452961087226868, 1.092991590499878, 0.9151291847229004, 1.0407928228378296, 0.8466120362281799, 0.7118513584136963, 0.7936258316040039, 0.6559226512908936, 0.8602730631828308, 0.7118897438049316, 0.8542611598968506, 0.5489292144775391, 0.6556499004364014, 0.5234351754188538, 0.4610103368759155, 0.3971606194972992]\n",
      "Mae:                                                                                                                   \n",
      "[0.9669777, 0.89624226, 0.9157399, 0.9367684, 0.99613905, 0.99736226, 0.9746825, 0.9964506, 0.9854121, 0.95527095, 0.9160798, 0.9144065, 0.8969698, 0.8702545, 0.8704956, 0.7967987, 0.8195795, 0.765186, 0.76082766, 0.6970378]\n",
      "Val_mae:                                                                                                               \n",
      "[0.9669777, 0.89624226, 0.9157399, 0.9367684, 0.99613905, 0.99736226, 0.9746825, 0.9964506, 0.9854121, 0.95527095, 0.9160798, 0.9144065, 0.8969698, 0.8702545, 0.8704956, 0.7967987, 0.8195795, 0.765186, 0.76082766, 0.6970378]\n",
      "Mape:                                                                                                                  \n",
      "[423.97958, 376.04785, 384.9105, 414.0197, 425.8834, 393.4371, 430.29956, 417.10318, 342.89304, 367.4336, 312.12238, 351.74872, 391.88962, 315.83337, 361.64432, 351.664, 295.29633, 311.53537, 298.45526, 226.7887]\n",
      "Val_mape:                                                                                                              \n",
      "[753.5488891601562, 1056.2421875, 1518.6197509765625, 1955.6290283203125, 1843.3092041015625, 1773.93115234375, 2403.42578125, 2003.0814208984375, 1756.0263671875, 1692.1123046875, 1827.6539306640625, 1090.2900390625, 1617.69287109375, 2014.84423828125, 1145.6326904296875, 1307.185791015625, 1373.4498291015625, 1077.8177490234375, 794.264404296875, 678.3622436523438]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:                                                                                                                  \n",
      "[314.0309326171875, 300.26894226074216, 287.10200653076174, 274.5917022705078, 262.6573913574219, 251.28655853271485, 240.44821014404297, 230.11757125854493, 220.25692825317384, 210.86374130249024, 201.9003593444824, 193.3476905822754, 185.18245391845704, 177.38620147705078, 169.93376846313475, 162.82268142700195, 156.01848907470702, 149.53384399414062, 143.29603881835936, 137.35805130004883]\n",
      "Val loss:                                                                                                              \n",
      "[306.5272928873698, 293.0316975911458, 280.2939453125, 268.15943400065106, 256.56500244140625, 245.49776204427084, 234.97582499186197, 224.90339152018228, 215.3416544596354, 206.1786905924479, 197.42593383789062, 189.05878194173178, 181.05925496419272, 173.44266764322916, 166.1960245768229, 159.24222819010416, 152.52223205566406, 146.16539510091147, 140.08919270833334, 134.3387654622396]\n",
      "Mse:                                                                                                                   \n",
      "[1.4643533, 1.1241996, 0.76537, 0.58331746, 0.41933125, 0.30007786, 0.21691683, 0.15906975, 0.11040298, 0.08636967, 0.06829745, 0.05817572, 0.051875256, 0.04700333, 0.04150505, 0.042668797, 0.038952596, 0.04712238, 0.03559644, 0.037936937]\n",
      "Val_mse:                                                                                                               \n",
      "[0.7551164627075195, 0.45786526799201965, 0.4203254282474518, 0.3750447928905487, 0.3123941719532013, 0.24762289226055145, 0.22537891566753387, 0.1813824623823166, 0.18335551023483276, 0.1573469191789627, 0.13158519566059113, 0.10517590492963791, 0.07537664473056793, 0.0648200735449791, 0.07272133231163025, 0.06512462347745895, 0.02641591615974903, 0.0241371039301157, 0.02026655711233616, 0.040216896682977676]\n",
      "Mae:                                                                                                                   \n",
      "[0.9318466, 0.83963096, 0.7005923, 0.60887444, 0.51492345, 0.43579167, 0.36868757, 0.3149408, 0.26161012, 0.22981867, 0.20421124, 0.18659875, 0.17367122, 0.16569778, 0.15511152, 0.15615413, 0.15036692, 0.16520172, 0.14224401, 0.14794275]\n",
      "Val_mae:                                                                                                               \n",
      "[0.9318466, 0.83963096, 0.7005923, 0.60887444, 0.51492345, 0.43579167, 0.36868757, 0.3149408, 0.26161012, 0.22981867, 0.20421124, 0.18659875, 0.17367122, 0.16569778, 0.15511152, 0.15615413, 0.15036692, 0.16520172, 0.14224401, 0.14794275]\n",
      "Mape:                                                                                                                  \n",
      "[341.4864, 294.4776, 261.2295, 225.94629, 176.60583, 184.84402, 113.44441, 116.60115, 103.31984, 74.70978, 72.86374, 67.395935, 63.62141, 68.82681, 62.07006, 62.086494, 54.667114, 58.40039, 58.929436, 53.25959]\n",
      "Val_mape:                                                                                                              \n",
      "[1042.1102294921875, 917.6835327148438, 765.8240356445312, 656.9949951171875, 547.0540771484375, 406.4917297363281, 377.4765930175781, 301.0549011230469, 232.77191162109375, 176.8582000732422, 129.5276336669922, 123.81781005859375, 109.33007049560547, 113.05810546875, 89.30947875976562, 110.809326171875, 107.31963348388672, 68.30254364013672, 69.9496078491211, 61.69255447387695]\n",
      "Lr:                                                                                                                    \n",
      "[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[40.2019603729248, 40.14788131713867, 40.02400646209717, 39.92040023803711, 39.80498733520508, 39.71835556030273, 39.612735176086424, 39.4938627243042, 39.403014183044434, 39.29953384399414, 39.199870109558105, 39.11549453735351, 39.001485443115236, 38.89212417602539, 38.79875888824463, 38.68769588470459, 38.61136264801026, 38.51208992004395, 38.42177505493164, 38.309186935424805]\n",
      "Val loss:                                                                                                              \n",
      "[39.66381327311198, 39.534114837646484, 39.41235097249349, 39.292772928873696, 39.17960993448893, 39.067763010660805, 38.95812479654948, 38.849327087402344, 38.74108632405599, 38.640061696370445, 38.53532918294271, 38.443339029947914, 38.349283854166664, 38.258365631103516, 38.16710408528646, 38.07796732584635, 37.986602783203125, 37.89546457926432, 37.804115295410156, 37.71296310424805]\n",
      "Mse:                                                                                                                   \n",
      "[2.5996587, 2.6826756, 2.5140746, 2.4642615, 2.3477287, 2.368755, 2.2979782, 2.2047136, 2.193319, 2.1615772, 2.1158338, 2.1669147, 2.0774767, 2.0218596, 2.0232875, 1.9569212, 2.0104575, 2.014606, 2.014155, 1.9617475]\n",
      "Val_mse:                                                                                                               \n",
      "[0.8713909983634949, 0.7460370659828186, 0.6514883041381836, 0.5674611926078796, 0.5016215443611145, 0.4450972080230713, 0.3957862854003906, 0.35406196117401123, 0.3158896267414093, 0.29696667194366455, 0.2712232172489166, 0.27386078238487244, 0.27297982573509216, 0.2792663276195526, 0.28509172797203064, 0.2964698374271393, 0.30305778980255127, 0.3111349940299988, 0.31884297728538513, 0.32700684666633606]\n",
      "Mae:                                                                                                                   \n",
      "[1.2738854, 1.3058106, 1.2652943, 1.247989, 1.2205237, 1.2233737, 1.2081199, 1.177552, 1.1790631, 1.1650193, 1.159138, 1.1651009, 1.1440003, 1.1252333, 1.1275145, 1.1096046, 1.1276097, 1.1228021, 1.129094, 1.1069833]\n",
      "Val_mae:                                                                                                               \n",
      "[1.2738854, 1.3058106, 1.2652943, 1.247989, 1.2205237, 1.2233737, 1.2081199, 1.177552, 1.1790631, 1.1650193, 1.159138, 1.1651009, 1.1440003, 1.1252333, 1.1275145, 1.1096046, 1.1276097, 1.1228021, 1.129094, 1.1069833]\n",
      "Mape:                                                                                                                  \n",
      "[442.5999, 464.0246, 444.11313, 472.0543, 446.53314, 429.45367, 434.63974, 439.00577, 469.79312, 491.18402, 443.93365, 469.489, 480.28265, 407.62344, 456.83707, 429.67123, 420.6219, 454.3156, 377.66452, 431.91397]\n",
      "Val_mape:                                                                                                              \n",
      "[404.3817138671875, 400.5219421386719, 390.0147705078125, 371.8580017089844, 355.395751953125, 341.7897033691406, 318.6531677246094, 301.6042175292969, 282.5580139160156, 255.470947265625, 219.1544952392578, 203.94679260253906, 168.74468994140625, 146.5559844970703, 175.66583251953125, 198.0465545654297, 219.6057891845703, 240.30938720703125, 272.4123229980469, 293.6795349121094]\n",
      "Lr:                                                                                                                    \n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]                   \n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[44.418439865112305, 24.429782485961915, 13.563175773620605, 7.884562563896179, 4.875484812259674, 3.2440248489379884, 2.327924430370331, 1.798305368423462, 1.4775786519050598, 1.273432159423828, 1.1334768772125243, 1.036533373594284, 0.9590425997972488, 0.895755997300148, 0.8430644392967224, 0.794075894355774, 0.7503452211618423, 0.7099809557199478, 0.6779522478580475, 0.6378383129835129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:                                                                                                              \n",
      "[32.051831563313804, 17.566436767578125, 9.98985226949056, 6.017335891723633, 3.900998036066691, 2.7579867045084634, 2.085484266281128, 1.7075944741566975, 1.4478883345921834, 1.293935736020406, 1.1924845774968464, 1.0936411619186401, 1.0326637427012126, 0.982049564520518, 0.9388792514801025, 0.8919219374656677, 0.8367401560147604, 0.7963147759437561, 0.7583038608233134, 0.7422115604082743]\n",
      "Mse:                                                                                                                   \n",
      "[1.0645926, 0.8183649, 0.73072594, 0.7555658, 0.7459881, 0.8099779, 0.8177347, 0.7799075, 0.75833464, 0.77186835, 0.72277415, 0.7787603, 0.7415545, 0.68363744, 0.6877495, 0.69167256, 0.66029984, 0.5875119, 0.5804838, 0.582873]\n",
      "Val_mse:                                                                                                               \n",
      "[0.28951677680015564, 0.23965342342853546, 0.27611875534057617, 0.31618836522102356, 0.3775893449783325, 0.4794919490814209, 0.4532202482223511, 0.5040426254272461, 0.47813189029693604, 0.4963955581188202, 0.5420535206794739, 0.4704063832759857, 0.5199005603790283, 0.5293594002723694, 0.557582437992096, 0.5452657341957092, 0.5132890343666077, 0.5406382083892822, 0.49829593300819397, 0.5315036177635193]\n",
      "Mae:                                                                                                                   \n",
      "[0.80148256, 0.7165524, 0.6646436, 0.66598356, 0.65303624, 0.66391957, 0.65830946, 0.6476275, 0.62882096, 0.6199025, 0.5968451, 0.6086939, 0.5947467, 0.5667803, 0.5657099, 0.5549589, 0.54209477, 0.508782, 0.52211636, 0.516188]\n",
      "Val_mae:                                                                                                               \n",
      "[0.80148256, 0.7165524, 0.6646436, 0.66598356, 0.65303624, 0.66391957, 0.65830946, 0.6476275, 0.62882096, 0.6199025, 0.5968451, 0.6086939, 0.5947467, 0.5667803, 0.5657099, 0.5549589, 0.54209477, 0.508782, 0.52211636, 0.516188]\n",
      "Mape:                                                                                                                  \n",
      "[333.65057, 292.4759, 274.4095, 286.45587, 221.98117, 238.26152, 264.36386, 182.63857, 224.73515, 188.72987, 170.72672, 161.83916, 163.74576, 134.1088, 135.07466, 133.53183, 123.85515, 109.30597, 120.845215, 122.720215]\n",
      "Val_mape:                                                                                                              \n",
      "[1692.2218017578125, 1262.0863037109375, 1337.1824951171875, 928.6365356445312, 990.3788452148438, 1015.3390502929688, 929.8313598632812, 877.7175903320312, 873.6637573242188, 899.7771606445312, 891.81201171875, 724.2918090820312, 962.9530639648438, 782.431396484375, 911.7337036132812, 876.9900512695312, 948.7344360351562, 884.05615234375, 870.169189453125, 769.2140502929688]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[145.15692291259765, 144.5476531982422, 143.87322540283202, 143.23149337768555, 142.6014190673828, 141.96007919311523, 141.32249069213867, 140.68579483032227, 140.0417823791504, 139.38922271728515, 138.74327087402344, 138.0810317993164, 137.42687072753907, 136.76646728515624, 136.10314254760743, 135.43954162597657, 134.76454162597656, 134.08677825927734, 133.4149085998535, 132.74184951782226]\n",
      "Val loss:                                                                                                              \n",
      "[144.70744832356772, 144.05018615722656, 143.39518229166666, 142.7582753499349, 142.12210591634116, 141.4864756266276, 140.8505096435547, 140.2064005533854, 139.56670125325522, 138.9203084309896, 138.27472941080728, 137.6263682047526, 136.97298685709634, 136.31800333658853, 135.65387471516928, 134.98811848958334, 134.3190155029297, 133.64554341634116, 132.97266133626303, 132.29401143391928]\n",
      "Mse:                                                                                                                   \n",
      "[1.3534329, 1.09567, 0.81935817, 0.69938725, 0.63677514, 0.56063575, 0.522535, 0.49697724, 0.4720466, 0.43625322, 0.41717094, 0.3895399, 0.37857443, 0.36575767, 0.34914652, 0.3417493, 0.3236064, 0.30897045, 0.30454427, 0.30067438]\n",
      "Val_mse:                                                                                                               \n",
      "[0.8672818541526794, 0.6750709414482117, 0.5222949981689453, 0.4231412410736084, 0.3626821041107178, 0.321279376745224, 0.2905685603618622, 0.25971338152885437, 0.2447177767753601, 0.2301655262708664, 0.22627699375152588, 0.22133231163024902, 0.2169886827468872, 0.21557195484638214, 0.2083965539932251, 0.20233039557933807, 0.19720101356506348, 0.1919238418340683, 0.19085966050624847, 0.18596501648426056]\n",
      "Mae:                                                                                                                   \n",
      "[0.91019166, 0.82612354, 0.715429, 0.65601707, 0.6227142, 0.58809984, 0.5650664, 0.55036855, 0.53412807, 0.5134632, 0.505616, 0.48656344, 0.47908694, 0.47025412, 0.46205336, 0.45634732, 0.44415188, 0.4327446, 0.42970783, 0.42849192]\n",
      "Val_mae:                                                                                                               \n",
      "[0.91019166, 0.82612354, 0.715429, 0.65601707, 0.6227142, 0.58809984, 0.5650664, 0.55036855, 0.53412807, 0.5134632, 0.505616, 0.48656344, 0.47908694, 0.47025412, 0.46205336, 0.45634732, 0.44415188, 0.4327446, 0.42970783, 0.42849192]\n",
      "Mape:                                                                                                                  \n",
      "[268.73547, 319.46762, 276.99063, 288.02124, 307.1855, 212.84897, 240.76123, 204.26346, 223.0878, 244.40054, 205.45535, 211.17639, 200.17877, 181.72513, 236.72368, 192.2691, 189.68631, 156.32658, 182.17117, 204.85713]\n",
      "Val_mape:                                                                                                              \n",
      "[270.7003173828125, 283.592529296875, 284.0177307128906, 276.2834777832031, 229.26641845703125, 158.069091796875, 180.1455841064453, 201.3380126953125, 184.2247314453125, 188.6259765625, 177.08738708496094, 175.08384704589844, 153.00323486328125, 165.70741271972656, 202.19708251953125, 232.672607421875, 241.2372283935547, 291.2806701660156, 316.5462341308594, 326.9350280761719]\n",
      "Lr:                                                                                                                    \n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]                   \n",
      "******************************Next Evaluation Results******************************                                    \n",
      " 30%|                                 | 6/20 [05:35<13:19, 57.12s/trial, best loss: 0.02026655711233616]WARNING:tensorflow:Large dropout rate: 0.694427 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.694427 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Loss:                                                                                                                  \n",
      "[108.86074180603028, 108.23422813415527, 107.68575096130371, 107.1713436126709, 106.65987358093261, 106.18324279785156, 105.67078742980956, 105.1803825378418, 104.69217567443847, 104.20857810974121, 103.71982078552246, 103.20546646118164, 102.73476638793946, 102.21402740478516, 101.74227561950684, 101.23611373901367, 100.7399185180664, 100.23650169372559, 99.75234298706054, 99.25848693847657]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:                                                                                                              \n",
      "[107.99345143636067, 107.47378540039062, 106.93971761067708, 106.40266418457031, 105.87095896402995, 105.35211435953777, 104.85089619954427, 104.36595153808594, 103.90061950683594, 103.45005798339844, 103.00314839680989, 102.55910237630208, 102.10014343261719, 101.64742533365886, 101.20035298665364, 100.74251302083333, 100.26742299397786, 99.78666432698567, 99.29826100667317, 98.81280771891277]\n",
      "Mse:                                                                                                                   \n",
      "[3.4172928, 2.7127662, 2.3331153, 2.1771102, 2.0251632, 1.9655329, 1.8789022, 1.8049548, 1.7742332, 1.7748015, 1.7411032, 1.6397156, 1.6932547, 1.5906333, 1.6179295, 1.5932503, 1.5827625, 1.5331155, 1.5681908, 1.5517541]\n",
      "Val_mse:                                                                                                               \n",
      "[0.9833210110664368, 0.8507037162780762, 0.7101871967315674, 0.5785020589828491, 0.48007118701934814, 0.43133091926574707, 0.42425575852394104, 0.4426437318325043, 0.47468921542167664, 0.5258633494377136, 0.5841054320335388, 0.6460142731666565, 0.6859686970710754, 0.7357792258262634, 0.8008675575256348, 0.8523551821708679, 0.8849063515663147, 0.9056503176689148, 0.9160487651824951, 0.9353532791137695]\n",
      "Mae:                                                                                                                   \n",
      "[1.4848659, 1.3082818, 1.2209696, 1.170731, 1.1270331, 1.1200927, 1.0793262, 1.0638697, 1.0528357, 1.0478406, 1.0399954, 1.0088269, 1.0225903, 0.9883934, 1.0038979, 0.9864542, 0.9805374, 0.96904576, 0.9784409, 0.9794059]\n",
      "Val_mae:                                                                                                               \n",
      "[1.4848659, 1.3082818, 1.2209696, 1.170731, 1.1270331, 1.1200927, 1.0793262, 1.0638697, 1.0528357, 1.0478406, 1.0399954, 1.0088269, 1.0225903, 0.9883934, 1.0038979, 0.9864542, 0.9805374, 0.96904576, 0.9784409, 0.9794059]\n",
      "Mape:                                                                                                                  \n",
      "[449.07324, 447.23154, 454.7059, 414.33447, 342.44934, 372.53656, 393.3835, 364.9034, 372.09387, 473.41083, 319.20926, 383.91412, 349.21277, 365.0133, 338.11005, 393.1653, 376.25494, 313.96353, 369.22028, 392.4413]\n",
      "Val_mape:                                                                                                              \n",
      "[353.4546203613281, 428.8326721191406, 483.8724365234375, 524.8054809570312, 580.1420288085938, 596.2947387695312, 613.0820922851562, 653.9639282226562, 671.9513549804688, 713.34912109375, 767.7379760742188, 776.6394653320312, 791.6953125, 802.9660034179688, 807.1521606445312, 789.7697143554688, 802.0311889648438, 807.6705932617188, 803.4462890625, 799.6670532226562]\n",
      "Lr:                                                                                                                    \n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]                   \n",
      "******************************Next Evaluation Results******************************                                    \n",
      " 35%|                               | 7/20 [06:34<12:29, 57.65s/trial, best loss: 0.02026655711233616]WARNING:tensorflow:Large dropout rate: 0.564933 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.564933 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Loss:                                                                                                                  \n",
      "[159.82175674438477, 97.4326774597168, 62.705387115478516, 44.37131004333496, 34.42609691619873, 28.742482280731203, 25.306982612609865, 23.039853668212892, 21.418826293945312, 20.144198608398437, 19.105558776855467, 18.120130443572997, 17.291128158569336, 16.530265522003173, 15.675441026687622, 14.913921308517455, 14.222924947738647, 13.533902406692505, 12.87505807876587, 12.372534132003784]\n",
      "Val loss:                                                                                                              \n",
      "[121.17642211914062, 75.2287368774414, 50.93510309855143, 38.09791692097982, 30.899030685424805, 26.680999120076496, 24.041191736857098, 22.203282674153645, 20.81986935933431, 19.682056427001953, 18.708948771158855, 17.88121287027995, 17.021704991658527, 16.205437978108723, 15.364282925923666, 14.635665575663248, 14.011025746663412, 13.314713795979818, 12.664423942565918, 12.11444091796875]\n",
      "Mse:                                                                                                                   \n",
      "[1.3706172, 0.92635375, 0.66279095, 0.57447207, 0.49105245, 0.41437697, 0.38464555, 0.34273368, 0.3254897, 0.30403274, 0.31155005, 0.25088325, 0.23963678, 0.21396646, 0.20516662, 0.17935507, 0.19396552, 0.17917606, 0.1486761, 0.15258463]\n",
      "Val_mse:                                                                                                               \n",
      "[0.17081904411315918, 0.16668598353862762, 0.16693024337291718, 0.2793348431587219, 0.270877480506897, 0.286294549703598, 0.2928888499736786, 0.31011319160461426, 0.3063923120498657, 0.3119131028652191, 0.31690144538879395, 0.3009370267391205, 0.34838104248046875, 0.25071975588798523, 0.19874775409698486, 0.1848113089799881, 0.2162976711988449, 0.21188098192214966, 0.1565285474061966, 0.09166723489761353]\n",
      "Mae:                                                                                                                   \n",
      "[0.92157745, 0.75285506, 0.62670624, 0.56912524, 0.5088578, 0.44935018, 0.41486025, 0.37063673, 0.35319752, 0.34687454, 0.3446588, 0.304684, 0.2799457, 0.30783078, 0.26973197, 0.24708104, 0.25564453, 0.24135932, 0.22739677, 0.24535725]\n",
      "Val_mae:                                                                                                               \n",
      "[0.92157745, 0.75285506, 0.62670624, 0.56912524, 0.5088578, 0.44935018, 0.41486025, 0.37063673, 0.35319752, 0.34687454, 0.3446588, 0.304684, 0.2799457, 0.30783078, 0.26973197, 0.24708104, 0.25564453, 0.24135932, 0.22739677, 0.24535725]\n",
      "Mape:                                                                                                                  \n",
      "[329.37, 234.39859, 207.22275, 159.8575, 144.03537, 121.85061, 98.363625, 95.00256, 79.3005, 93.86256, 141.96562, 97.25615, 86.87147, 80.14241, 68.499176, 59.233818, 64.40687, 53.214622, 50.7477, 62.429943]\n",
      "Val_mape:                                                                                                              \n",
      "[203.1543731689453, 478.236083984375, 414.4508361816406, 365.1307067871094, 320.27587890625, 390.0927429199219, 308.2437744140625, 428.05810546875, 348.9843444824219, 128.9799041748047, 180.0062713623047, 88.60110473632812, 86.94286346435547, 182.0603790283203, 119.64997100830078, 96.19509887695312, 130.2947540283203, 86.56937408447266, 77.45635223388672, 82.85163116455078]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      " 40%|                            | 8/20 [07:34<11:40, 58.35s/trial, best loss: 0.02026655711233616]WARNING:tensorflow:Large dropout rate: 0.773429 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Loss:                                                                                                                  \n",
      "[45.49210414886475, 25.89138870239258, 14.86610803604126, 8.934495091438293, 5.716928267478943, 3.9060737013816835, 2.877915632724762, 2.2575565695762636, 1.8561935245990753, 1.5923399686813355, 1.4114895582199096, 1.272885274887085, 1.1749900043010713, 1.0896933615207671, 1.003104716539383, 0.9325623363256454, 0.8729450285434723, 0.8189893245697022, 0.7729470431804657, 0.7358447253704071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss:                                                                                                              \n",
      "[33.1224110921224, 18.704784393310547, 10.93343702952067, 6.734041055043538, 4.432867050170898, 3.1403802235921225, 2.4047493934631348, 1.9692003726959229, 1.6267188787460327, 1.4262397289276123, 1.2971781094868977, 1.1714774370193481, 1.0892781813939412, 1.0002663532892864, 0.9258879025777181, 0.8815242449442545, 0.8228152791659037, 0.7738394141197205, 0.7156165440877279, 0.6842580238978068]\n",
      "Mse:                                                                                                                   \n",
      "[9.711229, 9.611502, 8.325483, 7.3859406, 6.6266556, 5.7231674, 5.506531, 5.0684404, 4.4754524, 4.179541, 3.7137322, 3.4394462, 3.1938381, 2.9610887, 2.7268293, 2.435762, 2.2445905, 2.310123, 2.1857975, 2.1994348]\n",
      "Val_mse:                                                                                                               \n",
      "[0.2887193262577057, 0.3066789209842682, 0.4262235164642334, 0.43202924728393555, 0.3690955340862274, 0.5022854208946228, 0.8348624110221863, 1.1296173334121704, 1.0663424730300903, 1.0231995582580566, 1.0151118040084839, 0.8565575480461121, 0.7355244159698486, 0.6219095587730408, 0.7877082824707031, 0.8584480285644531, 0.7044928669929504, 0.6874611973762512, 0.5163989067077637, 0.43782344460487366]\n",
      "Mae:                                                                                                                   \n",
      "[2.3191113, 2.2728133, 2.0968099, 1.9545752, 1.8466021, 1.730128, 1.6536642, 1.6124655, 1.4875655, 1.4396782, 1.3786178, 1.3294604, 1.3088428, 1.2277886, 1.1431093, 1.070914, 1.0025643, 0.9825193, 0.95001954, 0.9268468]\n",
      "Val_mae:                                                                                                               \n",
      "[2.3191113, 2.2728133, 2.0968099, 1.9545752, 1.8466021, 1.730128, 1.6536642, 1.6124655, 1.4875655, 1.4396782, 1.3786178, 1.3294604, 1.3088428, 1.2277886, 1.1431093, 1.070914, 1.0025643, 0.9825193, 0.95001954, 0.9268468]\n",
      "Mape:                                                                                                                  \n",
      "[834.7279, 897.39905, 740.8106, 725.7595, 933.3678, 685.8896, 653.1136, 600.1105, 648.31964, 674.6686, 672.82965, 683.2544, 600.76715, 727.3834, 574.0508, 566.28577, 368.4084, 307.59058, 352.64658, 245.18396]\n",
      "Val_mape:                                                                                                              \n",
      "[1800.86474609375, 1405.2210693359375, 2032.162109375, 2159.522705078125, 2570.91455078125, 2308.400390625, 2772.482421875, 2857.7998046875, 2607.154541015625, 2758.693603515625, 2770.419921875, 3037.922119140625, 2896.815185546875, 3723.3759765625, 3519.3916015625, 3122.137939453125, 2519.40771484375, 2258.925537109375, 743.7639770507812, 574.7852783203125]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[250.25150604248046, 134.95907135009764, 69.92072200775146, 37.5498869895935, 21.884726428985594, 14.226538038253784, 10.354355907440185, 8.255418992042541, 7.000002837181091, 6.1722979068756105, 5.558152747154236, 5.079710507392884, 4.686632800102234, 4.352495718002319, 4.055230748653412, 3.7981574892997743, 3.5615922808647156, 3.3477182269096373, 3.155198109149933, 2.978560185432434]\n",
      "Val loss:                                                                                                              \n",
      "[180.81684366861978, 93.72760009765625, 49.28555425008138, 27.72382926940918, 17.270943959554035, 12.117576281229654, 9.520751953125, 7.989234924316406, 6.972392241160075, 6.333997885386149, 5.743510882059733, 5.329165776570638, 4.941340605417888, 4.595372676849365, 4.36356242497762, 4.002306699752808, 3.7488345305124917, 3.6648637453715005, 3.3723594347635903, 3.2266581853230796]\n",
      "Mse:                                                                                                                   \n",
      "[0.8397525, 0.31175023, 0.22367191, 0.18874176, 0.18161543, 0.1604619, 0.1493719, 0.13867494, 0.13118565, 0.12727273, 0.121008396, 0.11658971, 0.11548136, 0.10855593, 0.102516726, 0.100038014, 0.09390254, 0.091671154, 0.088933386, 0.08934494]\n",
      "Val_mse:                                                                                                               \n",
      "[0.3815227448940277, 0.28652286529541016, 0.3488948345184326, 0.42598751187324524, 0.4480304419994354, 0.48648783564567566, 0.6053689122200012, 0.6225844025611877, 0.5838465094566345, 0.6362378001213074, 0.5734462738037109, 0.5839248895645142, 0.5490179657936096, 0.5097634196281433, 0.5482764840126038, 0.4293476641178131, 0.39672234654426575, 0.5105795860290527, 0.40145182609558105, 0.42646804451942444]\n",
      "Mae:                                                                                                                   \n",
      "[0.6531604, 0.43873554, 0.37053, 0.33895415, 0.33090243, 0.3089229, 0.29682088, 0.28465104, 0.27725866, 0.27238324, 0.26416105, 0.2605623, 0.25764674, 0.24743876, 0.24053021, 0.23645261, 0.23062292, 0.22444017, 0.22525045, 0.22217855]\n",
      "Val_mae:                                                                                                               \n",
      "[0.6531604, 0.43873554, 0.37053, 0.33895415, 0.33090243, 0.3089229, 0.29682088, 0.28465104, 0.27725866, 0.27238324, 0.26416105, 0.2605623, 0.25764674, 0.24743876, 0.24053021, 0.23645261, 0.23062292, 0.22444017, 0.22525045, 0.22217855]\n",
      "Mape:                                                                                                                  \n",
      "[217.62761, 197.00241, 147.24849, 123.310135, 143.5966, 129.66829, 100.98677, 112.1901, 100.567856, 96.291435, 112.933304, 79.833694, 82.353935, 88.53164, 90.170395, 84.53028, 69.25279, 80.58415, 69.13739, 63.35688]\n",
      "Val_mape:                                                                                                              \n",
      "[735.0899047851562, 169.03257751464844, 346.9132080078125, 715.2348022460938, 790.690185546875, 943.7979125976562, 1005.0100708007812, 913.9560546875, 974.446533203125, 917.7421875, 935.1863403320312, 852.1445922851562, 792.4501342773438, 764.71826171875, 695.5874633789062, 687.41748046875, 659.2333374023438, 454.3255310058594, 522.4495849609375, 491.6500244140625]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[970.3915496826172, 871.0428894042968, 815.0993530273438, 829.5032287597656, 902.9424926757813, 851.7687438964844, 780.3750366210937, 839.94130859375, 823.4470489501953, 855.9452056884766, 866.395361328125, 813.5710540771485, 797.9080413818359, 819.6270416259765, 815.9806884765625, 824.366342163086, 793.1175689697266, 764.9992828369141, 785.8645782470703, 794.508203125]\n",
      "Val loss:                                                                                                              \n",
      "[1077.7926839192708, 1070.4119669596355, 1095.5701700846355, 1225.130350748698, 1277.3412679036458, 1362.102762858073, 1415.4383544921875, 1477.0874633789062, 1576.987528483073, 1611.892333984375, 1652.4505818684895, 1609.3305257161458, 1614.493896484375, 1648.2582600911458, 1611.130839029948, 1666.6168619791667, 1630.6208902994792, 1592.5946655273438, 1566.8103434244792, 1616.1540934244792]\n",
      "Mse:                                                                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.663056, 2.0244787, 1.9037796, 1.794919, 1.7792494, 1.7523549, 1.7278614, 1.7035166, 1.7053057, 1.6606023, 1.6401749, 1.5943793, 1.6137056, 1.6101315, 1.6020391, 1.5733923, 1.5450265, 1.5373762, 1.5078132, 1.5322034]\n",
      "Val_mse:                                                                                                               \n",
      "[0.8630578517913818, 0.8556058406829834, 0.819758415222168, 0.793248176574707, 0.7662555575370789, 0.7490038871765137, 0.7344959378242493, 0.735968291759491, 0.7467411160469055, 0.7566490769386292, 0.7759200930595398, 0.7946112155914307, 0.8203921914100647, 0.852923572063446, 0.8791443705558777, 0.9089546203613281, 0.9296897053718567, 0.954664409160614, 0.9789884090423584, 0.9994144439697266]\n",
      "Mae:                                                                                                                   \n",
      "[1.0304573, 1.1236093, 1.091487, 1.0555145, 1.049465, 1.0470104, 1.034059, 1.0268279, 1.0296863, 1.0150969, 1.0116762, 0.99670345, 1.0039016, 1.0050184, 1.0004165, 0.99620837, 0.9852029, 0.980618, 0.9690803, 0.9840646]\n",
      "Val_mae:                                                                                                               \n",
      "[1.0304573, 1.1236093, 1.091487, 1.0555145, 1.049465, 1.0470104, 1.034059, 1.0268279, 1.0296863, 1.0150969, 1.0116762, 0.99670345, 1.0039016, 1.0050184, 1.0004165, 0.99620837, 0.9852029, 0.980618, 0.9690803, 0.9840646]\n",
      "Mape:                                                                                                                  \n",
      "[518.547, 419.2099, 363.27612, 377.68924, 451.13712, 399.97095, 328.5842, 388.15717, 371.6692, 404.17325, 414.6294, 361.81082, 346.15344, 367.8779, 364.23676, 372.6274, 341.3836, 313.2701, 334.14, 342.78827]\n",
      "Val_mape:                                                                                                              \n",
      "[625.9545288085938, 618.5846557617188, 643.7518310546875, 773.3211059570312, 825.5398559570312, 910.30859375, 963.6510620117188, 1025.3067626953125, 1125.2127685546875, 1160.12353515625, 1200.6876220703125, 1157.5732421875, 1162.7420654296875, 1196.511962890625, 1159.3895263671875, 1214.88037109375, 1178.8895263671875, 1140.8677978515625, 1115.0882568359375, 1164.4366455078125]\n",
      "Lr:                                                                                                                    \n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 0.95, 0.9025, 0.9025, 0.9025, 0.85737497, 0.85737497, 0.85737497, 0.81450623, 0.81450623, 0.81450623, 0.77378094, 0.77378094, 0.77378094]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[47.80023708343506, 26.879644775390624, 20.773804187774658, 18.23051252365112, 16.780090713500975, 15.784467792510986, 14.903625345230102, 14.10784740447998, 13.450556755065918, 12.732756519317627, 12.068037796020509, 11.435319948196412, 10.829075288772582, 10.251831007003783, 9.709509563446044, 9.143088388442994, 8.634431648254395, 8.225369477272034, 7.65798590183258, 7.14451425075531]\n",
      "Val loss:                                                                                                              \n",
      "[32.02724520365397, 22.46446927388509, 19.016041437784832, 17.29319953918457, 16.124553044637043, 15.209720293680826, 14.458325386047363, 13.727606455485025, 12.979001681009928, 12.434863726298014, 11.775702476501465, 11.235200881958008, 10.565365473429361, 10.095181465148926, 9.412649472554525, 8.85403029123942, 8.342109362284342, 7.981408913930257, 7.367108503977458, 7.053094069163005]\n",
      "Mse:                                                                                                                   \n",
      "[1.7155735, 0.85613173, 0.65795606, 0.5081818, 0.4613625, 0.4373024, 0.39464042, 0.33128947, 0.37063208, 0.30807197, 0.26332587, 0.25622562, 0.21629563, 0.2126729, 0.19800702, 0.18205486, 0.178494, 0.2999867, 0.13849072, 0.13623449]\n",
      "Val_mse:                                                                                                               \n",
      "[0.3222731649875641, 0.2354186773300171, 0.24039065837860107, 0.36051687598228455, 0.21868544816970825, 0.2599288523197174, 0.45581093430519104, 0.43353471159935, 0.20750969648361206, 0.3991014063358307, 0.4611607491970062, 0.18985503911972046, 0.24046464264392853, 0.18495525419712067, 0.27693793177604675, 0.1388206034898758, 0.1900434046983719, 0.16666877269744873, 0.1431545466184616, 0.24945195019245148]\n",
      "Mae:                                                                                                                   \n",
      "[1.0006943, 0.71728724, 0.6194131, 0.54345167, 0.5099723, 0.48071402, 0.4514769, 0.41765532, 0.42421812, 0.39081714, 0.36308005, 0.35068598, 0.32867917, 0.32038376, 0.31034103, 0.30815443, 0.30097836, 0.343475, 0.26510304, 0.26578048]\n",
      "Val_mae:                                                                                                               \n",
      "[1.0006943, 0.71728724, 0.6194131, 0.54345167, 0.5099723, 0.48071402, 0.4514769, 0.41765532, 0.42421812, 0.39081714, 0.36308005, 0.35068598, 0.32867917, 0.32038376, 0.31034103, 0.30815443, 0.30097836, 0.343475, 0.26510304, 0.26578048]\n",
      "Mape:                                                                                                                  \n",
      "[378.8692, 272.5514, 274.34232, 197.24364, 195.24718, 142.86171, 150.47725, 130.32281, 138.93773, 116.90755, 127.85049, 118.97693, 103.17495, 107.52828, 117.31217, 114.047806, 95.5522, 123.36588, 102.388626, 100.264404]\n",
      "Val_mape:                                                                                                              \n",
      "[499.9978942871094, 141.43177795410156, 143.99562072753906, 244.77276611328125, 274.5149841308594, 396.0433654785156, 99.5218505859375, 123.65143585205078, 352.5909118652344, 368.2096862792969, 77.10345458984375, 148.6624755859375, 142.7791290283203, 507.01123046875, 151.24314880371094, 89.1338119506836, 360.3580017089844, 125.59054565429688, 330.99859619140625, 263.9747314453125]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[219.79269638061524, 93.19434719085693, 39.943800163269046, 17.203834199905394, 9.360591459274293, 7.286148452758789, 6.503183674812317, 6.004035592079163, 5.642784166336059, 5.214611577987671, 5.025817203521728, 4.659317445755005, 4.495732140541077, 4.196971368789673, 4.006557440757751, 3.7815871477127074, 3.5524832963943482, 3.418499565124512, 3.1521108627319334, 3.09791157245636]\n",
      "Val loss:                                                                                                              \n",
      "[132.77142842610678, 57.71435292561849, 23.960269927978516, 11.156805356343588, 7.536805152893066, 6.70481808980306, 6.102112929026286, 5.759886582692464, 5.410431702931722, 5.186398188273112, 4.885980288187663, 4.584164937337239, 4.433765888214111, 4.157337029774983, 3.885171890258789, 3.6392273902893066, 3.53684139251709, 3.2356794675191245, 3.1048576831817627, 3.013908783594767]\n",
      "Mse:                                                                                                                   \n",
      "[5.4125123, 3.1920333, 2.6481743, 2.005683, 1.5157686, 1.1676806, 0.75933254, 0.56378424, 0.43448192, 0.2839579, 0.22231989, 0.16045415, 0.18644664, 0.11566377, 0.13281621, 0.12470607, 0.116149426, 0.11736317, 0.10876723, 0.14214167]\n",
      "Val_mse:                                                                                                               \n",
      "[0.7581994533538818, 0.44838762283325195, 0.6129566431045532, 0.5231357216835022, 0.5530675053596497, 0.4599947929382324, 0.5497386455535889, 0.5195140242576599, 0.3647814691066742, 0.30927613377571106, 0.18198736011981964, 0.16750483214855194, 0.21770019829273224, 0.25975361466407776, 0.29776832461357117, 0.18869344890117645, 0.11297789216041565, 0.2076997309923172, 0.2934808135032654, 0.4713113605976105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mae:                                                                                                                   \n",
      "[1.8217475, 1.4084893, 1.2820861, 1.1104906, 0.92973024, 0.7941067, 0.61783993, 0.5358137, 0.46366262, 0.37619668, 0.3315406, 0.2924862, 0.29562074, 0.24771932, 0.26596123, 0.25896335, 0.24909142, 0.24084432, 0.23982081, 0.2537564]\n",
      "Val_mae:                                                                                                               \n",
      "[1.8217475, 1.4084893, 1.2820861, 1.1104906, 0.92973024, 0.7941067, 0.61783993, 0.5358137, 0.46366262, 0.37619668, 0.3315406, 0.2924862, 0.29562074, 0.24771932, 0.26596123, 0.25896335, 0.24909142, 0.24084432, 0.23982081, 0.2537564]\n",
      "Mape:                                                                                                                  \n",
      "[697.72174, 558.72906, 481.17188, 372.66483, 298.25552, 259.73135, 193.26614, 168.81956, 136.5571, 151.81853, 115.65226, 115.34005, 100.81981, 108.3044, 105.034325, 90.687035, 110.17842, 109.262405, 102.64982, 115.53082]\n",
      "Val_mape:                                                                                                              \n",
      "[105.58658599853516, 201.5924530029297, 110.75235748291016, 115.8521499633789, 192.8251495361328, 518.6896362304688, 537.4627075195312, 298.9068298339844, 180.8062744140625, 131.23365783691406, 206.7943115234375, 110.72681427001953, 239.29042053222656, 228.8286590576172, 81.3132095336914, 155.40817260742188, 216.8308563232422, 65.06885528564453, 90.33374786376953, 596.2030639648438]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[72.39757843017578, 62.702745056152345, 56.089714241027835, 46.56926097869873, 38.930705833435056, 32.64469041824341, 28.297583770751952, 23.682321739196777, 20.7666277885437, 18.060325336456298, 15.238766241073609, 13.611570787429809, 11.917055654525758, 10.584251403808594, 9.40415906906128, 8.379159379005433, 7.631100702285766, 6.716085696220398, 5.977606344223022, 5.427977418899536]\n",
      "Val loss:                                                                                                              \n",
      "[48.276755015055336, 42.05533218383789, 36.032056172688804, 29.891285578409832, 25.002939860026043, 20.97254753112793, 17.953645706176758, 15.354627927144369, 13.333257039388021, 11.86527951558431, 10.70530923207601, 9.40893522898356, 8.131001154581705, 7.5546291669209795, 6.614332516988118, 6.556335926055908, 5.843194166819255, 5.174716790517171, 4.7982133229573565, 4.261220296223958]\n",
      "Mse:                                                                                                                   \n",
      "[21.171799, 18.14958, 17.492626, 14.072972, 11.946833, 10.083045, 9.392346, 7.671241, 6.935777, 6.0029516, 4.7710447, 4.4553533, 3.7506313, 3.2101123, 2.7219307, 2.294049, 2.1019137, 1.6651539, 1.3892663, 1.2444488]\n",
      "Val_mse:                                                                                                               \n",
      "[0.8948271870613098, 0.603360116481781, 0.659176766872406, 0.49855685234069824, 0.48366260528564453, 0.4768623113632202, 0.6982037425041199, 0.5758134722709656, 0.48982787132263184, 0.6862123608589172, 0.9798051714897156, 0.8056114315986633, 0.3936940133571625, 0.5450584888458252, 0.24693386256694794, 0.7564650177955627, 0.6045635342597961, 0.37015900015830994, 0.4348289668560028, 0.30190321803092957]\n",
      "Mae:                                                                                                                   \n",
      "[2.5099752, 2.2975066, 2.2241206, 2.028627, 1.8970814, 1.75291, 1.701696, 1.5819206, 1.5114691, 1.4436567, 1.3440816, 1.3023727, 1.2174032, 1.1581681, 1.0872787, 1.0233724, 0.9904669, 0.90994674, 0.8548797, 0.80923975]\n",
      "Val_mae:                                                                                                               \n",
      "[2.5099752, 2.2975066, 2.2241206, 2.028627, 1.8970814, 1.75291, 1.701696, 1.5819206, 1.5114691, 1.4436567, 1.3440816, 1.3023727, 1.2174032, 1.1581681, 1.0872787, 1.0233724, 0.9904669, 0.90994674, 0.8548797, 0.80923975]\n",
      "Mape:                                                                                                                  \n",
      "[723.18176, 625.2855, 810.1928, 594.54706, 657.90906, 505.05255, 558.42737, 635.6278, 414.63184, 493.2928, 366.54315, 328.81573, 443.5341, 290.63055, 239.06465, 224.71628, 179.11082, 177.25064, 172.613, 181.95457]\n",
      "Val_mape:                                                                                                              \n",
      "[476.2309265136719, 674.1912841796875, 950.9412231445312, 853.95654296875, 592.5729370117188, 1195.803466796875, 1221.0584716796875, 1131.0220947265625, 1113.3157958984375, 1361.0802001953125, 1305.2254638671875, 435.1735534667969, 596.6205444335938, 807.4655151367188, 640.4248657226562, 440.5078430175781, 606.5009155273438, 972.097412109375, 594.2520141601562, 320.02716064453125]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[33.27436428070068, 22.9728102684021, 16.419821977615356, 12.966829013824462, 11.282595682144166, 10.364201641082763, 9.7908269405365, 9.241050291061402, 8.754965925216675, 8.251697301864624, 7.7262277841568, 7.35372006893158, 6.943134903907776, 6.5509497880935665, 6.137496423721314, 5.770369338989258, 5.428651642799378, 5.164481496810913, 4.7824015617370605, 4.502062964439392]\n",
      "Val loss:                                                                                                              \n",
      "[27.427987416585285, 19.4340394337972, 14.627199490865072, 12.291627248128256, 11.061631202697754, 10.327510833740234, 9.799832026163736, 9.374631881713867, 8.605575243631998, 8.153074264526367, 7.67065413792928, 7.282746156056722, 6.922379811604817, 6.436206658681233, 6.072958787282308, 5.684556643168132, 5.496510823567708, 5.1036631266276045, 4.724990526835124, 4.482566038767497]\n",
      "Mse:                                                                                                                   \n",
      "[0.32227367, 0.15407327, 0.11197621, 0.13758606, 0.10056271, 0.08937346, 0.10899609, 0.11575091, 0.11008505, 0.090471916, 0.050311375, 0.07421545, 0.06789189, 0.07020265, 0.04727122, 0.043867934, 0.050940644, 0.06914355, 0.040202033, 0.05185297]\n",
      "Val_mse:                                                                                                               \n",
      "[0.6845476627349854, 0.5801351070404053, 0.6180210113525391, 0.5167621970176697, 0.4420396387577057, 0.42558273673057556, 0.47572293877601624, 0.5479607582092285, 0.24564838409423828, 0.20766811072826385, 0.23476319015026093, 0.2682059705257416, 0.30406197905540466, 0.13333947956562042, 0.1895260065793991, 0.15565745532512665, 0.3018558919429779, 0.20829199254512787, 0.14305707812309265, 0.14796294271945953]\n",
      "Mae:                                                                                                                   \n",
      "[0.3586983, 0.2854846, 0.2566374, 0.28581652, 0.24450898, 0.23032263, 0.24404486, 0.25456557, 0.2516959, 0.21801563, 0.17136958, 0.20480995, 0.18558045, 0.19655643, 0.16368032, 0.1547647, 0.17010537, 0.17537126, 0.14740075, 0.16459024]\n",
      "Val_mae:                                                                                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3586983, 0.2854846, 0.2566374, 0.28581652, 0.24450898, 0.23032263, 0.24404486, 0.25456557, 0.2516959, 0.21801563, 0.17136958, 0.20480995, 0.18558045, 0.19655643, 0.16368032, 0.1547647, 0.17010537, 0.17537126, 0.14740075, 0.16459024]\n",
      "Mape:                                                                                                                  \n",
      "[127.885925, 110.708664, 100.04801, 101.97493, 91.64896, 90.48602, 81.2657, 87.70431, 80.3112, 80.63063, 69.44115, 84.0017, 77.31889, 77.07599, 60.996216, 59.536827, 53.773304, 64.806656, 56.577026, 67.11139]\n",
      "Val_mape:                                                                                                              \n",
      "[388.976318359375, 388.6455383300781, 340.4365234375, 217.21629333496094, 147.93853759765625, 158.994384765625, 271.511962890625, 235.041259765625, 83.9319839477539, 380.458251953125, 192.1586456298828, 243.1179962158203, 243.55931091308594, 171.70867919921875, 447.1448669433594, 270.8131103515625, 379.8157958984375, 495.0540466308594, 600.8755493164062, 427.6141662597656]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[24.262683200836182, 16.954361152648925, 12.963109302520753, 10.780267095565796, 9.352543020248413, 8.299952244758606, 7.460828065872192, 6.764217209815979, 6.206675624847412, 5.712901377677918, 5.29451813697815, 4.910560274124146, 4.573273634910583, 4.286003565788269, 4.015694570541382, 3.764300835132599, 3.538171684741974, 3.336517703533173, 3.146816539764404, 2.9643041491508484]\n",
      "Val loss:                                                                                                              \n",
      "[19.306320826212566, 14.084247271219889, 11.39421526590983, 9.830112139383951, 8.627256393432617, 7.820996125539144, 7.0349704424540205, 6.489131609598796, 5.9982320467631025, 5.564748764038086, 5.189727783203125, 4.8595560391743975, 4.586513678232829, 4.306357224782308, 4.095270156860352, 3.8471247355143228, 3.6214025020599365, 3.4330915609995523, 3.2984093030293784, 3.0613718032836914]\n",
      "Mse:                                                                                                                   \n",
      "[1.0923818, 0.79898626, 0.6532275, 0.5609912, 0.48397797, 0.4280502, 0.36996284, 0.31896892, 0.2963117, 0.25875244, 0.23680308, 0.20381308, 0.1782171, 0.16714783, 0.14996597, 0.13013995, 0.117091954, 0.11181799, 0.10390089, 0.09255959]\n",
      "Val_mse:                                                                                                               \n",
      "[0.3934912383556366, 0.3598102629184723, 0.362772136926651, 0.39625632762908936, 0.32603827118873596, 0.3866015672683716, 0.30354925990104675, 0.33917126059532166, 0.33869826793670654, 0.32660266757011414, 0.3229517638683319, 0.3231610357761383, 0.3426614999771118, 0.32342731952667236, 0.35528501868247986, 0.3271375894546509, 0.3058168590068817, 0.3058898150920868, 0.3471701145172119, 0.276623010635376]\n",
      "Mae:                                                                                                                   \n",
      "[0.81464994, 0.7042943, 0.63547164, 0.58240306, 0.54150903, 0.50799876, 0.47095013, 0.4354702, 0.41916937, 0.3917771, 0.37141806, 0.34340388, 0.31862533, 0.3057647, 0.2900806, 0.26871789, 0.25438312, 0.24665987, 0.23593669, 0.22158165]\n",
      "Val_mae:                                                                                                               \n",
      "[0.81464994, 0.7042943, 0.63547164, 0.58240306, 0.54150903, 0.50799876, 0.47095013, 0.4354702, 0.41916937, 0.3917771, 0.37141806, 0.34340388, 0.31862533, 0.3057647, 0.2900806, 0.26871789, 0.25438312, 0.24665987, 0.23593669, 0.22158165]\n",
      "Mape:                                                                                                                  \n",
      "[368.52228, 271.05008, 232.26904, 196.85767, 206.20425, 180.99783, 161.95511, 174.53728, 126.98879, 185.04088, 116.109604, 138.5998, 104.0399, 110.44141, 79.59237, 69.9428, 84.959915, 67.488914, 61.08367, 59.183067]\n",
      "Val_mape:                                                                                                              \n",
      "[533.9916381835938, 182.6742706298828, 89.77193450927734, 86.20891571044922, 146.00186157226562, 182.0943145751953, 145.9634246826172, 164.33775329589844, 224.08734130859375, 219.3970947265625, 188.9049835205078, 247.02525329589844, 375.5369873046875, 195.2476043701172, 261.7649841308594, 283.0869445800781, 256.71563720703125, 249.386474609375, 190.697265625, 99.2682876586914]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[46.32274951934814, 43.52990798950195, 40.89615936279297, 38.436600875854495, 36.12523708343506, 33.95804405212402, 31.92630910873413, 30.016925811767578, 28.224509906768798, 26.540779209136964, 24.96237983703613, 23.47512788772583, 22.078538608551025, 20.766010570526124, 19.536638927459716, 18.375621223449706, 17.291727924346922, 16.267649745941164, 15.307327222824096, 14.403757810592651]\n",
      "Val loss:                                                                                                              \n",
      "[44.70069885253906, 42.02028020222982, 39.50079091389974, 37.13299306233724, 34.908766428629555, 32.81860097249349, 30.85586929321289, 29.01197687784831, 27.279539744059246, 25.65264956156413, 24.12309392293294, 22.685964584350586, 21.33647346496582, 20.067455291748047, 18.874469757080078, 17.755128224690754, 16.702802022298176, 15.713711738586426, 14.783172925313314, 13.910576502482096]\n",
      "Mse:                                                                                                                   \n",
      "[2.2941966, 1.94846, 1.503631, 1.2800232, 1.0657691, 0.95059574, 0.894108, 0.85022867, 0.800461, 0.77129084, 0.754402, 0.71845496, 0.70712376, 0.6771019, 0.68864095, 0.66447544, 0.67862356, 0.6588063, 0.652499, 0.62694347]\n",
      "Val_mse:                                                                                                               \n",
      "[0.34732556343078613, 0.30918270349502563, 0.29318320751190186, 0.2787748873233795, 0.26769253611564636, 0.24851401150226593, 0.23863129317760468, 0.22476519644260406, 0.21470282971858978, 0.20516641438007355, 0.1991078108549118, 0.19024938344955444, 0.19006669521331787, 0.18564699590206146, 0.18042469024658203, 0.19002431631088257, 0.19385945796966553, 0.19817864894866943, 0.18698330223560333, 0.1915377378463745]\n",
      "Mae:                                                                                                                   \n",
      "[1.1619323, 1.0734636, 0.9467993, 0.8801702, 0.8082787, 0.7621237, 0.7375966, 0.7219808, 0.6987977, 0.68656814, 0.6816982, 0.6608461, 0.66103464, 0.64451283, 0.6473955, 0.63766, 0.6409153, 0.6317506, 0.63389814, 0.61900157]\n",
      "Val_mae:                                                                                                               \n",
      "[1.1619323, 1.0734636, 0.9467993, 0.8801702, 0.8082787, 0.7621237, 0.7375966, 0.7219808, 0.6987977, 0.68656814, 0.6816982, 0.6608461, 0.66103464, 0.64451283, 0.6473955, 0.63766, 0.6409153, 0.6317506, 0.63389814, 0.61900157]\n",
      "Mape:                                                                                                                  \n",
      "[329.98495, 417.39606, 419.90274, 335.4297, 316.61145, 271.1501, 331.9292, 303.47394, 236.25623, 233.72275, 281.18524, 222.22818, 267.78552, 234.38103, 287.60794, 279.08505, 248.95871, 268.86462, 250.95523, 244.08618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_mape:                                                                                                              \n",
      "[331.0209655761719, 522.7743530273438, 610.7068481445312, 683.0194702148438, 712.3880004882812, 775.7293090820312, 788.0090942382812, 783.3298950195312, 764.2140502929688, 743.2837524414062, 766.6948852539062, 757.9164428710938, 786.1857299804688, 809.5419921875, 851.1148071289062, 904.0130004882812, 919.326904296875, 978.105224609375, 981.3580932617188, 1025.258056640625]\n",
      "Lr:                                                                                                                    \n",
      "[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[29.2926646232605, 29.355447864532472, 29.160998344421387, 29.082685470581055, 29.046368026733397, 28.980836963653566, 28.941932010650635, 28.84541015625, 28.770691871643066, 28.74694957733154, 28.739613628387453, 28.71473741531372, 28.714052200317383, 28.639253044128417, 28.639061164855956, 28.620199966430665, 28.637753105163576, 28.550493049621583, 28.542119598388673, 28.545177745819093]\n",
      "Val loss:                                                                                                              \n",
      "[27.427832285563152, 27.337992986043293, 27.302954355875652, 27.312002817789715, 27.365782419840496, 27.46188735961914, 27.5742925008138, 27.68896993001302, 27.827107747395832, 27.915664037068684, 28.051820119222004, 28.144758224487305, 28.182993570963543, 28.246792475382488, 28.29882558186849, 28.337297439575195, 28.386239369710285, 28.401234944661457, 28.422640482584637, 28.422382990519207]\n",
      "Mse:                                                                                                                   \n",
      "[2.29562, 2.3744767, 2.1947093, 2.1307347, 2.1084285, 2.0566936, 2.0310798, 1.947272, 1.8851706, 1.8736537, 1.8781639, 1.8650697, 1.8758726, 1.812227, 1.8231652, 1.8151337, 1.843235, 1.7664385, 1.7683694, 1.7814846]\n",
      "Val_mse:                                                                                                               \n",
      "[0.43971550464630127, 0.3648044764995575, 0.3442486524581909, 0.367413192987442, 0.4351520538330078, 0.5449070334434509, 0.6701239943504333, 0.7974941730499268, 0.9481483101844788, 1.0486072301864624, 1.1965786218643188, 1.3012796640396118, 1.3506664037704468, 1.4256218671798706, 1.4887577295303345, 1.5377966165542603, 1.5972057580947876, 1.622729778289795, 1.6541470289230347, 1.6640119552612305]\n",
      "Mae:                                                                                                                   \n",
      "[1.1998007, 1.2207959, 1.177501, 1.1548569, 1.1581266, 1.1391144, 1.1299366, 1.1078017, 1.092872, 1.0932603, 1.0933801, 1.085139, 1.0937098, 1.0716429, 1.079128, 1.0748394, 1.0827788, 1.0584574, 1.0610254, 1.0617937]\n",
      "Val_mae:                                                                                                               \n",
      "[1.1998007, 1.2207959, 1.177501, 1.1548569, 1.1581266, 1.1391144, 1.1299366, 1.1078017, 1.092872, 1.0932603, 1.0933801, 1.085139, 1.0937098, 1.0716429, 1.079128, 1.0748394, 1.0827788, 1.0584574, 1.0610254, 1.0617937]\n",
      "Mape:                                                                                                                  \n",
      "[445.42474, 426.85605, 354.52023, 427.6889, 433.48737, 417.62946, 384.63586, 439.4704, 366.68793, 433.74496, 343.66757, 359.0068, 354.81302, 416.90665, 414.63702, 366.5533, 345.85162, 400.7021, 380.74896, 391.19284]\n",
      "Val_mape:                                                                                                              \n",
      "[1428.53271484375, 1466.06005859375, 1600.0377197265625, 1679.9920654296875, 1776.3487548828125, 1843.63427734375, 1891.7183837890625, 1908.2532958984375, 1889.716796875, 1823.05078125, 1717.5142822265625, 1671.6617431640625, 1572.0028076171875, 1473.8067626953125, 1398.59228515625, 1310.60888671875, 1229.46435546875, 1140.8624267578125, 1142.1673583984375, 1125.08203125]\n",
      "Lr:                                                                                                                    \n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95, 0.95, 0.95, 0.9025, 0.9025, 0.9025, 0.85737497, 0.85737497, 0.85737497, 0.81450623, 0.81450623, 0.81450623, 0.77378094, 0.77378094]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[149.77937507629395, 97.02848167419434, 60.66478519439697, 37.368725299835205, 24.454072189331054, 18.175343227386474, 15.045096349716186, 13.263229036331177, 12.11280107498169, 11.44651656150818, 10.669743537902832, 9.995991611480713, 9.32219910621643, 8.773538208007812, 8.311512422561645, 7.79245343208313, 7.316307783126831, 7.042002201080322, 6.455674481391907, 6.060462522506714]\n",
      "Val loss:                                                                                                              \n",
      "[117.6302007039388, 75.27409108479817, 46.2823486328125, 28.896663665771484, 20.111220677693684, 16.020146052042644, 13.798531850179037, 12.40291945139567, 11.574641863505045, 10.729040463765463, 10.257281303405762, 9.580182711283365, 8.98732058207194, 8.478459358215332, 8.174884478251139, 7.541650136311849, 7.046096483866374, 6.67159366607666, 6.185397307078044, 5.8207071622212725]\n",
      "Mse:                                                                                                                   \n",
      "[1.7395651, 0.7454643, 0.6637317, 0.539761, 0.4883893, 0.52797383, 0.4063087, 0.33027554, 0.298418, 0.3461116, 0.27296868, 0.23391533, 0.19302168, 0.18182763, 0.18525556, 0.1357188, 0.14117834, 0.15648255, 0.109146416, 0.11681597]\n",
      "Val_mse:                                                                                                               \n",
      "[0.4244656264781952, 0.35324332118034363, 0.4944540560245514, 0.1861683875322342, 0.2491959184408188, 0.2917212247848511, 0.22768338024616241, 0.16102862358093262, 0.2641960084438324, 0.07455572485923767, 0.15266261994838715, 0.1620550900697708, 0.1770356446504593, 0.1571054309606552, 0.10626629739999771, 0.17067556083202362, 0.10081390291452408, 0.07810515910387039, 0.061593785881996155, 0.04502080753445625]\n",
      "Mae:                                                                                                                   \n",
      "[0.85781777, 0.6670504, 0.6413001, 0.5811485, 0.5545881, 0.5582875, 0.4958229, 0.44851202, 0.42853928, 0.4526003, 0.39696687, 0.37442598, 0.3416273, 0.33040383, 0.32546085, 0.28583893, 0.28816122, 0.2841436, 0.251176, 0.25591844]\n",
      "Val_mae:                                                                                                               \n",
      "[0.85781777, 0.6670504, 0.6413001, 0.5811485, 0.5545881, 0.5582875, 0.4958229, 0.44851202, 0.42853928, 0.4526003, 0.39696687, 0.37442598, 0.3416273, 0.33040383, 0.32546085, 0.28583893, 0.28816122, 0.2841436, 0.251176, 0.25591844]\n",
      "Mape:                                                                                                                  \n",
      "[316.92816, 258.99396, 283.44455, 253.96957, 246.67102, 218.36002, 179.52864, 175.82164, 172.66487, 183.79239, 131.70506, 125.103615, 139.04843, 123.99205, 137.82483, 94.76997, 139.39091, 108.464195, 91.58665, 99.33293]\n",
      "Val_mape:                                                                                                              \n",
      "[912.8569946289062, 351.157470703125, 195.7958526611328, 478.4258117675781, 253.04273986816406, 161.1193084716797, 391.7845153808594, 271.0807800292969, 323.5547790527344, 224.8922576904297, 140.0640411376953, 243.1854705810547, 249.6026153564453, 125.44544219970703, 1289.3443603515625, 365.4806213378906, 499.7262268066406, 293.4717102050781, 450.6673583984375, 589.3679809570312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "Loss:                                                                                                                  \n",
      "[43.080416679382324, 30.85949354171753, 23.93250789642334, 19.63314018249512, 16.633715867996216, 14.461123943328857, 12.710919761657715, 11.256296300888062, 10.050124979019165, 9.069994831085205, 8.322541046142579, 7.770295906066894, 7.3374734163284305, 6.939401865005493, 6.570835900306702, 6.220275640487671, 5.879922151565552, 5.551142859458923, 5.221444940567016, 4.904464912414551]\n",
      "Val loss:                                                                                                              \n",
      "[31.079079310099285, 23.419605890909832, 18.738646189371746, 15.856936772664389, 13.872711181640625, 12.371559778849283, 11.19587484995524, 10.386174201965332, 9.297174771626791, 8.553921381632486, 7.992266019185384, 7.490370591481526, 7.0830756823221845, 6.679787317911784, 6.398785909016927, 6.001751899719238, 5.66435178120931, 5.347208182017009, 5.0093309084574384, 4.75310484568278]\n",
      "Mse:                                                                                                                   \n",
      "[51.914192, 38.039654, 25.263947, 18.579605, 11.8934, 7.5439835, 4.3844748, 2.3782647, 1.2379174, 0.6151463, 0.3687236, 0.3211334, 0.31403306, 0.30324477, 0.29404476, 0.30265212, 0.2993079, 0.31336018, 0.2911823, 0.29318243]\n",
      "Val_mse:                                                                                                               \n",
      "[0.7225854992866516, 0.5456522107124329, 0.5374757647514343, 0.6289886236190796, 0.43971261382102966, 0.3383859395980835, 0.33668947219848633, 0.9112012982368469, 0.2627072334289551, 0.20293205976486206, 0.23100702464580536, 0.2059118151664734, 0.25927263498306274, 0.21255844831466675, 0.3015129864215851, 0.1583610624074936, 0.20090146362781525, 0.1376798450946808, 0.2213302105665207, 0.2979816794395447]\n",
      "Mae:                                                                                                                   \n",
      "[5.219537, 4.4226065, 3.6018748, 3.0584965, 2.441631, 1.9144024, 1.4585174, 1.0599166, 0.7655311, 0.55655754, 0.44755954, 0.41027054, 0.41021413, 0.39919934, 0.39150727, 0.40098414, 0.3994543, 0.4108611, 0.3945172, 0.39205658]\n",
      "Val_mae:                                                                                                               \n",
      "[5.219537, 4.4226065, 3.6018748, 3.0584965, 2.441631, 1.9144024, 1.4585174, 1.0599166, 0.7655311, 0.55655754, 0.44755954, 0.41027054, 0.41021413, 0.39919934, 0.39150727, 0.40098414, 0.3994543, 0.4108611, 0.3945172, 0.39205658]\n",
      "Mape:                                                                                                                  \n",
      "[1797.4294, 1880.291, 1224.175, 913.6459, 927.10077, 779.5248, 499.31403, 310.17908, 237.51787, 194.38593, 111.85454, 99.11003, 119.69431, 104.32179, 95.336525, 107.30973, 103.5598, 106.8672, 118.76335, 102.97595]\n",
      "Val_mape:                                                                                                              \n",
      "[298.7695007324219, 116.48009490966797, 258.6689147949219, 459.1041564941406, 189.2796630859375, 222.58477783203125, 908.7993774414062, 1414.5103759765625, 987.3712768554688, 651.9341430664062, 414.4884338378906, 251.69325256347656, 347.2542724609375, 62.328006744384766, 140.82765197753906, 66.25627899169922, 302.6754150390625, 299.7887878417969, 285.1846008300781, 395.414794921875]\n",
      "Lr:                                                                                                                    \n",
      "[0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "******************************Next Evaluation Results******************************                                    \n",
      "100%|| 20/20 [19:02<00:00, 57.10s/trial, best loss: 0.02026655711233616]\n",
      "Evaluation of best performing model:\n",
      "Directory  best_model  already exists\n",
      "\n",
      "********************Best Model Configuration:********************\n",
      "{'name': 'sequential_3', 'layers': [{'class_name': 'CuDNNLSTM', 'config': {'name': 'cu_dnnlstm_5', 'trainable': True, 'batch_input_shape': (512, 28, 2085), 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': True, 'units': 254, 'kernel_initializer': {'class_name': 'RandomNormal', 'config': {'mean': 0.0, 'stddev': 0.05, 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': {'class_name': 'L1L2', 'config': {'l1': 0.0, 'l2': 0.05000000074505806}}, 'recurrent_regularizer': None, 'bias_regularizer': {'class_name': 'L1L2', 'config': {'l1': 0.0, 'l2': 0.10000000149011612}}, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_7', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_7', 'trainable': True, 'dtype': 'float32', 'rate': 0.29665005881763407, 'noise_shape': None, 'seed': None}}, {'class_name': 'CuDNNLSTM', 'config': {'name': 'cu_dnnlstm_6', 'trainable': True, 'dtype': 'float32', 'return_sequences': False, 'return_state': False, 'go_backwards': True, 'stateful': True, 'units': 128, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': {'class_name': 'L1L2', 'config': {'l1': 0.0, 'l2': 0.05000000074505806}}, 'recurrent_regularizer': None, 'bias_regularizer': {'class_name': 'L1L2', 'config': {'l1': 0.0, 'l2': 0.10000000149011612}}, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None}}, {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_8', 'trainable': True, 'dtype': 'float32', 'axis': -1, 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}, {'class_name': 'Dropout', 'config': {'name': 'dropout_8', 'trainable': True, 'dtype': 'float32', 'rate': 0.29665005881763407, 'noise_shape': None, 'seed': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Activation', 'config': {'name': 'activation_3', 'trainable': True, 'dtype': 'float32', 'activation': 'linear'}}]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data with a train_test_split of  0.85\n",
      "1536/1536 [==============================] - 0s 266us/step\n",
      "val_loss:  134.3385213216146  val_mse:  0.040105804800987244  val_mae:  0.155134379863739  val_mape:  61.69073486328125\n",
      "\n",
      "********************Best Run hyper-parameters:********************\n",
      "{'activation_choice': 'selu', 'dropout_choice': 0.29665005881763407, 'dropout_choice1': 0.07607030848224017, 'dropout_choice1_1': 'no_dense', 'kernel_initializer': 'RandomNormal', 'l1_k_regularizer_choice': 0.1, 'l1_k_regularizer_choice_1': 0.05, 'l1_k_regularizer_choice_2': 0.1, 'l1_k_regularizer_choice_3': 0.03, 'loss_choice': 'mean_absolute_error', 'optimizer_choice': 'sgd', 'units': 254, 'units1': 128, 'units2': 32}\n",
      "Prepared data with a train_test_split of  0.85\n",
      "******************************BEST MODEL SUMMARY******************************\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_5 (CuDNNLSTM)     (512, 28, 254)            2378456   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (512, 28, 254)            1016      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (512, 28, 254)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_6 (CuDNNLSTM)     (512, 128)                196608    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (512, 128)                512       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (512, 128)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (512, 1)                  129       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (512, 1)                  0         \n",
      "=================================================================\n",
      "Total params: 2,576,721\n",
      "Trainable params: 2,575,957\n",
      "Non-trainable params: 764\n",
      "_________________________________________________________________\n",
      "Directory  train_model  already exists\n",
      "Directory  train_model/checkpoint  already exists\n",
      "Directory  logs  already exists\n",
      "Directory  train_metrics  already exists\n",
      "Train on 10240 samples, validate on 1536 samples\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/2000\n",
      " - 5s - loss: 131.7253 - mse: 0.0620 - mae: 0.2006 - mape: 57.7236 - val_loss: 128.9001 - val_mse: 0.1074 - val_mae: 0.2711 - val_mape: 90.6062\n",
      "\n",
      "Epoch 00001: val_mse improved from inf to 0.10743, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "WARNING:tensorflow:From C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/2000\n",
      " - 3s - loss: 126.2279 - mse: 0.0365 - mae: 0.1422 - mape: 48.4830 - val_loss: 123.4178 - val_mse: 0.0203 - val_mae: 0.1023 - val_mape: 57.2931\n",
      "\n",
      "Epoch 00002: val_mse improved from 0.10743 to 0.02030, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 3/2000\n",
      " - 3s - loss: 121.0338 - mse: 0.0390 - mae: 0.1520 - mape: 47.9139 - val_loss: 118.3758 - val_mse: 0.0339 - val_mae: 0.1448 - val_mape: 52.1743\n",
      "\n",
      "Epoch 00003: val_mse did not improve from 0.02030\n",
      "Epoch 4/2000\n",
      " - 3s - loss: 116.0480 - mse: 0.0362 - mae: 0.1462 - mape: 52.3543 - val_loss: 113.5033 - val_mse: 0.0317 - val_mae: 0.1385 - val_mape: 44.2830\n",
      "\n",
      "Epoch 00004: val_mse did not improve from 0.02030\n",
      "Epoch 5/2000\n",
      " - 3s - loss: 111.2769 - mse: 0.0344 - mae: 0.1414 - mape: 46.0434 - val_loss: 108.8264 - val_mse: 0.0239 - val_mae: 0.1192 - val_mape: 44.8139\n",
      "\n",
      "Epoch 00005: val_mse did not improve from 0.02030\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.009499999787658453.\n",
      "Epoch 6/2000\n",
      " - 3s - loss: 106.8126 - mse: 0.0310 - mae: 0.1341 - mape: 45.0866 - val_loss: 104.5703 - val_mse: 0.0189 - val_mae: 0.1033 - val_mape: 53.0748\n",
      "\n",
      "Epoch 00006: val_mse improved from 0.02030 to 0.01893, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 7/2000\n",
      " - 3s - loss: 102.6622 - mse: 0.0343 - mae: 0.1417 - mape: 44.2615 - val_loss: 100.5485 - val_mse: 0.0352 - val_mae: 0.1500 - val_mape: 44.0304\n",
      "\n",
      "Epoch 00007: val_mse did not improve from 0.01893\n",
      "Epoch 8/2000\n",
      " - 3s - loss: 98.6785 - mse: 0.0381 - mae: 0.1478 - mape: 44.6863 - val_loss: 96.6383 - val_mse: 0.0314 - val_mae: 0.1439 - val_mape: 55.5852\n",
      "\n",
      "Epoch 00008: val_mse did not improve from 0.01893\n",
      "Epoch 9/2000\n",
      " - 3s - loss: 94.8372 - mse: 0.0315 - mae: 0.1353 - mape: 43.0968 - val_loss: 92.8412 - val_mse: 0.0145 - val_mae: 0.0934 - val_mape: 69.2236\n",
      "\n",
      "Epoch 00009: val_mse improved from 0.01893 to 0.01450, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 10/2000\n",
      " - 3s - loss: 91.1684 - mse: 0.0344 - mae: 0.1411 - mape: 50.0105 - val_loss: 89.2601 - val_mse: 0.0181 - val_mae: 0.1086 - val_mape: 70.9643\n",
      "\n",
      "Epoch 00010: val_mse did not improve from 0.01450\n",
      "Epoch 11/2000\n",
      " - 3s - loss: 87.6321 - mse: 0.0304 - mae: 0.1321 - mape: 41.4451 - val_loss: 85.7690 - val_mse: 0.0093 - val_mae: 0.0695 - val_mape: 41.2240\n",
      "\n",
      "Epoch 00011: val_mse improved from 0.01450 to 0.00929, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 12/2000\n",
      " - 3s - loss: 84.2568 - mse: 0.0344 - mae: 0.1427 - mape: 42.0823 - val_loss: 82.5040 - val_mse: 0.0210 - val_mae: 0.1184 - val_mape: 76.7768\n",
      "\n",
      "Epoch 00012: val_mse did not improve from 0.00929\n",
      "Epoch 13/2000\n",
      " - 3s - loss: 81.0067 - mse: 0.0344 - mae: 0.1432 - mape: 49.1312 - val_loss: 79.2671 - val_mse: 0.0090 - val_mae: 0.0633 - val_mape: 82.9585\n",
      "\n",
      "Epoch 00013: val_mse improved from 0.00929 to 0.00901, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 14/2000\n",
      " - 3s - loss: 77.8763 - mse: 0.0307 - mae: 0.1339 - mape: 38.8311 - val_loss: 76.2074 - val_mse: 0.0070 - val_mae: 0.0587 - val_mape: 40.5704\n",
      "\n",
      "Epoch 00014: val_mse improved from 0.00901 to 0.00704, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 15/2000\n",
      " - 3s - loss: 74.8837 - mse: 0.0335 - mae: 0.1383 - mape: 40.4073 - val_loss: 73.2907 - val_mse: 0.0110 - val_mae: 0.0756 - val_mape: 71.9332\n",
      "\n",
      "Epoch 00015: val_mse did not improve from 0.00704\n",
      "Epoch 16/2000\n",
      " - 3s - loss: 72.0135 - mse: 0.0366 - mae: 0.1462 - mape: 42.4966 - val_loss: 70.5238 - val_mse: 0.0252 - val_mae: 0.1262 - val_mape: 66.4753\n",
      "\n",
      "Epoch 00016: val_mse did not improve from 0.00704\n",
      "Epoch 17/2000\n",
      " - 3s - loss: 69.2320 - mse: 0.0294 - mae: 0.1288 - mape: 51.4572 - val_loss: 67.8018 - val_mse: 0.0183 - val_mae: 0.1102 - val_mape: 136.2395\n",
      "\n",
      "Epoch 00017: val_mse did not improve from 0.00704\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.009024999709799886.\n",
      "Epoch 18/2000\n",
      " - 3s - loss: 66.6559 - mse: 0.0365 - mae: 0.1460 - mape: 43.0507 - val_loss: 65.2935 - val_mse: 0.0095 - val_mae: 0.0734 - val_mape: 88.1082\n",
      "\n",
      "Epoch 00018: val_mse did not improve from 0.00704\n",
      "Epoch 19/2000\n",
      " - 3s - loss: 64.2143 - mse: 0.0302 - mae: 0.1317 - mape: 39.2333 - val_loss: 62.9307 - val_mse: 0.0126 - val_mae: 0.0896 - val_mape: 110.2811\n",
      "\n",
      "Epoch 00019: val_mse did not improve from 0.00704\n",
      "Epoch 20/2000\n",
      " - 3s - loss: 61.8915 - mse: 0.0366 - mae: 0.1453 - mape: 39.7804 - val_loss: 60.6117 - val_mse: 0.0068 - val_mae: 0.0606 - val_mape: 60.2742\n",
      "\n",
      "Epoch 00020: val_mse improved from 0.00704 to 0.00684, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 21/2000\n",
      " - 3s - loss: 59.6273 - mse: 0.0294 - mae: 0.1302 - mape: 38.4447 - val_loss: 58.4821 - val_mse: 0.0273 - val_mae: 0.1355 - val_mape: 73.8212\n",
      "\n",
      "Epoch 00021: val_mse did not improve from 0.00684\n",
      "Epoch 22/2000\n",
      " - 3s - loss: 57.4750 - mse: 0.0358 - mae: 0.1431 - mape: 41.4052 - val_loss: 56.3592 - val_mse: 0.0294 - val_mae: 0.1349 - val_mape: 106.4485\n",
      "\n",
      "Epoch 00022: val_mse did not improve from 0.00684\n",
      "Epoch 23/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 55.3789 - mse: 0.0307 - mae: 0.1316 - mape: 39.8823 - val_loss: 54.2905 - val_mse: 0.0189 - val_mae: 0.1095 - val_mape: 39.1017\n",
      "\n",
      "Epoch 00023: val_mse did not improve from 0.00684\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.008573750033974648.\n",
      "Epoch 24/2000\n",
      " - 3s - loss: 53.4268 - mse: 0.0344 - mae: 0.1399 - mape: 40.3509 - val_loss: 52.4411 - val_mse: 0.0262 - val_mae: 0.1309 - val_mape: 40.3895\n",
      "\n",
      "Epoch 00024: val_mse did not improve from 0.00684\n",
      "Epoch 25/2000\n",
      " - 3s - loss: 51.5716 - mse: 0.0278 - mae: 0.1240 - mape: 36.5964 - val_loss: 50.6583 - val_mse: 0.0356 - val_mae: 0.1528 - val_mape: 67.4526\n",
      "\n",
      "Epoch 00025: val_mse did not improve from 0.00684\n",
      "Epoch 26/2000\n",
      " - 3s - loss: 49.8007 - mse: 0.0286 - mae: 0.1274 - mape: 38.2032 - val_loss: 48.8284 - val_mse: 0.0077 - val_mae: 0.0641 - val_mape: 134.3394\n",
      "\n",
      "Epoch 00026: val_mse did not improve from 0.00684\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.008145062532275914.\n",
      "Epoch 27/2000\n",
      " - 3s - loss: 48.1324 - mse: 0.0300 - mae: 0.1313 - mape: 37.6245 - val_loss: 47.2378 - val_mse: 0.0092 - val_mae: 0.0708 - val_mape: 111.2697\n",
      "\n",
      "Epoch 00027: val_mse did not improve from 0.00684\n",
      "Epoch 28/2000\n",
      " - 3s - loss: 46.5552 - mse: 0.0283 - mae: 0.1259 - mape: 33.1606 - val_loss: 45.7143 - val_mse: 0.0135 - val_mae: 0.0912 - val_mape: 154.6294\n",
      "\n",
      "Epoch 00028: val_mse did not improve from 0.00684\n",
      "Epoch 29/2000\n",
      " - 3s - loss: 45.0223 - mse: 0.0221 - mae: 0.1123 - mape: 34.3880 - val_loss: 44.1944 - val_mse: 0.0076 - val_mae: 0.0638 - val_mape: 47.8697\n",
      "\n",
      "Epoch 00029: val_mse did not improve from 0.00684\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0077378091402351854.\n",
      "Epoch 30/2000\n",
      " - 3s - loss: 43.5939 - mse: 0.0256 - mae: 0.1185 - mape: 38.9944 - val_loss: 42.8187 - val_mse: 0.0068 - val_mae: 0.0598 - val_mape: 38.9711\n",
      "\n",
      "Epoch 00030: val_mse improved from 0.00684 to 0.00682, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 31/2000\n",
      " - 3s - loss: 42.2451 - mse: 0.0260 - mae: 0.1207 - mape: 34.7180 - val_loss: 41.5903 - val_mse: 0.0387 - val_mae: 0.1598 - val_mape: 98.4484\n",
      "\n",
      "Epoch 00031: val_mse did not improve from 0.00682\n",
      "Epoch 32/2000\n",
      " - 3s - loss: 40.9493 - mse: 0.0308 - mae: 0.1331 - mape: 41.4710 - val_loss: 40.2379 - val_mse: 0.0132 - val_mae: 0.0936 - val_mape: 24.2789\n",
      "\n",
      "Epoch 00032: val_mse did not improve from 0.00682\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.007350918860174715.\n",
      "Epoch 33/2000\n",
      " - 3s - loss: 39.6949 - mse: 0.0237 - mae: 0.1162 - mape: 38.0819 - val_loss: 39.0226 - val_mse: 0.0073 - val_mae: 0.0626 - val_mape: 56.7956\n",
      "\n",
      "Epoch 00033: val_mse did not improve from 0.00682\n",
      "Epoch 34/2000\n",
      " - 3s - loss: 38.5230 - mse: 0.0224 - mae: 0.1116 - mape: 37.8166 - val_loss: 37.9561 - val_mse: 0.0307 - val_mae: 0.1450 - val_mape: 86.9315\n",
      "\n",
      "Epoch 00034: val_mse did not improve from 0.00682\n",
      "Epoch 35/2000\n",
      " - 3s - loss: 37.4049 - mse: 0.0284 - mae: 0.1259 - mape: 43.4548 - val_loss: 36.8087 - val_mse: 0.0185 - val_mae: 0.1120 - val_mape: 52.9492\n",
      "\n",
      "Epoch 00035: val_mse did not improve from 0.00682\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.006983372895047068.\n",
      "Epoch 36/2000\n",
      " - 3s - loss: 36.3180 - mse: 0.0223 - mae: 0.1119 - mape: 31.9705 - val_loss: 35.7538 - val_mse: 0.0127 - val_mae: 0.0849 - val_mape: 53.3635\n",
      "\n",
      "Epoch 00036: val_mse did not improve from 0.00682\n",
      "Epoch 37/2000\n",
      " - 3s - loss: 35.3148 - mse: 0.0269 - mae: 0.1225 - mape: 35.2294 - val_loss: 34.7853 - val_mse: 0.0191 - val_mae: 0.1148 - val_mape: 44.1670\n",
      "\n",
      "Epoch 00037: val_mse did not improve from 0.00682\n",
      "Epoch 38/2000\n",
      " - 3s - loss: 34.3229 - mse: 0.0241 - mae: 0.1155 - mape: 33.6630 - val_loss: 33.7704 - val_mse: 0.0091 - val_mae: 0.0700 - val_mape: 117.0313\n",
      "\n",
      "Epoch 00038: val_mse did not improve from 0.00682\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.006634204206056892.\n",
      "Epoch 39/2000\n",
      " - 3s - loss: 33.3845 - mse: 0.0227 - mae: 0.1118 - mape: 32.3963 - val_loss: 32.8841 - val_mse: 0.0115 - val_mae: 0.0799 - val_mape: 111.6968\n",
      "\n",
      "Epoch 00039: val_mse did not improve from 0.00682\n",
      "Epoch 40/2000\n",
      " - 3s - loss: 32.5150 - mse: 0.0291 - mae: 0.1269 - mape: 41.6772 - val_loss: 31.9817 - val_mse: 0.0052 - val_mae: 0.0493 - val_mape: 67.4124\n",
      "\n",
      "Epoch 00040: val_mse improved from 0.00682 to 0.00520, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 41/2000\n",
      " - 3s - loss: 31.6470 - mse: 0.0262 - mae: 0.1196 - mape: 36.8339 - val_loss: 31.1860 - val_mse: 0.0172 - val_mae: 0.1020 - val_mape: 114.2176\n",
      "\n",
      "Epoch 00041: val_mse did not improve from 0.00520\n",
      "Epoch 42/2000\n",
      " - 3s - loss: 30.7983 - mse: 0.0214 - mae: 0.1083 - mape: 34.1352 - val_loss: 30.3117 - val_mse: 0.0060 - val_mae: 0.0533 - val_mape: 87.1560\n",
      "\n",
      "Epoch 00042: val_mse did not improve from 0.00520\n",
      "Epoch 43/2000\n",
      " - 3s - loss: 29.9812 - mse: 0.0209 - mae: 0.1063 - mape: 34.0236 - val_loss: 29.5419 - val_mse: 0.0123 - val_mae: 0.0868 - val_mape: 146.7789\n",
      "\n",
      "Epoch 00043: val_mse did not improve from 0.00520\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.006302493973635137.\n",
      "Epoch 44/2000\n",
      " - 3s - loss: 29.2084 - mse: 0.0216 - mae: 0.1079 - mape: 32.9203 - val_loss: 28.7651 - val_mse: 0.0057 - val_mae: 0.0532 - val_mape: 57.1021\n",
      "\n",
      "Epoch 00044: val_mse did not improve from 0.00520\n",
      "Epoch 45/2000\n",
      " - 3s - loss: 28.4895 - mse: 0.0277 - mae: 0.1231 - mape: 33.2475 - val_loss: 28.0972 - val_mse: 0.0204 - val_mae: 0.1095 - val_mape: 90.3243\n",
      "\n",
      "Epoch 00045: val_mse did not improve from 0.00520\n",
      "Epoch 46/2000\n",
      " - 3s - loss: 27.7687 - mse: 0.0253 - mae: 0.1176 - mape: 31.0548 - val_loss: 27.4067 - val_mse: 0.0221 - val_mae: 0.1246 - val_mape: 32.0061\n",
      "\n",
      "Epoch 00046: val_mse did not improve from 0.00520\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.005987369385547936.\n",
      "Epoch 47/2000\n",
      " - 3s - loss: 27.0793 - mse: 0.0218 - mae: 0.1090 - mape: 32.2281 - val_loss: 26.7140 - val_mse: 0.0126 - val_mae: 0.0855 - val_mape: 127.1205\n",
      "\n",
      "Epoch 00047: val_mse did not improve from 0.00520\n",
      "Epoch 48/2000\n",
      " - 3s - loss: 26.4481 - mse: 0.0277 - mae: 0.1239 - mape: 35.0360 - val_loss: 26.0601 - val_mse: 0.0090 - val_mae: 0.0694 - val_mape: 107.4363\n",
      "\n",
      "Epoch 00048: val_mse did not improve from 0.00520\n",
      "Epoch 49/2000\n",
      " - 3s - loss: 25.8110 - mse: 0.0253 - mae: 0.1172 - mape: 33.8069 - val_loss: 25.4230 - val_mse: 0.0061 - val_mae: 0.0547 - val_mape: 103.0893\n",
      "\n",
      "Epoch 00049: val_mse did not improve from 0.00520\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.005688000982627272.\n",
      "Epoch 50/2000\n",
      " - 3s - loss: 25.1973 - mse: 0.0202 - mae: 0.1043 - mape: 29.7267 - val_loss: 24.8438 - val_mse: 0.0055 - val_mae: 0.0528 - val_mape: 78.5875\n",
      "\n",
      "Epoch 00050: val_mse did not improve from 0.00520\n",
      "Epoch 51/2000\n",
      " - 3s - loss: 24.6322 - mse: 0.0217 - mae: 0.1102 - mape: 30.5679 - val_loss: 24.3254 - val_mse: 0.0151 - val_mae: 0.0984 - val_mape: 75.2911\n",
      "\n",
      "Epoch 00051: val_mse did not improve from 0.00520\n",
      "Epoch 52/2000\n",
      " - 3s - loss: 24.0681 - mse: 0.0200 - mae: 0.1039 - mape: 31.6913 - val_loss: 23.7239 - val_mse: 0.0048 - val_mae: 0.0480 - val_mape: 58.8094\n",
      "\n",
      "Epoch 00052: val_mse improved from 0.00520 to 0.00483, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 53/2000\n",
      " - 3s - loss: 23.5455 - mse: 0.0293 - mae: 0.1263 - mape: 36.6752 - val_loss: 23.1966 - val_mse: 0.0059 - val_mae: 0.0591 - val_mape: 32.4703\n",
      "\n",
      "Epoch 00053: val_mse did not improve from 0.00483\n",
      "Epoch 54/2000\n",
      " - 3s - loss: 22.9906 - mse: 0.0198 - mae: 0.1039 - mape: 31.2819 - val_loss: 22.6745 - val_mse: 0.0074 - val_mae: 0.0629 - val_mape: 112.9073\n",
      "\n",
      "Epoch 00054: val_mse did not improve from 0.00483\n",
      "Epoch 55/2000\n",
      " - 3s - loss: 22.4731 - mse: 0.0216 - mae: 0.1066 - mape: 31.6903 - val_loss: 22.1915 - val_mse: 0.0135 - val_mae: 0.0938 - val_mape: 79.8096\n",
      "\n",
      "Epoch 00055: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 0.005403600889258086.\n",
      "Epoch 56/2000\n",
      " - 3s - loss: 21.9801 - mse: 0.0222 - mae: 0.1100 - mape: 32.7759 - val_loss: 21.6866 - val_mse: 0.0082 - val_mae: 0.0662 - val_mape: 100.9108\n",
      "\n",
      "Epoch 00056: val_mse did not improve from 0.00483\n",
      "Epoch 57/2000\n",
      " - 3s - loss: 21.4929 - mse: 0.0172 - mae: 0.0951 - mape: 28.3272 - val_loss: 21.2333 - val_mse: 0.0101 - val_mae: 0.0797 - val_mape: 77.4618\n",
      "\n",
      "Epoch 00057: val_mse did not improve from 0.00483\n",
      "Epoch 58/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 21.0387 - mse: 0.0205 - mae: 0.1029 - mape: 34.1374 - val_loss: 20.7574 - val_mse: 0.0066 - val_mae: 0.0606 - val_mape: 89.3589\n",
      "\n",
      "Epoch 00058: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.005133421043865383.\n",
      "Epoch 59/2000\n",
      " - 3s - loss: 20.5926 - mse: 0.0181 - mae: 0.0983 - mape: 26.7921 - val_loss: 20.3644 - val_mse: 0.0138 - val_mae: 0.0922 - val_mape: 163.6552\n",
      "\n",
      "Epoch 00059: val_mse did not improve from 0.00483\n",
      "Epoch 60/2000\n",
      " - 3s - loss: 20.1899 - mse: 0.0248 - mae: 0.1159 - mape: 35.7101 - val_loss: 19.9528 - val_mse: 0.0153 - val_mae: 0.0965 - val_mape: 146.0314\n",
      "\n",
      "Epoch 00060: val_mse did not improve from 0.00483\n",
      "Epoch 61/2000\n",
      " - 3s - loss: 19.7648 - mse: 0.0196 - mae: 0.1026 - mape: 30.7845 - val_loss: 19.5041 - val_mse: 0.0059 - val_mae: 0.0550 - val_mape: 97.2747\n",
      "\n",
      "Epoch 00061: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 0.004876750102266669.\n",
      "Epoch 62/2000\n",
      " - 3s - loss: 19.3697 - mse: 0.0191 - mae: 0.1013 - mape: 35.6006 - val_loss: 19.1235 - val_mse: 0.0056 - val_mae: 0.0534 - val_mape: 119.5206\n",
      "\n",
      "Epoch 00062: val_mse did not improve from 0.00483\n",
      "Epoch 63/2000\n",
      " - 3s - loss: 18.9941 - mse: 0.0195 - mae: 0.1012 - mape: 31.4648 - val_loss: 18.7873 - val_mse: 0.0133 - val_mae: 0.0889 - val_mape: 147.6292\n",
      "\n",
      "Epoch 00063: val_mse did not improve from 0.00483\n",
      "Epoch 64/2000\n",
      " - 3s - loss: 18.6339 - mse: 0.0216 - mae: 0.1091 - mape: 32.8534 - val_loss: 18.3938 - val_mse: 0.0068 - val_mae: 0.0597 - val_mape: 137.6626\n",
      "\n",
      "Epoch 00064: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.004632912552915513.\n",
      "Epoch 65/2000\n",
      " - 3s - loss: 18.2662 - mse: 0.0169 - mae: 0.0938 - mape: 28.3069 - val_loss: 18.0538 - val_mse: 0.0069 - val_mae: 0.0591 - val_mape: 130.3784\n",
      "\n",
      "Epoch 00065: val_mse did not improve from 0.00483\n",
      "Epoch 66/2000\n",
      " - 3s - loss: 17.9411 - mse: 0.0202 - mae: 0.1052 - mape: 29.6981 - val_loss: 17.7627 - val_mse: 0.0150 - val_mae: 0.1012 - val_mape: 117.0628\n",
      "\n",
      "Epoch 00066: val_mse did not improve from 0.00483\n",
      "Epoch 67/2000\n",
      " - 3s - loss: 17.6150 - mse: 0.0225 - mae: 0.1093 - mape: 29.9060 - val_loss: 17.3857 - val_mse: 0.0055 - val_mae: 0.0511 - val_mape: 105.8152\n",
      "\n",
      "Epoch 00067: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0044012669473886485.\n",
      "Epoch 68/2000\n",
      " - 3s - loss: 17.2840 - mse: 0.0169 - mae: 0.0947 - mape: 33.7887 - val_loss: 17.1072 - val_mse: 0.0105 - val_mae: 0.0775 - val_mape: 141.7444\n",
      "\n",
      "Epoch 00068: val_mse did not improve from 0.00483\n",
      "Epoch 69/2000\n",
      " - 3s - loss: 16.9828 - mse: 0.0174 - mae: 0.0958 - mape: 31.5537 - val_loss: 16.7799 - val_mse: 0.0051 - val_mae: 0.0497 - val_mape: 109.6279\n",
      "\n",
      "Epoch 00069: val_mse did not improve from 0.00483\n",
      "Epoch 70/2000\n",
      " - 3s - loss: 16.7003 - mse: 0.0228 - mae: 0.1103 - mape: 29.9856 - val_loss: 16.5030 - val_mse: 0.0082 - val_mae: 0.0670 - val_mape: 124.1198\n",
      "\n",
      "Epoch 00070: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.004181203688494861.\n",
      "Epoch 71/2000\n",
      " - 3s - loss: 16.4046 - mse: 0.0185 - mae: 0.0995 - mape: 27.7655 - val_loss: 16.2175 - val_mse: 0.0063 - val_mae: 0.0561 - val_mape: 118.0941\n",
      "\n",
      "Epoch 00071: val_mse did not improve from 0.00483\n",
      "Epoch 72/2000\n",
      " - 3s - loss: 16.1308 - mse: 0.0184 - mae: 0.0981 - mape: 34.9625 - val_loss: 15.9942 - val_mse: 0.0173 - val_mae: 0.1029 - val_mape: 155.2169\n",
      "\n",
      "Epoch 00072: val_mse did not improve from 0.00483\n",
      "Epoch 73/2000\n",
      " - 3s - loss: 15.8653 - mse: 0.0185 - mae: 0.1004 - mape: 30.4332 - val_loss: 15.6865 - val_mse: 0.0068 - val_mae: 0.0606 - val_mape: 128.1102\n",
      "\n",
      "Epoch 00073: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.003972143703140319.\n",
      "Epoch 74/2000\n",
      " - 3s - loss: 15.6086 - mse: 0.0189 - mae: 0.1009 - mape: 29.2383 - val_loss: 15.4668 - val_mse: 0.0130 - val_mae: 0.0890 - val_mape: 154.9940\n",
      "\n",
      "Epoch 00074: val_mse did not improve from 0.00483\n",
      "Epoch 75/2000\n",
      " - 3s - loss: 15.3570 - mse: 0.0173 - mae: 0.0955 - mape: 32.4654 - val_loss: 15.2311 - val_mse: 0.0153 - val_mae: 0.0973 - val_mape: 140.8344\n",
      "\n",
      "Epoch 00075: val_mse did not improve from 0.00483\n",
      "Epoch 76/2000\n",
      " - 3s - loss: 15.1110 - mse: 0.0159 - mae: 0.0916 - mape: 32.8756 - val_loss: 14.9752 - val_mse: 0.0111 - val_mae: 0.0816 - val_mape: 116.9742\n",
      "\n",
      "Epoch 00076: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.0037735366728156804.\n",
      "Epoch 77/2000\n",
      " - 3s - loss: 14.8896 - mse: 0.0202 - mae: 0.1031 - mape: 35.3243 - val_loss: 14.7302 - val_mse: 0.0068 - val_mae: 0.0613 - val_mape: 104.8869\n",
      "\n",
      "Epoch 00077: val_mse did not improve from 0.00483\n",
      "Epoch 78/2000\n",
      " - 3s - loss: 14.6606 - mse: 0.0179 - mae: 0.0971 - mape: 31.9475 - val_loss: 14.5059 - val_mse: 0.0063 - val_mae: 0.0582 - val_mape: 91.5185\n",
      "\n",
      "Epoch 00078: val_mse did not improve from 0.00483\n",
      "Epoch 79/2000\n",
      " - 3s - loss: 14.4329 - mse: 0.0150 - mae: 0.0890 - mape: 25.7656 - val_loss: 14.3391 - val_mse: 0.0186 - val_mae: 0.1093 - val_mape: 119.0739\n",
      "\n",
      "Epoch 00079: val_mse did not improve from 0.00483\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.0035848599276505407.\n",
      "Epoch 80/2000\n",
      " - 3s - loss: 14.2247 - mse: 0.0163 - mae: 0.0921 - mape: 29.5574 - val_loss: 14.0696 - val_mse: 0.0040 - val_mae: 0.0437 - val_mape: 107.2427\n",
      "\n",
      "Epoch 00080: val_mse improved from 0.00483 to 0.00405, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 81/2000\n",
      " - 3s - loss: 14.0228 - mse: 0.0165 - mae: 0.0927 - mape: 26.6285 - val_loss: 13.8967 - val_mse: 0.0087 - val_mae: 0.0718 - val_mape: 98.3267\n",
      "\n",
      "Epoch 00081: val_mse did not improve from 0.00405\n",
      "Epoch 82/2000\n",
      " - 3s - loss: 13.8205 - mse: 0.0154 - mae: 0.0899 - mape: 31.4789 - val_loss: 13.6725 - val_mse: 0.0042 - val_mae: 0.0456 - val_mape: 134.5690\n",
      "\n",
      "Epoch 00082: val_mse did not improve from 0.00405\n",
      "Epoch 83/2000\n",
      " - 3s - loss: 13.6255 - mse: 0.0155 - mae: 0.0917 - mape: 28.2829 - val_loss: 13.4899 - val_mse: 0.0061 - val_mae: 0.0583 - val_mape: 78.1360\n",
      "\n",
      "Epoch 00083: val_mse did not improve from 0.00405\n",
      "\n",
      "Epoch 00083: ReduceLROnPlateau reducing learning rate to 0.003405616898089647.\n",
      "Epoch 84/2000\n",
      " - 3s - loss: 13.4348 - mse: 0.0150 - mae: 0.0903 - mape: 34.4270 - val_loss: 13.2913 - val_mse: 0.0037 - val_mae: 0.0425 - val_mape: 68.5502\n",
      "\n",
      "Epoch 00084: val_mse improved from 0.00405 to 0.00365, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 85/2000\n",
      " - 3s - loss: 13.2531 - mse: 0.0157 - mae: 0.0902 - mape: 26.1314 - val_loss: 13.1087 - val_mse: 0.0034 - val_mae: 0.0403 - val_mape: 68.4277\n",
      "\n",
      "Epoch 00085: val_mse improved from 0.00365 to 0.00339, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 86/2000\n",
      " - 3s - loss: 13.0744 - mse: 0.0157 - mae: 0.0907 - mape: 26.0213 - val_loss: 12.9412 - val_mse: 0.0050 - val_mae: 0.0507 - val_mape: 88.1125\n",
      "\n",
      "Epoch 00086: val_mse did not improve from 0.00339\n",
      "Epoch 87/2000\n",
      " - 3s - loss: 12.9023 - mse: 0.0183 - mae: 0.0953 - mape: 28.3971 - val_loss: 12.8026 - val_mse: 0.0126 - val_mae: 0.0876 - val_mape: 82.9643\n",
      "\n",
      "Epoch 00087: val_mse did not improve from 0.00339\n",
      "Epoch 88/2000\n",
      " - 3s - loss: 12.7304 - mse: 0.0174 - mae: 0.0977 - mape: 35.1776 - val_loss: 12.5977 - val_mse: 0.0054 - val_mae: 0.0557 - val_mape: 77.7372\n",
      "\n",
      "Epoch 00088: val_mse did not improve from 0.00339\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0032353360089473425.\n",
      "Epoch 89/2000\n",
      " - 3s - loss: 12.5516 - mse: 0.0142 - mae: 0.0868 - mape: 30.7808 - val_loss: 12.4258 - val_mse: 0.0042 - val_mae: 0.0460 - val_mape: 51.3427\n",
      "\n",
      "Epoch 00089: val_mse did not improve from 0.00339\n",
      "Epoch 90/2000\n",
      " - 3s - loss: 12.3918 - mse: 0.0147 - mae: 0.0881 - mape: 26.8307 - val_loss: 12.2761 - val_mse: 0.0056 - val_mae: 0.0563 - val_mape: 114.1486\n",
      "\n",
      "Epoch 00090: val_mse did not improve from 0.00339\n",
      "Epoch 91/2000\n",
      " - 3s - loss: 12.2300 - mse: 0.0139 - mae: 0.0855 - mape: 26.0523 - val_loss: 12.0988 - val_mse: 0.0030 - val_mae: 0.0370 - val_mape: 61.7991\n",
      "\n",
      "Epoch 00091: val_mse improved from 0.00339 to 0.00303, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 92/2000\n",
      " - 3s - loss: 12.0817 - mse: 0.0173 - mae: 0.0942 - mape: 29.9877 - val_loss: 12.0090 - val_mse: 0.0149 - val_mae: 0.1032 - val_mape: 55.6140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00092: val_mse did not improve from 0.00303\n",
      "Epoch 93/2000\n",
      " - 3s - loss: 11.9286 - mse: 0.0175 - mae: 0.0961 - mape: 31.3668 - val_loss: 11.8381 - val_mse: 0.0117 - val_mae: 0.0863 - val_mape: 110.7061\n",
      "\n",
      "Epoch 00093: val_mse did not improve from 0.00303\n",
      "Epoch 94/2000\n",
      " - 3s - loss: 11.7687 - mse: 0.0150 - mae: 0.0892 - mape: 29.6201 - val_loss: 11.6819 - val_mse: 0.0100 - val_mae: 0.0820 - val_mape: 63.6396\n",
      "\n",
      "Epoch 00094: val_mse did not improve from 0.00303\n",
      "\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 0.003073569131083786.\n",
      "Epoch 95/2000\n",
      " - 3s - loss: 11.6219 - mse: 0.0154 - mae: 0.0897 - mape: 28.7807 - val_loss: 11.4954 - val_mse: 0.0030 - val_mae: 0.0379 - val_mape: 64.8196\n",
      "\n",
      "Epoch 00095: val_mse improved from 0.00303 to 0.00299, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 96/2000\n",
      " - 3s - loss: 11.4803 - mse: 0.0149 - mae: 0.0898 - mape: 27.4503 - val_loss: 11.3603 - val_mse: 0.0038 - val_mae: 0.0436 - val_mape: 96.9120\n",
      "\n",
      "Epoch 00096: val_mse did not improve from 0.00299\n",
      "Epoch 97/2000\n",
      " - 3s - loss: 11.3386 - mse: 0.0147 - mae: 0.0881 - mape: 27.3824 - val_loss: 11.2178 - val_mse: 0.0036 - val_mae: 0.0401 - val_mape: 58.4501\n",
      "\n",
      "Epoch 00097: val_mse did not improve from 0.00299\n",
      "\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.002919890696648508.\n",
      "Epoch 98/2000\n",
      " - 3s - loss: 11.2011 - mse: 0.0140 - mae: 0.0855 - mape: 27.6367 - val_loss: 11.0859 - val_mse: 0.0032 - val_mae: 0.0386 - val_mape: 92.3372\n",
      "\n",
      "Epoch 00098: val_mse did not improve from 0.00299\n",
      "Epoch 99/2000\n",
      " - 3s - loss: 11.0693 - mse: 0.0138 - mae: 0.0835 - mape: 24.7881 - val_loss: 11.0018 - val_mse: 0.0114 - val_mae: 0.0835 - val_mape: 103.5456\n",
      "\n",
      "Epoch 00099: val_mse did not improve from 0.00299\n",
      "Epoch 100/2000\n",
      " - 3s - loss: 10.9405 - mse: 0.0132 - mae: 0.0829 - mape: 23.6069 - val_loss: 10.8544 - val_mse: 0.0061 - val_mae: 0.0635 - val_mape: 44.4975\n",
      "\n",
      "Epoch 00100: val_mse did not improve from 0.00299\n",
      "\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.0027738961507566273.\n",
      "Epoch 101/2000\n",
      " - 3s - loss: 10.8250 - mse: 0.0158 - mae: 0.0910 - mape: 30.1228 - val_loss: 10.7254 - val_mse: 0.0049 - val_mae: 0.0542 - val_mape: 81.1519\n",
      "\n",
      "Epoch 00101: val_mse did not improve from 0.00299\n",
      "Epoch 102/2000\n",
      " - 3s - loss: 10.7037 - mse: 0.0149 - mae: 0.0888 - mape: 28.4350 - val_loss: 10.6157 - val_mse: 0.0073 - val_mae: 0.0628 - val_mape: 76.9279\n",
      "\n",
      "Epoch 00102: val_mse did not improve from 0.00299\n",
      "Epoch 103/2000\n",
      " - 3s - loss: 10.5758 - mse: 0.0117 - mae: 0.0786 - mape: 27.0956 - val_loss: 10.4744 - val_mse: 0.0032 - val_mae: 0.0385 - val_mape: 111.8538\n",
      "\n",
      "Epoch 00103: val_mse did not improve from 0.00299\n",
      "\n",
      "Epoch 00103: ReduceLROnPlateau reducing learning rate to 0.002635201287921518.\n",
      "Epoch 104/2000\n",
      " - 3s - loss: 10.4669 - mse: 0.0131 - mae: 0.0833 - mape: 27.5131 - val_loss: 10.3812 - val_mse: 0.0050 - val_mae: 0.0553 - val_mape: 68.6032\n",
      "\n",
      "Epoch 00104: val_mse did not improve from 0.00299\n",
      "Epoch 105/2000\n",
      " - 3s - loss: 10.3532 - mse: 0.0122 - mae: 0.0790 - mape: 26.3734 - val_loss: 10.2629 - val_mse: 0.0042 - val_mae: 0.0457 - val_mape: 98.1999\n",
      "\n",
      "Epoch 00105: val_mse did not improve from 0.00299\n",
      "Epoch 106/2000\n",
      " - 3s - loss: 10.2561 - mse: 0.0157 - mae: 0.0902 - mape: 26.3554 - val_loss: 10.1445 - val_mse: 0.0028 - val_mae: 0.0350 - val_mape: 105.8990\n",
      "\n",
      "Epoch 00106: val_mse improved from 0.00299 to 0.00277, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 107/2000\n",
      " - 3s - loss: 10.1423 - mse: 0.0131 - mae: 0.0835 - mape: 29.1975 - val_loss: 10.0387 - val_mse: 0.0029 - val_mae: 0.0356 - val_mape: 83.3844\n",
      "\n",
      "Epoch 00107: val_mse did not improve from 0.00277\n",
      "Epoch 108/2000\n",
      " - 3s - loss: 10.0435 - mse: 0.0155 - mae: 0.0906 - mape: 27.5346 - val_loss: 9.9406 - val_mse: 0.0036 - val_mae: 0.0429 - val_mape: 120.4417\n",
      "\n",
      "Epoch 00108: val_mse did not improve from 0.00277\n",
      "Epoch 109/2000\n",
      " - 3s - loss: 9.9308 - mse: 0.0129 - mae: 0.0828 - mape: 30.8633 - val_loss: 9.8799 - val_mse: 0.0112 - val_mae: 0.0865 - val_mape: 87.5562\n",
      "\n",
      "Epoch 00109: val_mse did not improve from 0.00277\n",
      "\n",
      "Epoch 00109: ReduceLROnPlateau reducing learning rate to 0.002503441146109253.\n",
      "Epoch 110/2000\n",
      " - 3s - loss: 9.8325 - mse: 0.0145 - mae: 0.0857 - mape: 28.4512 - val_loss: 9.7399 - val_mse: 0.0035 - val_mae: 0.0445 - val_mape: 70.4155\n",
      "\n",
      "Epoch 00110: val_mse did not improve from 0.00277\n",
      "Epoch 111/2000\n",
      " - 3s - loss: 9.7368 - mse: 0.0146 - mae: 0.0876 - mape: 29.0734 - val_loss: 9.6628 - val_mse: 0.0072 - val_mae: 0.0644 - val_mape: 99.0644\n",
      "\n",
      "Epoch 00111: val_mse did not improve from 0.00277\n",
      "Epoch 112/2000\n",
      " - 3s - loss: 9.6347 - mse: 0.0131 - mae: 0.0820 - mape: 29.8002 - val_loss: 9.5399 - val_mse: 0.0028 - val_mae: 0.0376 - val_mape: 70.2978\n",
      "\n",
      "Epoch 00112: val_mse did not improve from 0.00277\n",
      "\n",
      "Epoch 00112: ReduceLROnPlateau reducing learning rate to 0.002378269121982157.\n",
      "Epoch 113/2000\n",
      " - 3s - loss: 9.5442 - mse: 0.0138 - mae: 0.0849 - mape: 29.0639 - val_loss: 9.4661 - val_mse: 0.0054 - val_mae: 0.0542 - val_mape: 90.7895\n",
      "\n",
      "Epoch 00113: val_mse did not improve from 0.00277\n",
      "Epoch 114/2000\n",
      " - 3s - loss: 9.4503 - mse: 0.0128 - mae: 0.0809 - mape: 27.0793 - val_loss: 9.3813 - val_mse: 0.0056 - val_mae: 0.0588 - val_mape: 75.2194\n",
      "\n",
      "Epoch 00114: val_mse did not improve from 0.00277\n",
      "Epoch 115/2000\n",
      " - 3s - loss: 9.3627 - mse: 0.0132 - mae: 0.0824 - mape: 24.1017 - val_loss: 9.2852 - val_mse: 0.0049 - val_mae: 0.0514 - val_mape: 97.0608\n",
      "\n",
      "Epoch 00115: val_mse did not improve from 0.00277\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 0.002259355643764138.\n",
      "Epoch 116/2000\n",
      " - 3s - loss: 9.2794 - mse: 0.0144 - mae: 0.0853 - mape: 23.6301 - val_loss: 9.1890 - val_mse: 0.0032 - val_mae: 0.0386 - val_mape: 98.9371\n",
      "\n",
      "Epoch 00116: val_mse did not improve from 0.00277\n",
      "Epoch 117/2000\n",
      " - 3s - loss: 9.1989 - mse: 0.0148 - mae: 0.0878 - mape: 27.7359 - val_loss: 9.1347 - val_mse: 0.0065 - val_mae: 0.0669 - val_mape: 40.7041\n",
      "\n",
      "Epoch 00117: val_mse did not improve from 0.00277\n",
      "Epoch 118/2000\n",
      " - 3s - loss: 9.1065 - mse: 0.0115 - mae: 0.0778 - mape: 22.7092 - val_loss: 9.0178 - val_mse: 0.0026 - val_mae: 0.0320 - val_mape: 86.2748\n",
      "\n",
      "Epoch 00118: val_mse improved from 0.00277 to 0.00260, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 119/2000\n",
      " - 3s - loss: 9.0302 - mse: 0.0132 - mae: 0.0830 - mape: 27.4778 - val_loss: 8.9385 - val_mse: 0.0025 - val_mae: 0.0339 - val_mape: 62.6811\n",
      "\n",
      "Epoch 00119: val_mse improved from 0.00260 to 0.00254, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 120/2000\n",
      " - 3s - loss: 8.9511 - mse: 0.0143 - mae: 0.0847 - mape: 23.5860 - val_loss: 8.8562 - val_mse: 0.0025 - val_mae: 0.0320 - val_mape: 85.8087\n",
      "\n",
      "Epoch 00120: val_mse improved from 0.00254 to 0.00252, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 121/2000\n",
      " - 3s - loss: 8.8680 - mse: 0.0130 - mae: 0.0817 - mape: 23.0139 - val_loss: 8.7803 - val_mse: 0.0030 - val_mae: 0.0358 - val_mape: 88.4248\n",
      "\n",
      "Epoch 00121: val_mse did not improve from 0.00252\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 0.0021463879500515756.\n",
      "Epoch 122/2000\n",
      " - 3s - loss: 8.7909 - mse: 0.0129 - mae: 0.0821 - mape: 26.5050 - val_loss: 8.7015 - val_mse: 0.0025 - val_mae: 0.0321 - val_mape: 96.0038\n",
      "\n",
      "Epoch 00122: val_mse improved from 0.00252 to 0.00250, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 123/2000\n",
      " - 3s - loss: 8.7264 - mse: 0.0166 - mae: 0.0924 - mape: 26.2575 - val_loss: 8.6440 - val_mse: 0.0042 - val_mae: 0.0489 - val_mape: 82.2170\n",
      "\n",
      "Epoch 00123: val_mse did not improve from 0.00250\n",
      "Epoch 124/2000\n",
      " - 3s - loss: 8.6469 - mse: 0.0153 - mae: 0.0870 - mape: 29.1928 - val_loss: 8.5671 - val_mse: 0.0036 - val_mae: 0.0459 - val_mape: 55.0130\n",
      "\n",
      "Epoch 00124: val_mse did not improve from 0.00250\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 0.0020390685414895414.\n",
      "Epoch 125/2000\n",
      " - 3s - loss: 8.5637 - mse: 0.0112 - mae: 0.0755 - mape: 22.9448 - val_loss: 8.4946 - val_mse: 0.0034 - val_mae: 0.0429 - val_mape: 71.7557\n",
      "\n",
      "Epoch 00125: val_mse did not improve from 0.00250\n",
      "Epoch 126/2000\n",
      " - 3s - loss: 8.4956 - mse: 0.0118 - mae: 0.0766 - mape: 22.8204 - val_loss: 8.4139 - val_mse: 0.0024 - val_mae: 0.0311 - val_mape: 73.0788\n",
      "\n",
      "Epoch 00126: val_mse improved from 0.00250 to 0.00244, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 127/2000\n",
      " - 3s - loss: 8.4265 - mse: 0.0110 - mae: 0.0761 - mape: 21.2526 - val_loss: 8.3446 - val_mse: 0.0023 - val_mae: 0.0301 - val_mape: 70.3531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00127: val_mse improved from 0.00244 to 0.00235, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 128/2000\n",
      " - 3s - loss: 8.3603 - mse: 0.0120 - mae: 0.0781 - mape: 24.1096 - val_loss: 8.2807 - val_mse: 0.0028 - val_mae: 0.0340 - val_mape: 72.9257\n",
      "\n",
      "Epoch 00128: val_mse did not improve from 0.00235\n",
      "Epoch 129/2000\n",
      " - 3s - loss: 8.2956 - mse: 0.0123 - mae: 0.0809 - mape: 28.6223 - val_loss: 8.2237 - val_mse: 0.0038 - val_mae: 0.0442 - val_mape: 77.9479\n",
      "\n",
      "Epoch 00129: val_mse did not improve from 0.00235\n",
      "\n",
      "Epoch 00129: ReduceLROnPlateau reducing learning rate to 0.0019371151807717978.\n",
      "Epoch 130/2000\n",
      " - 3s - loss: 8.2254 - mse: 0.0116 - mae: 0.0760 - mape: 22.1988 - val_loss: 8.1518 - val_mse: 0.0026 - val_mae: 0.0357 - val_mape: 63.1978\n",
      "\n",
      "Epoch 00130: val_mse did not improve from 0.00235\n",
      "Epoch 131/2000\n",
      " - 3s - loss: 8.1612 - mse: 0.0110 - mae: 0.0750 - mape: 23.4439 - val_loss: 8.0857 - val_mse: 0.0024 - val_mae: 0.0324 - val_mape: 54.3460\n",
      "\n",
      "Epoch 00131: val_mse did not improve from 0.00235\n",
      "Epoch 132/2000\n",
      " - 3s - loss: 8.1043 - mse: 0.0123 - mae: 0.0807 - mape: 28.5475 - val_loss: 8.0257 - val_mse: 0.0029 - val_mae: 0.0349 - val_mape: 69.6168\n",
      "\n",
      "Epoch 00132: val_mse did not improve from 0.00235\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 0.0018402594549115747.\n",
      "Epoch 133/2000\n",
      " - 3s - loss: 8.0416 - mse: 0.0118 - mae: 0.0787 - mape: 23.0498 - val_loss: 7.9621 - val_mse: 0.0023 - val_mae: 0.0301 - val_mape: 59.7027\n",
      "\n",
      "Epoch 00133: val_mse improved from 0.00235 to 0.00233, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 134/2000\n",
      " - 3s - loss: 7.9824 - mse: 0.0115 - mae: 0.0781 - mape: 23.7565 - val_loss: 7.9106 - val_mse: 0.0030 - val_mae: 0.0369 - val_mape: 78.3527\n",
      "\n",
      "Epoch 00134: val_mse did not improve from 0.00233\n",
      "Epoch 135/2000\n",
      " - 3s - loss: 7.9262 - mse: 0.0128 - mae: 0.0801 - mape: 25.6167 - val_loss: 7.8506 - val_mse: 0.0028 - val_mae: 0.0349 - val_mape: 76.7906\n",
      "\n",
      "Epoch 00135: val_mse did not improve from 0.00233\n",
      "Epoch 136/2000\n",
      " - 3s - loss: 7.8682 - mse: 0.0128 - mae: 0.0798 - mape: 23.3501 - val_loss: 7.8164 - val_mse: 0.0059 - val_mae: 0.0582 - val_mape: 83.1801\n",
      "\n",
      "Epoch 00136: val_mse did not improve from 0.00233\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 0.0017482464434579014.\n",
      "Epoch 137/2000\n",
      " - 3s - loss: 7.8088 - mse: 0.0115 - mae: 0.0764 - mape: 21.1159 - val_loss: 7.7366 - val_mse: 0.0028 - val_mae: 0.0326 - val_mape: 53.6267\n",
      "\n",
      "Epoch 00137: val_mse did not improve from 0.00233\n",
      "Epoch 138/2000\n",
      " - 3s - loss: 7.7538 - mse: 0.0118 - mae: 0.0755 - mape: 22.9960 - val_loss: 7.6807 - val_mse: 0.0025 - val_mae: 0.0307 - val_mape: 60.0253\n",
      "\n",
      "Epoch 00138: val_mse did not improve from 0.00233\n",
      "Epoch 139/2000\n",
      " - 3s - loss: 7.7053 - mse: 0.0124 - mae: 0.0807 - mape: 27.2930 - val_loss: 7.6274 - val_mse: 0.0024 - val_mae: 0.0308 - val_mape: 31.3412\n",
      "\n",
      "Epoch 00139: val_mse did not improve from 0.00233\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 0.0016608341655228287.\n",
      "Epoch 140/2000\n",
      " - 3s - loss: 7.6529 - mse: 0.0126 - mae: 0.0804 - mape: 28.8229 - val_loss: 7.5843 - val_mse: 0.0028 - val_mae: 0.0382 - val_mape: 50.8823\n",
      "\n",
      "Epoch 00140: val_mse did not improve from 0.00233\n",
      "Epoch 141/2000\n",
      " - 3s - loss: 7.5952 - mse: 0.0104 - mae: 0.0730 - mape: 23.6747 - val_loss: 7.5277 - val_mse: 0.0025 - val_mae: 0.0318 - val_mape: 87.2700\n",
      "\n",
      "Epoch 00141: val_mse did not improve from 0.00233\n",
      "Epoch 142/2000\n",
      " - 3s - loss: 7.5528 - mse: 0.0121 - mae: 0.0806 - mape: 27.8972 - val_loss: 7.4763 - val_mse: 0.0024 - val_mae: 0.0302 - val_mape: 54.4831\n",
      "\n",
      "Epoch 00142: val_mse did not improve from 0.00233\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 0.0015777924738358705.\n",
      "Epoch 143/2000\n",
      " - 3s - loss: 7.5045 - mse: 0.0129 - mae: 0.0807 - mape: 25.4127 - val_loss: 7.4396 - val_mse: 0.0029 - val_mae: 0.0405 - val_mape: 50.4360\n",
      "\n",
      "Epoch 00143: val_mse did not improve from 0.00233\n",
      "Epoch 144/2000\n",
      " - 3s - loss: 7.4565 - mse: 0.0127 - mae: 0.0796 - mape: 26.3220 - val_loss: 7.3808 - val_mse: 0.0021 - val_mae: 0.0284 - val_mape: 57.0505\n",
      "\n",
      "Epoch 00144: val_mse improved from 0.00233 to 0.00206, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 145/2000\n",
      " - 3s - loss: 7.4063 - mse: 0.0117 - mae: 0.0759 - mape: 24.8885 - val_loss: 7.3390 - val_mse: 0.0025 - val_mae: 0.0330 - val_mape: 71.1014\n",
      "\n",
      "Epoch 00145: val_mse did not improve from 0.00206\n",
      "Epoch 146/2000\n",
      " - 3s - loss: 7.3576 - mse: 0.0110 - mae: 0.0735 - mape: 26.2492 - val_loss: 7.3059 - val_mse: 0.0041 - val_mae: 0.0460 - val_mape: 79.5461\n",
      "\n",
      "Epoch 00146: val_mse did not improve from 0.00206\n",
      "Epoch 147/2000\n",
      " - 3s - loss: 7.3219 - mse: 0.0129 - mae: 0.0838 - mape: 32.1560 - val_loss: 7.2463 - val_mse: 0.0027 - val_mae: 0.0322 - val_mape: 53.2763\n",
      "\n",
      "Epoch 00147: val_mse did not improve from 0.00206\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.0014989028335548936.\n",
      "Epoch 148/2000\n",
      " - 3s - loss: 7.2741 - mse: 0.0128 - mae: 0.0806 - mape: 25.9393 - val_loss: 7.2056 - val_mse: 0.0025 - val_mae: 0.0347 - val_mape: 60.4762\n",
      "\n",
      "Epoch 00148: val_mse did not improve from 0.00206\n",
      "Epoch 149/2000\n",
      " - 3s - loss: 7.2251 - mse: 0.0110 - mae: 0.0747 - mape: 23.5610 - val_loss: 7.1562 - val_mse: 0.0021 - val_mae: 0.0284 - val_mape: 70.4773\n",
      "\n",
      "Epoch 00149: val_mse did not improve from 0.00206\n",
      "Epoch 150/2000\n",
      " - 3s - loss: 7.1824 - mse: 0.0110 - mae: 0.0749 - mape: 21.3464 - val_loss: 7.1130 - val_mse: 0.0020 - val_mae: 0.0279 - val_mape: 66.1663\n",
      "\n",
      "Epoch 00150: val_mse improved from 0.00206 to 0.00201, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "\n",
      "Epoch 00150: ReduceLROnPlateau reducing learning rate to 0.0014239576586987822.\n",
      "Epoch 151/2000\n",
      " - 3s - loss: 7.1470 - mse: 0.0125 - mae: 0.0811 - mape: 24.4492 - val_loss: 7.0730 - val_mse: 0.0022 - val_mae: 0.0283 - val_mape: 39.8310\n",
      "\n",
      "Epoch 00151: val_mse did not improve from 0.00201\n",
      "Epoch 152/2000\n",
      " - 3s - loss: 7.0991 - mse: 0.0103 - mae: 0.0735 - mape: 25.7976 - val_loss: 7.0321 - val_mse: 0.0020 - val_mae: 0.0275 - val_mape: 54.4629\n",
      "\n",
      "Epoch 00152: val_mse did not improve from 0.00201\n",
      "Epoch 153/2000\n",
      " - 3s - loss: 7.0652 - mse: 0.0127 - mae: 0.0796 - mape: 25.4590 - val_loss: 7.0186 - val_mse: 0.0050 - val_mae: 0.0539 - val_mape: 92.2644\n",
      "\n",
      "Epoch 00153: val_mse did not improve from 0.00201\n",
      "\n",
      "Epoch 00153: ReduceLROnPlateau reducing learning rate to 0.0013527597591746598.\n",
      "Epoch 154/2000\n",
      " - 3s - loss: 7.0235 - mse: 0.0116 - mae: 0.0768 - mape: 23.1240 - val_loss: 6.9618 - val_mse: 0.0025 - val_mae: 0.0349 - val_mape: 58.6134\n",
      "\n",
      "Epoch 00154: val_mse did not improve from 0.00201\n",
      "Epoch 155/2000\n",
      " - 3s - loss: 6.9810 - mse: 0.0101 - mae: 0.0718 - mape: 23.9210 - val_loss: 6.9176 - val_mse: 0.0022 - val_mae: 0.0282 - val_mape: 45.0914\n",
      "\n",
      "Epoch 00155: val_mse did not improve from 0.00201\n",
      "Epoch 156/2000\n",
      " - 3s - loss: 6.9439 - mse: 0.0103 - mae: 0.0722 - mape: 25.9622 - val_loss: 6.8860 - val_mse: 0.0027 - val_mae: 0.0338 - val_mape: 66.6709\n",
      "\n",
      "Epoch 00156: val_mse did not improve from 0.00201\n",
      "\n",
      "Epoch 00156: ReduceLROnPlateau reducing learning rate to 0.0012851217878051102.\n",
      "Epoch 157/2000\n",
      " - 3s - loss: 6.9091 - mse: 0.0106 - mae: 0.0737 - mape: 22.6360 - val_loss: 6.8567 - val_mse: 0.0038 - val_mae: 0.0398 - val_mape: 47.7450\n",
      "\n",
      "Epoch 00157: val_mse did not improve from 0.00201\n",
      "Epoch 158/2000\n",
      " - 3s - loss: 6.8764 - mse: 0.0117 - mae: 0.0761 - mape: 27.3290 - val_loss: 6.8196 - val_mse: 0.0027 - val_mae: 0.0378 - val_mape: 53.3079\n",
      "\n",
      "Epoch 00158: val_mse did not improve from 0.00201\n",
      "Epoch 159/2000\n",
      " - 3s - loss: 6.8443 - mse: 0.0118 - mae: 0.0790 - mape: 25.8938 - val_loss: 6.7917 - val_mse: 0.0044 - val_mae: 0.0447 - val_mape: 52.5377\n",
      "\n",
      "Epoch 00159: val_mse did not improve from 0.00201\n",
      "\n",
      "Epoch 00159: ReduceLROnPlateau reducing learning rate to 0.0012208656873553992.\n",
      "Epoch 160/2000\n",
      " - 3s - loss: 6.8064 - mse: 0.0110 - mae: 0.0751 - mape: 25.3364 - val_loss: 6.7483 - val_mse: 0.0028 - val_mae: 0.0342 - val_mape: 67.9474\n",
      "\n",
      "Epoch 00160: val_mse did not improve from 0.00201\n",
      "Epoch 161/2000\n",
      " - 3s - loss: 6.7724 - mse: 0.0108 - mae: 0.0740 - mape: 22.4559 - val_loss: 6.7241 - val_mse: 0.0042 - val_mae: 0.0429 - val_mape: 48.8146\n",
      "\n",
      "Epoch 00161: val_mse did not improve from 0.00201\n",
      "Epoch 162/2000\n",
      " - 3s - loss: 6.7499 - mse: 0.0141 - mae: 0.0842 - mape: 24.5974 - val_loss: 6.6813 - val_mse: 0.0026 - val_mae: 0.0328 - val_mape: 69.1283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00162: val_mse did not improve from 0.00201\n",
      "\n",
      "Epoch 00162: ReduceLROnPlateau reducing learning rate to 0.0011598223587498068.\n",
      "Epoch 163/2000\n",
      " - 3s - loss: 6.7077 - mse: 0.0109 - mae: 0.0738 - mape: 22.9665 - val_loss: 6.6468 - val_mse: 0.0024 - val_mae: 0.0291 - val_mape: 58.6170\n",
      "\n",
      "Epoch 00163: val_mse did not improve from 0.00201\n",
      "Epoch 164/2000\n",
      " - 3s - loss: 6.6765 - mse: 0.0106 - mae: 0.0733 - mape: 23.0815 - val_loss: 6.6169 - val_mse: 0.0021 - val_mae: 0.0299 - val_mape: 63.5565\n",
      "\n",
      "Epoch 00164: val_mse did not improve from 0.00201\n",
      "Epoch 165/2000\n",
      " - 3s - loss: 6.6461 - mse: 0.0109 - mae: 0.0736 - mape: 24.2773 - val_loss: 6.5911 - val_mse: 0.0025 - val_mae: 0.0347 - val_mape: 60.4566\n",
      "\n",
      "Epoch 00165: val_mse did not improve from 0.00201\n",
      "\n",
      "Epoch 00165: ReduceLROnPlateau reducing learning rate to 0.0011018312186934053.\n",
      "Epoch 166/2000\n",
      " - 3s - loss: 6.6173 - mse: 0.0108 - mae: 0.0747 - mape: 21.9615 - val_loss: 6.5543 - val_mse: 0.0019 - val_mae: 0.0268 - val_mape: 61.9975\n",
      "\n",
      "Epoch 00166: val_mse improved from 0.00201 to 0.00193, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 167/2000\n",
      " - 3s - loss: 6.5891 - mse: 0.0110 - mae: 0.0753 - mape: 25.2138 - val_loss: 6.5320 - val_mse: 0.0023 - val_mae: 0.0333 - val_mape: 38.3504\n",
      "\n",
      "Epoch 00167: val_mse did not improve from 0.00193\n",
      "Epoch 168/2000\n",
      " - 3s - loss: 6.5542 - mse: 0.0097 - mae: 0.0692 - mape: 22.9437 - val_loss: 6.4990 - val_mse: 0.0021 - val_mae: 0.0290 - val_mape: 46.1153\n",
      "\n",
      "Epoch 00168: val_mse did not improve from 0.00193\n",
      "Epoch 169/2000\n",
      " - 3s - loss: 6.5293 - mse: 0.0105 - mae: 0.0728 - mape: 23.3781 - val_loss: 6.4763 - val_mse: 0.0027 - val_mae: 0.0348 - val_mape: 61.2054\n",
      "\n",
      "Epoch 00169: val_mse did not improve from 0.00193\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.0010467396466992795.\n",
      "Epoch 170/2000\n",
      " - 3s - loss: 6.5009 - mse: 0.0106 - mae: 0.0722 - mape: 25.0166 - val_loss: 6.4432 - val_mse: 0.0023 - val_mae: 0.0287 - val_mape: 43.8609\n",
      "\n",
      "Epoch 00170: val_mse did not improve from 0.00193\n",
      "Epoch 171/2000\n",
      " - 3s - loss: 6.4730 - mse: 0.0101 - mae: 0.0712 - mape: 21.0092 - val_loss: 6.4232 - val_mse: 0.0026 - val_mae: 0.0356 - val_mape: 65.3408\n",
      "\n",
      "Epoch 00171: val_mse did not improve from 0.00193\n",
      "Epoch 172/2000\n",
      " - 3s - loss: 6.4503 - mse: 0.0117 - mae: 0.0754 - mape: 23.0247 - val_loss: 6.3872 - val_mse: 0.0020 - val_mae: 0.0263 - val_mape: 52.7883\n",
      "\n",
      "Epoch 00172: val_mse did not improve from 0.00193\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 0.00099440265330486.\n",
      "Epoch 173/2000\n",
      " - 3s - loss: 6.4253 - mse: 0.0113 - mae: 0.0764 - mape: 24.3271 - val_loss: 6.3694 - val_mse: 0.0024 - val_mae: 0.0339 - val_mape: 50.7570\n",
      "\n",
      "Epoch 00173: val_mse did not improve from 0.00193\n",
      "Epoch 174/2000\n",
      " - 3s - loss: 6.4014 - mse: 0.0116 - mae: 0.0778 - mape: 24.4454 - val_loss: 6.3436 - val_mse: 0.0024 - val_mae: 0.0333 - val_mape: 55.0493\n",
      "\n",
      "Epoch 00174: val_mse did not improve from 0.00193\n",
      "Epoch 175/2000\n",
      " - 3s - loss: 6.3718 - mse: 0.0107 - mae: 0.0734 - mape: 26.0452 - val_loss: 6.3117 - val_mse: 0.0020 - val_mae: 0.0265 - val_mape: 40.6538\n",
      "\n",
      "Epoch 00175: val_mse did not improve from 0.00193\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 0.0009446825482882559.\n",
      "Epoch 176/2000\n",
      " - 3s - loss: 6.3483 - mse: 0.0109 - mae: 0.0744 - mape: 24.3845 - val_loss: 6.2891 - val_mse: 0.0020 - val_mae: 0.0277 - val_mape: 67.0889\n",
      "\n",
      "Epoch 00176: val_mse did not improve from 0.00193\n",
      "Epoch 177/2000\n",
      " - 3s - loss: 6.3210 - mse: 0.0100 - mae: 0.0708 - mape: 23.8495 - val_loss: 6.2710 - val_mse: 0.0023 - val_mae: 0.0332 - val_mape: 45.3618\n",
      "\n",
      "Epoch 00177: val_mse did not improve from 0.00193\n",
      "Epoch 178/2000\n",
      " - 3s - loss: 6.2980 - mse: 0.0101 - mae: 0.0714 - mape: 26.6111 - val_loss: 6.2417 - val_mse: 0.0019 - val_mae: 0.0275 - val_mape: 53.9802\n",
      "\n",
      "Epoch 00178: val_mse did not improve from 0.00193\n",
      "\n",
      "Epoch 00178: ReduceLROnPlateau reducing learning rate to 0.0008974484429927543.\n",
      "Epoch 179/2000\n",
      " - 3s - loss: 6.2774 - mse: 0.0107 - mae: 0.0738 - mape: 26.8857 - val_loss: 6.2180 - val_mse: 0.0019 - val_mae: 0.0262 - val_mape: 51.6939\n",
      "\n",
      "Epoch 00179: val_mse improved from 0.00193 to 0.00189, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 180/2000\n",
      " - 3s - loss: 6.2550 - mse: 0.0112 - mae: 0.0737 - mape: 22.5915 - val_loss: 6.1951 - val_mse: 0.0018 - val_mae: 0.0255 - val_mape: 60.4100\n",
      "\n",
      "Epoch 00180: val_mse improved from 0.00189 to 0.00184, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 181/2000\n",
      " - 3s - loss: 6.2298 - mse: 0.0098 - mae: 0.0707 - mape: 23.0324 - val_loss: 6.1766 - val_mse: 0.0020 - val_mae: 0.0292 - val_mape: 51.2266\n",
      "\n",
      "Epoch 00181: val_mse did not improve from 0.00184\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 0.0008525760291377082.\n",
      "Epoch 182/2000\n",
      " - 3s - loss: 6.2104 - mse: 0.0110 - mae: 0.0729 - mape: 23.1156 - val_loss: 6.1629 - val_mse: 0.0025 - val_mae: 0.0365 - val_mape: 47.6486\n",
      "\n",
      "Epoch 00182: val_mse did not improve from 0.00184\n",
      "Epoch 183/2000\n",
      " - 3s - loss: 6.1852 - mse: 0.0098 - mae: 0.0687 - mape: 19.8438 - val_loss: 6.1334 - val_mse: 0.0020 - val_mae: 0.0278 - val_mape: 55.0986\n",
      "\n",
      "Epoch 00183: val_mse did not improve from 0.00184\n",
      "Epoch 184/2000\n",
      " - 3s - loss: 6.1693 - mse: 0.0106 - mae: 0.0736 - mape: 24.0302 - val_loss: 6.1158 - val_mse: 0.0025 - val_mae: 0.0311 - val_mape: 49.0557\n",
      "\n",
      "Epoch 00184: val_mse did not improve from 0.00184\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 0.0008099472470348701.\n",
      "Epoch 185/2000\n",
      " - 3s - loss: 6.1434 - mse: 0.0089 - mae: 0.0681 - mape: 20.6367 - val_loss: 6.0937 - val_mse: 0.0023 - val_mae: 0.0287 - val_mape: 52.1302\n",
      "\n",
      "Epoch 00185: val_mse did not improve from 0.00184\n",
      "Epoch 186/2000\n",
      " - 3s - loss: 6.1252 - mse: 0.0098 - mae: 0.0696 - mape: 21.7970 - val_loss: 6.0724 - val_mse: 0.0021 - val_mae: 0.0271 - val_mape: 55.4372\n",
      "\n",
      "Epoch 00186: val_mse did not improve from 0.00184\n",
      "Epoch 187/2000\n",
      " - 3s - loss: 6.1104 - mse: 0.0107 - mae: 0.0744 - mape: 24.5836 - val_loss: 6.0510 - val_mse: 0.0018 - val_mae: 0.0254 - val_mape: 54.8184\n",
      "\n",
      "Epoch 00187: val_mse improved from 0.00184 to 0.00184, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 0.000769449898507446.\n",
      "Epoch 188/2000\n",
      " - 3s - loss: 6.0874 - mse: 0.0102 - mae: 0.0705 - mape: 20.7920 - val_loss: 6.0345 - val_mse: 0.0019 - val_mae: 0.0274 - val_mape: 48.0069\n",
      "\n",
      "Epoch 00188: val_mse did not improve from 0.00184\n",
      "Epoch 189/2000\n",
      " - 3s - loss: 6.0717 - mse: 0.0107 - mae: 0.0734 - mape: 24.4964 - val_loss: 6.0186 - val_mse: 0.0024 - val_mae: 0.0300 - val_mape: 54.8418\n",
      "\n",
      "Epoch 00189: val_mse did not improve from 0.00184\n",
      "Epoch 190/2000\n",
      " - 3s - loss: 6.0525 - mse: 0.0103 - mae: 0.0726 - mape: 22.8814 - val_loss: 5.9949 - val_mse: 0.0018 - val_mae: 0.0247 - val_mape: 60.0707\n",
      "\n",
      "Epoch 00190: val_mse improved from 0.00184 to 0.00181, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 191/2000\n",
      " - 3s - loss: 6.0335 - mse: 0.0106 - mae: 0.0720 - mape: 22.4379 - val_loss: 5.9802 - val_mse: 0.0023 - val_mae: 0.0284 - val_mape: 48.9100\n",
      "\n",
      "Epoch 00191: val_mse did not improve from 0.00181\n",
      "Epoch 192/2000\n",
      " - 3s - loss: 6.0112 - mse: 0.0094 - mae: 0.0681 - mape: 21.8470 - val_loss: 5.9592 - val_mse: 0.0019 - val_mae: 0.0258 - val_mape: 54.9084\n",
      "\n",
      "Epoch 00192: val_mse did not improve from 0.00181\n",
      "Epoch 193/2000\n",
      " - 3s - loss: 6.0006 - mse: 0.0112 - mae: 0.0758 - mape: 32.0160 - val_loss: 5.9413 - val_mse: 0.0019 - val_mae: 0.0262 - val_mape: 58.7433\n",
      "\n",
      "Epoch 00193: val_mse did not improve from 0.00181\n",
      "\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 0.0007309774257009848.\n",
      "Epoch 194/2000\n",
      " - 3s - loss: 5.9863 - mse: 0.0119 - mae: 0.0794 - mape: 28.8015 - val_loss: 5.9273 - val_mse: 0.0023 - val_mae: 0.0295 - val_mape: 49.6417\n",
      "\n",
      "Epoch 00194: val_mse did not improve from 0.00181\n",
      "Epoch 195/2000\n",
      " - 3s - loss: 5.9630 - mse: 0.0108 - mae: 0.0734 - mape: 21.4712 - val_loss: 5.9054 - val_mse: 0.0018 - val_mae: 0.0249 - val_mape: 57.1255\n",
      "\n",
      "Epoch 00195: val_mse improved from 0.00181 to 0.00179, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 196/2000\n",
      " - 3s - loss: 5.9413 - mse: 0.0096 - mae: 0.0689 - mape: 22.1505 - val_loss: 5.8895 - val_mse: 0.0019 - val_mae: 0.0261 - val_mape: 37.8176\n",
      "\n",
      "Epoch 00196: val_mse did not improve from 0.00179\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 0.0006944285792997107.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/2000\n",
      " - 3s - loss: 5.9308 - mse: 0.0116 - mae: 0.0752 - mape: 22.9198 - val_loss: 5.8862 - val_mse: 0.0032 - val_mae: 0.0391 - val_mape: 59.1772\n",
      "\n",
      "Epoch 00197: val_mse did not improve from 0.00179\n",
      "Epoch 198/2000\n",
      " - 3s - loss: 5.9098 - mse: 0.0101 - mae: 0.0705 - mape: 23.5737 - val_loss: 5.8560 - val_mse: 0.0018 - val_mae: 0.0252 - val_mape: 49.0703\n",
      "\n",
      "Epoch 00198: val_mse did not improve from 0.00179\n",
      "Epoch 199/2000\n",
      " - 3s - loss: 5.9007 - mse: 0.0114 - mae: 0.0776 - mape: 23.8075 - val_loss: 5.8434 - val_mse: 0.0020 - val_mae: 0.0288 - val_mape: 47.7146\n",
      "\n",
      "Epoch 00199: val_mse did not improve from 0.00179\n",
      "\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 0.0006597071420401335.\n",
      "Epoch 200/2000\n",
      " - 3s - loss: 5.8776 - mse: 0.0099 - mae: 0.0703 - mape: 20.2669 - val_loss: 5.8271 - val_mse: 0.0022 - val_mae: 0.0279 - val_mape: 44.1498\n",
      "\n",
      "Epoch 00200: val_mse did not improve from 0.00179\n",
      "Epoch 201/2000\n",
      " - 3s - loss: 5.8602 - mse: 0.0094 - mae: 0.0682 - mape: 23.2617 - val_loss: 5.8126 - val_mse: 0.0020 - val_mae: 0.0287 - val_mape: 47.5716\n",
      "\n",
      "Epoch 00201: val_mse did not improve from 0.00179\n",
      "Epoch 202/2000\n",
      " - 3s - loss: 5.8471 - mse: 0.0100 - mae: 0.0705 - mape: 25.3086 - val_loss: 5.7937 - val_mse: 0.0018 - val_mae: 0.0251 - val_mape: 58.5755\n",
      "\n",
      "Epoch 00202: val_mse did not improve from 0.00179\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 0.0006267217628192157.\n",
      "Epoch 203/2000\n",
      " - 3s - loss: 5.8353 - mse: 0.0107 - mae: 0.0736 - mape: 26.3780 - val_loss: 5.7792 - val_mse: 0.0018 - val_mae: 0.0250 - val_mape: 66.1239\n",
      "\n",
      "Epoch 00203: val_mse improved from 0.00179 to 0.00179, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 204/2000\n",
      " - 3s - loss: 5.8231 - mse: 0.0119 - mae: 0.0759 - mape: 28.4910 - val_loss: 5.7693 - val_mse: 0.0024 - val_mae: 0.0296 - val_mape: 45.7398\n",
      "\n",
      "Epoch 00204: val_mse did not improve from 0.00179\n",
      "Epoch 205/2000\n",
      " - 3s - loss: 5.8027 - mse: 0.0100 - mae: 0.0699 - mape: 22.3963 - val_loss: 5.7516 - val_mse: 0.0019 - val_mae: 0.0263 - val_mape: 58.5456\n",
      "\n",
      "Epoch 00205: val_mse did not improve from 0.00179\n",
      "\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 0.0005953856802079826.\n",
      "Epoch 206/2000\n",
      " - 3s - loss: 5.7933 - mse: 0.0108 - mae: 0.0745 - mape: 24.3362 - val_loss: 5.7379 - val_mse: 0.0018 - val_mae: 0.0263 - val_mape: 68.0110\n",
      "\n",
      "Epoch 00206: val_mse did not improve from 0.00179\n",
      "Epoch 207/2000\n",
      " - 3s - loss: 5.7768 - mse: 0.0107 - mae: 0.0716 - mape: 23.8319 - val_loss: 5.7225 - val_mse: 0.0018 - val_mae: 0.0244 - val_mape: 56.2132\n",
      "\n",
      "Epoch 00207: val_mse improved from 0.00179 to 0.00176, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 208/2000\n",
      " - 3s - loss: 5.7674 - mse: 0.0111 - mae: 0.0758 - mape: 23.6636 - val_loss: 5.7122 - val_mse: 0.0019 - val_mae: 0.0278 - val_mape: 45.1852\n",
      "\n",
      "Epoch 00208: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00208: ReduceLROnPlateau reducing learning rate to 0.0005656163906678557.\n",
      "Epoch 209/2000\n",
      " - 3s - loss: 5.7534 - mse: 0.0109 - mae: 0.0751 - mape: 25.7247 - val_loss: 5.6969 - val_mse: 0.0018 - val_mae: 0.0254 - val_mape: 60.0043\n",
      "\n",
      "Epoch 00209: val_mse did not improve from 0.00176\n",
      "Epoch 210/2000\n",
      " - 3s - loss: 5.7350 - mse: 0.0093 - mae: 0.0696 - mape: 23.4769 - val_loss: 5.6869 - val_mse: 0.0021 - val_mae: 0.0282 - val_mape: 50.0399\n",
      "\n",
      "Epoch 00210: val_mse did not improve from 0.00176\n",
      "Epoch 211/2000\n",
      " - 3s - loss: 5.7200 - mse: 0.0092 - mae: 0.0674 - mape: 23.8986 - val_loss: 5.6738 - val_mse: 0.0021 - val_mae: 0.0280 - val_mape: 48.5206\n",
      "\n",
      "Epoch 00211: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 0.0005373355932533741.\n",
      "Epoch 212/2000\n",
      " - 3s - loss: 5.7145 - mse: 0.0111 - mae: 0.0744 - mape: 24.7901 - val_loss: 5.6582 - val_mse: 0.0018 - val_mae: 0.0245 - val_mape: 44.7209\n",
      "\n",
      "Epoch 00212: val_mse did not improve from 0.00176\n",
      "Epoch 213/2000\n",
      " - 3s - loss: 5.6973 - mse: 0.0094 - mae: 0.0693 - mape: 19.9660 - val_loss: 5.6470 - val_mse: 0.0018 - val_mae: 0.0254 - val_mape: 45.2995\n",
      "\n",
      "Epoch 00213: val_mse did not improve from 0.00176\n",
      "Epoch 214/2000\n",
      " - 3s - loss: 5.6833 - mse: 0.0091 - mae: 0.0675 - mape: 22.2489 - val_loss: 5.6344 - val_mse: 0.0018 - val_mae: 0.0249 - val_mape: 44.6169\n",
      "\n",
      "Epoch 00214: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 0.0005104688025312498.\n",
      "Epoch 215/2000\n",
      " - 3s - loss: 5.6794 - mse: 0.0114 - mae: 0.0754 - mape: 26.8457 - val_loss: 5.6257 - val_mse: 0.0019 - val_mae: 0.0276 - val_mape: 45.2471\n",
      "\n",
      "Epoch 00215: val_mse did not improve from 0.00176\n",
      "Epoch 216/2000\n",
      " - 3s - loss: 5.6578 - mse: 0.0085 - mae: 0.0652 - mape: 20.9477 - val_loss: 5.6113 - val_mse: 0.0018 - val_mae: 0.0247 - val_mape: 46.5608\n",
      "\n",
      "Epoch 00216: val_mse did not improve from 0.00176\n",
      "Epoch 217/2000\n",
      " - 3s - loss: 5.6539 - mse: 0.0102 - mae: 0.0727 - mape: 25.2068 - val_loss: 5.6068 - val_mse: 0.0021 - val_mae: 0.0317 - val_mape: 41.9218\n",
      "\n",
      "Epoch 00217: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 0.00048494534858036783.\n",
      "Epoch 218/2000\n",
      " - 3s - loss: 5.6417 - mse: 0.0105 - mae: 0.0717 - mape: 23.2126 - val_loss: 5.5947 - val_mse: 0.0020 - val_mae: 0.0304 - val_mape: 44.8250\n",
      "\n",
      "Epoch 00218: val_mse did not improve from 0.00176\n",
      "Epoch 219/2000\n",
      " - 3s - loss: 5.6270 - mse: 0.0094 - mae: 0.0678 - mape: 21.3791 - val_loss: 5.5824 - val_mse: 0.0021 - val_mae: 0.0288 - val_mape: 56.5839\n",
      "\n",
      "Epoch 00219: val_mse did not improve from 0.00176\n",
      "Epoch 220/2000\n",
      " - 3s - loss: 5.6269 - mse: 0.0115 - mae: 0.0785 - mape: 26.2424 - val_loss: 5.5724 - val_mse: 0.0023 - val_mae: 0.0296 - val_mape: 49.3702\n",
      "\n",
      "Epoch 00220: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00220: ReduceLROnPlateau reducing learning rate to 0.0004606980728567578.\n",
      "Epoch 221/2000\n",
      " - 3s - loss: 5.6110 - mse: 0.0104 - mae: 0.0732 - mape: 26.3177 - val_loss: 5.5581 - val_mse: 0.0018 - val_mae: 0.0256 - val_mape: 45.7863\n",
      "\n",
      "Epoch 00221: val_mse did not improve from 0.00176\n",
      "Epoch 222/2000\n",
      " - 3s - loss: 5.5985 - mse: 0.0104 - mae: 0.0709 - mape: 22.4191 - val_loss: 5.5511 - val_mse: 0.0023 - val_mae: 0.0288 - val_mape: 46.6605\n",
      "\n",
      "Epoch 00222: val_mse did not improve from 0.00176\n",
      "Epoch 223/2000\n",
      " - 3s - loss: 5.5867 - mse: 0.0096 - mae: 0.0692 - mape: 22.0553 - val_loss: 5.5378 - val_mse: 0.0018 - val_mae: 0.0257 - val_mape: 37.4692\n",
      "\n",
      "Epoch 00223: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 0.00043766316230176017.\n",
      "Epoch 224/2000\n",
      " - 3s - loss: 5.5790 - mse: 0.0102 - mae: 0.0715 - mape: 23.4854 - val_loss: 5.5265 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 51.9144\n",
      "\n",
      "Epoch 00224: val_mse did not improve from 0.00176\n",
      "Epoch 225/2000\n",
      " - 3s - loss: 5.5706 - mse: 0.0106 - mae: 0.0728 - mape: 23.4428 - val_loss: 5.5177 - val_mse: 0.0018 - val_mae: 0.0250 - val_mape: 44.1409\n",
      "\n",
      "Epoch 00225: val_mse improved from 0.00176 to 0.00176, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 226/2000\n",
      " - 3s - loss: 5.5617 - mse: 0.0107 - mae: 0.0735 - mape: 22.6026 - val_loss: 5.5076 - val_mse: 0.0018 - val_mae: 0.0244 - val_mape: 44.7570\n",
      "\n",
      "Epoch 00226: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00226: ReduceLROnPlateau reducing learning rate to 0.00041578001109883185.\n",
      "Epoch 227/2000\n",
      " - 3s - loss: 5.5477 - mse: 0.0099 - mae: 0.0689 - mape: 23.5882 - val_loss: 5.4994 - val_mse: 0.0018 - val_mae: 0.0253 - val_mape: 42.2589\n",
      "\n",
      "Epoch 00227: val_mse did not improve from 0.00176\n",
      "Epoch 228/2000\n",
      " - 3s - loss: 5.5380 - mse: 0.0094 - mae: 0.0683 - mape: 22.2739 - val_loss: 5.4895 - val_mse: 0.0018 - val_mae: 0.0246 - val_mape: 43.3580\n",
      "\n",
      "Epoch 00228: val_mse did not improve from 0.00176\n",
      "Epoch 229/2000\n",
      " - 3s - loss: 5.5303 - mse: 0.0100 - mae: 0.0698 - mape: 21.8711 - val_loss: 5.4850 - val_mse: 0.0019 - val_mae: 0.0292 - val_mape: 44.2499\n",
      "\n",
      "Epoch 00229: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 0.0003949909994844347.\n",
      "Epoch 230/2000\n",
      " - 3s - loss: 5.5199 - mse: 0.0093 - mae: 0.0682 - mape: 23.4680 - val_loss: 5.4714 - val_mse: 0.0018 - val_mae: 0.0242 - val_mape: 47.0786\n",
      "\n",
      "Epoch 00230: val_mse did not improve from 0.00176\n",
      "Epoch 231/2000\n",
      " - 3s - loss: 5.5105 - mse: 0.0091 - mae: 0.0674 - mape: 23.7615 - val_loss: 5.4640 - val_mse: 0.0019 - val_mae: 0.0254 - val_mape: 44.2678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00231: val_mse did not improve from 0.00176\n",
      "Epoch 232/2000\n",
      " - 3s - loss: 5.5028 - mse: 0.0099 - mae: 0.0683 - mape: 23.4999 - val_loss: 5.4553 - val_mse: 0.0019 - val_mae: 0.0253 - val_mape: 45.1661\n",
      "\n",
      "Epoch 00232: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00232: ReduceLROnPlateau reducing learning rate to 0.00037524143845075737.\n",
      "Epoch 233/2000\n",
      " - 3s - loss: 5.4971 - mse: 0.0098 - mae: 0.0710 - mape: 21.2783 - val_loss: 5.4486 - val_mse: 0.0020 - val_mae: 0.0268 - val_mape: 55.1216\n",
      "\n",
      "Epoch 00233: val_mse did not improve from 0.00176\n",
      "Epoch 234/2000\n",
      " - 3s - loss: 5.4908 - mse: 0.0107 - mae: 0.0729 - mape: 23.1084 - val_loss: 5.4381 - val_mse: 0.0018 - val_mae: 0.0244 - val_mape: 53.1704\n",
      "\n",
      "Epoch 00234: val_mse did not improve from 0.00176\n",
      "Epoch 235/2000\n",
      " - 3s - loss: 5.4844 - mse: 0.0105 - mae: 0.0746 - mape: 22.7396 - val_loss: 5.4310 - val_mse: 0.0018 - val_mae: 0.0255 - val_mape: 45.2994\n",
      "\n",
      "Epoch 00235: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 0.00035647937620524316.\n",
      "Epoch 236/2000\n",
      " - 3s - loss: 5.4721 - mse: 0.0098 - mae: 0.0703 - mape: 21.5914 - val_loss: 5.4241 - val_mse: 0.0018 - val_mae: 0.0264 - val_mape: 44.2538\n",
      "\n",
      "Epoch 00236: val_mse did not improve from 0.00176\n",
      "Epoch 237/2000\n",
      " - 3s - loss: 5.4613 - mse: 0.0092 - mae: 0.0672 - mape: 18.6445 - val_loss: 5.4150 - val_mse: 0.0018 - val_mae: 0.0249 - val_mape: 51.7248\n",
      "\n",
      "Epoch 00237: val_mse did not improve from 0.00176\n",
      "Epoch 238/2000\n",
      " - 3s - loss: 5.4572 - mse: 0.0100 - mae: 0.0708 - mape: 21.5496 - val_loss: 5.4069 - val_mse: 0.0018 - val_mae: 0.0245 - val_mape: 46.3657\n",
      "\n",
      "Epoch 00238: val_mse did not improve from 0.00176\n",
      "\n",
      "Epoch 00238: ReduceLROnPlateau reducing learning rate to 0.0003386554046301171.\n",
      "Epoch 239/2000\n",
      " - 3s - loss: 5.4463 - mse: 0.0093 - mae: 0.0675 - mape: 19.3355 - val_loss: 5.3996 - val_mse: 0.0018 - val_mae: 0.0246 - val_mape: 49.4678\n",
      "\n",
      "Epoch 00239: val_mse did not improve from 0.00176\n",
      "Epoch 240/2000\n",
      " - 3s - loss: 5.4367 - mse: 0.0085 - mae: 0.0651 - mape: 22.9688 - val_loss: 5.3916 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 50.4976\n",
      "\n",
      "Epoch 00240: val_mse improved from 0.00176 to 0.00171, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 241/2000\n",
      " - 3s - loss: 5.4360 - mse: 0.0099 - mae: 0.0717 - mape: 24.0065 - val_loss: 5.3865 - val_mse: 0.0019 - val_mae: 0.0260 - val_mape: 53.3958\n",
      "\n",
      "Epoch 00241: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 0.00032172263163374735.\n",
      "Epoch 242/2000\n",
      " - 3s - loss: 5.4246 - mse: 0.0091 - mae: 0.0674 - mape: 23.1894 - val_loss: 5.3782 - val_mse: 0.0018 - val_mae: 0.0246 - val_mape: 51.9139\n",
      "\n",
      "Epoch 00242: val_mse did not improve from 0.00171\n",
      "Epoch 243/2000\n",
      " - 3s - loss: 5.4177 - mse: 0.0088 - mae: 0.0674 - mape: 20.7355 - val_loss: 5.3724 - val_mse: 0.0020 - val_mae: 0.0257 - val_mape: 44.5126\n",
      "\n",
      "Epoch 00243: val_mse did not improve from 0.00171\n",
      "Epoch 244/2000\n",
      " - 3s - loss: 5.4182 - mse: 0.0108 - mae: 0.0748 - mape: 25.9651 - val_loss: 5.3637 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 50.8811\n",
      "\n",
      "Epoch 00244: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 0.0003056364876101725.\n",
      "Epoch 245/2000\n",
      " - 3s - loss: 5.4015 - mse: 0.0084 - mae: 0.0649 - mape: 24.2926 - val_loss: 5.3577 - val_mse: 0.0018 - val_mae: 0.0244 - val_mape: 48.7213\n",
      "\n",
      "Epoch 00245: val_mse did not improve from 0.00171\n",
      "Epoch 246/2000\n",
      " - 3s - loss: 5.3976 - mse: 0.0091 - mae: 0.0675 - mape: 23.5611 - val_loss: 5.3517 - val_mse: 0.0018 - val_mae: 0.0250 - val_mape: 49.2123\n",
      "\n",
      "Epoch 00246: val_mse did not improve from 0.00171\n",
      "Epoch 247/2000\n",
      " - 3s - loss: 5.3900 - mse: 0.0085 - mae: 0.0664 - mape: 22.4644 - val_loss: 5.3440 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 49.5991\n",
      "\n",
      "Epoch 00247: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 0.0002903546701418236.\n",
      "Epoch 248/2000\n",
      " - 3s - loss: 5.3882 - mse: 0.0098 - mae: 0.0709 - mape: 21.6045 - val_loss: 5.3384 - val_mse: 0.0017 - val_mae: 0.0244 - val_mape: 43.7099\n",
      "\n",
      "Epoch 00248: val_mse did not improve from 0.00171\n",
      "Epoch 249/2000\n",
      " - 3s - loss: 5.3845 - mse: 0.0105 - mae: 0.0734 - mape: 23.9757 - val_loss: 5.3322 - val_mse: 0.0017 - val_mae: 0.0243 - val_mape: 47.5674\n",
      "\n",
      "Epoch 00249: val_mse did not improve from 0.00171\n",
      "Epoch 250/2000\n",
      " - 3s - loss: 5.3737 - mse: 0.0096 - mae: 0.0688 - mape: 21.6447 - val_loss: 5.3269 - val_mse: 0.0019 - val_mae: 0.0252 - val_mape: 45.8260\n",
      "\n",
      "Epoch 00250: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00250: ReduceLROnPlateau reducing learning rate to 0.00027583692281041294.\n",
      "Epoch 251/2000\n",
      " - 3s - loss: 5.3707 - mse: 0.0103 - mae: 0.0719 - mape: 21.5781 - val_loss: 5.3210 - val_mse: 0.0019 - val_mae: 0.0252 - val_mape: 48.3682\n",
      "\n",
      "Epoch 00251: val_mse did not improve from 0.00171\n",
      "Epoch 252/2000\n",
      " - 3s - loss: 5.3653 - mse: 0.0107 - mae: 0.0723 - mape: 23.2370 - val_loss: 5.3137 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.9101\n",
      "\n",
      "Epoch 00252: val_mse did not improve from 0.00171\n",
      "Epoch 253/2000\n",
      " - 3s - loss: 5.3588 - mse: 0.0102 - mae: 0.0716 - mape: 26.5742 - val_loss: 5.3082 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 44.8548\n",
      "\n",
      "Epoch 00253: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 0.0002620450628455728.\n",
      "Epoch 254/2000\n",
      " - 3s - loss: 5.3521 - mse: 0.0097 - mae: 0.0707 - mape: 23.8591 - val_loss: 5.3025 - val_mse: 0.0017 - val_mae: 0.0240 - val_mape: 47.4135\n",
      "\n",
      "Epoch 00254: val_mse did not improve from 0.00171\n",
      "Epoch 255/2000\n",
      " - 3s - loss: 5.3502 - mse: 0.0105 - mae: 0.0743 - mape: 24.0671 - val_loss: 5.2969 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 48.1713\n",
      "\n",
      "Epoch 00255: val_mse did not improve from 0.00171\n",
      "Epoch 256/2000\n",
      " - 3s - loss: 5.3397 - mse: 0.0094 - mae: 0.0694 - mape: 22.3809 - val_loss: 5.2914 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 47.4475\n",
      "\n",
      "Epoch 00256: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00256: ReduceLROnPlateau reducing learning rate to 0.000248942815233022.\n",
      "Epoch 257/2000\n",
      " - 3s - loss: 5.3336 - mse: 0.0095 - mae: 0.0686 - mape: 20.6682 - val_loss: 5.2873 - val_mse: 0.0018 - val_mae: 0.0251 - val_mape: 49.5319\n",
      "\n",
      "Epoch 00257: val_mse did not improve from 0.00171\n",
      "Epoch 258/2000\n",
      " - 3s - loss: 5.3356 - mse: 0.0111 - mae: 0.0759 - mape: 25.8126 - val_loss: 5.2808 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 47.3843\n",
      "\n",
      "Epoch 00258: val_mse did not improve from 0.00171\n",
      "Epoch 259/2000\n",
      " - 3s - loss: 5.3227 - mse: 0.0095 - mae: 0.0682 - mape: 22.6182 - val_loss: 5.2788 - val_mse: 0.0020 - val_mae: 0.0271 - val_mape: 49.9060\n",
      "\n",
      "Epoch 00259: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00259: ReduceLROnPlateau reducing learning rate to 0.00023649567447137088.\n",
      "Epoch 260/2000\n",
      " - 3s - loss: 5.3144 - mse: 0.0087 - mae: 0.0651 - mape: 20.5400 - val_loss: 5.2708 - val_mse: 0.0017 - val_mae: 0.0240 - val_mape: 46.3005\n",
      "\n",
      "Epoch 00260: val_mse did not improve from 0.00171\n",
      "Epoch 261/2000\n",
      " - 3s - loss: 5.3146 - mse: 0.0097 - mae: 0.0703 - mape: 23.8814 - val_loss: 5.2657 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 44.4789\n",
      "\n",
      "Epoch 00261: val_mse did not improve from 0.00171\n",
      "Epoch 262/2000\n",
      " - 3s - loss: 5.3154 - mse: 0.0113 - mae: 0.0760 - mape: 26.8366 - val_loss: 5.2608 - val_mse: 0.0017 - val_mae: 0.0241 - val_mape: 43.9848\n",
      "\n",
      "Epoch 00262: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00262: ReduceLROnPlateau reducing learning rate to 0.00022467089074780232.\n",
      "Epoch 263/2000\n",
      " - 3s - loss: 5.3039 - mse: 0.0096 - mae: 0.0693 - mape: 22.6169 - val_loss: 5.2572 - val_mse: 0.0017 - val_mae: 0.0251 - val_mape: 44.7807\n",
      "\n",
      "Epoch 00263: val_mse did not improve from 0.00171\n",
      "Epoch 264/2000\n",
      " - 3s - loss: 5.3027 - mse: 0.0103 - mae: 0.0729 - mape: 23.0810 - val_loss: 5.2517 - val_mse: 0.0018 - val_mae: 0.0243 - val_mape: 51.7470\n",
      "\n",
      "Epoch 00264: val_mse did not improve from 0.00171\n",
      "Epoch 265/2000\n",
      " - 3s - loss: 5.2969 - mse: 0.0102 - mae: 0.0718 - mape: 25.8125 - val_loss: 5.2467 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 48.1667\n",
      "\n",
      "Epoch 00265: val_mse did not improve from 0.00171\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 0.00021343734551919623.\n",
      "Epoch 266/2000\n",
      " - 3s - loss: 5.2907 - mse: 0.0096 - mae: 0.0701 - mape: 24.9260 - val_loss: 5.2463 - val_mse: 0.0021 - val_mae: 0.0281 - val_mape: 51.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00266: val_mse did not improve from 0.00171\n",
      "Epoch 267/2000\n",
      " - 3s - loss: 5.2849 - mse: 0.0093 - mae: 0.0688 - mape: 21.9773 - val_loss: 5.2376 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 49.6987\n",
      "\n",
      "Epoch 00267: val_mse did not improve from 0.00171\n",
      "Epoch 268/2000\n",
      " - 3s - loss: 5.2831 - mse: 0.0104 - mae: 0.0715 - mape: 20.9558 - val_loss: 5.2332 - val_mse: 0.0017 - val_mae: 0.0240 - val_mape: 45.3101\n",
      "\n",
      "Epoch 00268: val_mse improved from 0.00171 to 0.00171, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 269/2000\n",
      " - 3s - loss: 5.2796 - mse: 0.0102 - mae: 0.0725 - mape: 22.0672 - val_loss: 5.2284 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 50.9687\n",
      "\n",
      "Epoch 00269: val_mse improved from 0.00171 to 0.00170, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 270/2000\n",
      " - 3s - loss: 5.2747 - mse: 0.0104 - mae: 0.0720 - mape: 22.5186 - val_loss: 5.2242 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.5659\n",
      "\n",
      "Epoch 00270: val_mse did not improve from 0.00170\n",
      "Epoch 271/2000\n",
      " - 3s - loss: 5.2729 - mse: 0.0107 - mae: 0.0747 - mape: 26.8553 - val_loss: 5.2196 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.3035\n",
      "\n",
      "Epoch 00271: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00271: ReduceLROnPlateau reducing learning rate to 0.00020276548239053226.\n",
      "Epoch 272/2000\n",
      " - 3s - loss: 5.2643 - mse: 0.0099 - mae: 0.0704 - mape: 20.3871 - val_loss: 5.2162 - val_mse: 0.0017 - val_mae: 0.0245 - val_mape: 44.5473\n",
      "\n",
      "Epoch 00272: val_mse did not improve from 0.00170\n",
      "Epoch 273/2000\n",
      " - 3s - loss: 5.2574 - mse: 0.0092 - mae: 0.0678 - mape: 21.0351 - val_loss: 5.2115 - val_mse: 0.0017 - val_mae: 0.0241 - val_mape: 45.0520\n",
      "\n",
      "Epoch 00273: val_mse did not improve from 0.00170\n",
      "Epoch 274/2000\n",
      " - 3s - loss: 5.2534 - mse: 0.0090 - mae: 0.0680 - mape: 21.3017 - val_loss: 5.2078 - val_mse: 0.0017 - val_mae: 0.0245 - val_mape: 42.0407\n",
      "\n",
      "Epoch 00274: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00274: ReduceLROnPlateau reducing learning rate to 0.00019262721034465357.\n",
      "Epoch 275/2000\n",
      " - 3s - loss: 5.2532 - mse: 0.0100 - mae: 0.0718 - mape: 23.8128 - val_loss: 5.2030 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 40.1025\n",
      "\n",
      "Epoch 00275: val_mse did not improve from 0.00170\n",
      "Epoch 276/2000\n",
      " - 3s - loss: 5.2479 - mse: 0.0102 - mae: 0.0706 - mape: 25.4257 - val_loss: 5.1995 - val_mse: 0.0018 - val_mae: 0.0243 - val_mape: 41.1008\n",
      "\n",
      "Epoch 00276: val_mse did not improve from 0.00170\n",
      "Epoch 277/2000\n",
      " - 3s - loss: 5.2422 - mse: 0.0094 - mae: 0.0688 - mape: 20.8834 - val_loss: 5.1952 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 40.5518\n",
      "\n",
      "Epoch 00277: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00277: ReduceLROnPlateau reducing learning rate to 0.00018299584844498894.\n",
      "Epoch 278/2000\n",
      " - 3s - loss: 5.2371 - mse: 0.0092 - mae: 0.0677 - mape: 27.1457 - val_loss: 5.1913 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 43.1209\n",
      "\n",
      "Epoch 00278: val_mse did not improve from 0.00170\n",
      "Epoch 279/2000\n",
      " - 3s - loss: 5.2367 - mse: 0.0104 - mae: 0.0710 - mape: 20.9746 - val_loss: 5.1880 - val_mse: 0.0018 - val_mae: 0.0243 - val_mape: 46.9619\n",
      "\n",
      "Epoch 00279: val_mse did not improve from 0.00170\n",
      "Epoch 280/2000\n",
      " - 3s - loss: 5.2324 - mse: 0.0101 - mae: 0.0706 - mape: 24.6758 - val_loss: 5.1839 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 45.8870\n",
      "\n",
      "Epoch 00280: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00280: ReduceLROnPlateau reducing learning rate to 0.00017384605671395546.\n",
      "Epoch 281/2000\n",
      " - 3s - loss: 5.2238 - mse: 0.0091 - mae: 0.0656 - mape: 19.3904 - val_loss: 5.1803 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 44.6476\n",
      "\n",
      "Epoch 00281: val_mse did not improve from 0.00170\n",
      "Epoch 282/2000\n",
      " - 3s - loss: 5.2229 - mse: 0.0095 - mae: 0.0683 - mape: 24.7643 - val_loss: 5.1771 - val_mse: 0.0018 - val_mae: 0.0244 - val_mape: 42.0310\n",
      "\n",
      "Epoch 00282: val_mse did not improve from 0.00170\n",
      "Epoch 283/2000\n",
      " - 3s - loss: 5.2197 - mse: 0.0094 - mae: 0.0687 - mape: 20.1470 - val_loss: 5.1740 - val_mse: 0.0017 - val_mae: 0.0248 - val_mape: 40.1968\n",
      "\n",
      "Epoch 00283: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00283: ReduceLROnPlateau reducing learning rate to 0.00016515375318704173.\n",
      "Epoch 284/2000\n",
      " - 3s - loss: 5.2172 - mse: 0.0097 - mae: 0.0697 - mape: 23.9676 - val_loss: 5.1697 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 43.5035\n",
      "\n",
      "Epoch 00284: val_mse did not improve from 0.00170\n",
      "Epoch 285/2000\n",
      " - 3s - loss: 5.2144 - mse: 0.0098 - mae: 0.0703 - mape: 22.3066 - val_loss: 5.1667 - val_mse: 0.0018 - val_mae: 0.0244 - val_mape: 41.6166\n",
      "\n",
      "Epoch 00285: val_mse did not improve from 0.00170\n",
      "Epoch 286/2000\n",
      " - 3s - loss: 5.2236 - mse: 0.0135 - mae: 0.0829 - mape: 28.2864 - val_loss: 5.1637 - val_mse: 0.0019 - val_mae: 0.0248 - val_mape: 41.5485\n",
      "\n",
      "Epoch 00286: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00286: ReduceLROnPlateau reducing learning rate to 0.00015689607243984938.\n",
      "Epoch 287/2000\n",
      " - 3s - loss: 5.2042 - mse: 0.0089 - mae: 0.0668 - mape: 18.9803 - val_loss: 5.1596 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 42.5191\n",
      "\n",
      "Epoch 00287: val_mse did not improve from 0.00170\n",
      "Epoch 288/2000\n",
      " - 3s - loss: 5.1991 - mse: 0.0086 - mae: 0.0650 - mape: 19.8746 - val_loss: 5.1566 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 42.2748\n",
      "\n",
      "Epoch 00288: val_mse did not improve from 0.00170\n",
      "Epoch 289/2000\n",
      " - 3s - loss: 5.2088 - mse: 0.0123 - mae: 0.0779 - mape: 23.9851 - val_loss: 5.1531 - val_mse: 0.0018 - val_mae: 0.0239 - val_mape: 43.5357\n",
      "\n",
      "Epoch 00289: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00289: ReduceLROnPlateau reducing learning rate to 0.0001490512688178569.\n",
      "Epoch 290/2000\n",
      " - 3s - loss: 5.1998 - mse: 0.0105 - mae: 0.0721 - mape: 21.7326 - val_loss: 5.1498 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6532\n",
      "\n",
      "Epoch 00290: val_mse did not improve from 0.00170\n",
      "Epoch 291/2000\n",
      " - 3s - loss: 5.1926 - mse: 0.0093 - mae: 0.0679 - mape: 20.8173 - val_loss: 5.1467 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2535\n",
      "\n",
      "Epoch 00291: val_mse did not improve from 0.00170\n",
      "Epoch 292/2000\n",
      " - 3s - loss: 5.1921 - mse: 0.0101 - mae: 0.0705 - mape: 22.3545 - val_loss: 5.1445 - val_mse: 0.0018 - val_mae: 0.0245 - val_mape: 46.5830\n",
      "\n",
      "Epoch 00292: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00292: ReduceLROnPlateau reducing learning rate to 0.00014159870261210017.\n",
      "Epoch 293/2000\n",
      " - 3s - loss: 5.1902 - mse: 0.0101 - mae: 0.0715 - mape: 23.0109 - val_loss: 5.1410 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 45.7366\n",
      "\n",
      "Epoch 00293: val_mse did not improve from 0.00170\n",
      "Epoch 294/2000\n",
      " - 3s - loss: 5.1849 - mse: 0.0095 - mae: 0.0691 - mape: 25.5859 - val_loss: 5.1386 - val_mse: 0.0018 - val_mae: 0.0244 - val_mape: 45.2132\n",
      "\n",
      "Epoch 00294: val_mse did not improve from 0.00170\n",
      "Epoch 295/2000\n",
      " - 3s - loss: 5.1852 - mse: 0.0104 - mae: 0.0723 - mape: 21.7622 - val_loss: 5.1349 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 44.6698\n",
      "\n",
      "Epoch 00295: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00295: ReduceLROnPlateau reducing learning rate to 0.000134518770937575.\n",
      "Epoch 296/2000\n",
      " - 3s - loss: 5.1821 - mse: 0.0102 - mae: 0.0721 - mape: 24.1835 - val_loss: 5.1322 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 42.6947\n",
      "\n",
      "Epoch 00296: val_mse did not improve from 0.00170\n",
      "Epoch 297/2000\n",
      " - 3s - loss: 5.1718 - mse: 0.0084 - mae: 0.0645 - mape: 19.6960 - val_loss: 5.1295 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 44.0683\n",
      "\n",
      "Epoch 00297: val_mse improved from 0.00170 to 0.00170, saving model to train_model/3.2P_CuDNNLSTM.h5\n",
      "Epoch 298/2000\n",
      " - 3s - loss: 5.1745 - mse: 0.0098 - mae: 0.0700 - mape: 22.0583 - val_loss: 5.1274 - val_mse: 0.0018 - val_mae: 0.0243 - val_mape: 45.0890\n",
      "\n",
      "Epoch 00298: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00298: ReduceLROnPlateau reducing learning rate to 0.00012779283861164004.\n",
      "Epoch 299/2000\n",
      " - 3s - loss: 5.1758 - mse: 0.0109 - mae: 0.0740 - mape: 24.7488 - val_loss: 5.1254 - val_mse: 0.0019 - val_mae: 0.0250 - val_mape: 46.2828\n",
      "\n",
      "Epoch 00299: val_mse did not improve from 0.00170\n",
      "Epoch 300/2000\n",
      " - 3s - loss: 5.1727 - mse: 0.0109 - mae: 0.0735 - mape: 20.7870 - val_loss: 5.1215 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 44.9064\n",
      "\n",
      "Epoch 00300: val_mse did not improve from 0.00170\n",
      "Epoch 301/2000\n",
      " - 3s - loss: 5.1689 - mse: 0.0102 - mae: 0.0723 - mape: 24.8704 - val_loss: 5.1193 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 44.9116\n",
      "\n",
      "Epoch 00301: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00301: ReduceLROnPlateau reducing learning rate to 0.00012140319668105803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/2000\n",
      " - 3s - loss: 5.1633 - mse: 0.0094 - mae: 0.0693 - mape: 24.2877 - val_loss: 5.1165 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 43.5497\n",
      "\n",
      "Epoch 00302: val_mse did not improve from 0.00170\n",
      "Epoch 303/2000\n",
      " - 3s - loss: 5.1565 - mse: 0.0090 - mae: 0.0650 - mape: 21.9220 - val_loss: 5.1141 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 40.8590\n",
      "\n",
      "Epoch 00303: val_mse did not improve from 0.00170\n",
      "Epoch 304/2000\n",
      " - 3s - loss: 5.1573 - mse: 0.0095 - mae: 0.0683 - mape: 21.6262 - val_loss: 5.1117 - val_mse: 0.0018 - val_mae: 0.0239 - val_mape: 42.9806\n",
      "\n",
      "Epoch 00304: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00304: ReduceLROnPlateau reducing learning rate to 0.00011533303477335721.\n",
      "Epoch 305/2000\n",
      " - 3s - loss: 5.1597 - mse: 0.0102 - mae: 0.0730 - mape: 21.7955 - val_loss: 5.1097 - val_mse: 0.0018 - val_mae: 0.0243 - val_mape: 42.8761\n",
      "\n",
      "Epoch 00305: val_mse did not improve from 0.00170\n",
      "Epoch 306/2000\n",
      " - 3s - loss: 5.1659 - mse: 0.0130 - mae: 0.0816 - mape: 28.4742 - val_loss: 5.1068 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 45.3107\n",
      "\n",
      "Epoch 00306: val_mse did not improve from 0.00170\n",
      "Epoch 307/2000\n",
      " - 3s - loss: 5.1514 - mse: 0.0099 - mae: 0.0695 - mape: 22.0702 - val_loss: 5.1043 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3091\n",
      "\n",
      "Epoch 00307: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00307: ReduceLROnPlateau reducing learning rate to 0.00010956638579955323.\n",
      "Epoch 308/2000\n",
      " - 3s - loss: 5.1580 - mse: 0.0122 - mae: 0.0783 - mape: 23.2014 - val_loss: 5.1022 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 44.6790\n",
      "\n",
      "Epoch 00308: val_mse did not improve from 0.00170\n",
      "Epoch 309/2000\n",
      " - 3s - loss: 5.1481 - mse: 0.0093 - mae: 0.0707 - mape: 26.7915 - val_loss: 5.1009 - val_mse: 0.0018 - val_mae: 0.0247 - val_mape: 45.5230\n",
      "\n",
      "Epoch 00309: val_mse did not improve from 0.00170\n",
      "Epoch 310/2000\n",
      " - 3s - loss: 5.1466 - mse: 0.0104 - mae: 0.0714 - mape: 22.3773 - val_loss: 5.0978 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 42.9477\n",
      "\n",
      "Epoch 00310: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00310: ReduceLROnPlateau reducing learning rate to 0.00010408806374471168.\n",
      "Epoch 311/2000\n",
      " - 3s - loss: 5.1431 - mse: 0.0100 - mae: 0.0701 - mape: 23.0914 - val_loss: 5.0958 - val_mse: 0.0018 - val_mae: 0.0239 - val_mape: 44.0641\n",
      "\n",
      "Epoch 00311: val_mse did not improve from 0.00170\n",
      "Epoch 312/2000\n",
      " - 3s - loss: 5.1413 - mse: 0.0099 - mae: 0.0704 - mape: 22.1458 - val_loss: 5.0938 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 45.7088\n",
      "\n",
      "Epoch 00312: val_mse did not improve from 0.00170\n",
      "Epoch 313/2000\n",
      " - 3s - loss: 5.1427 - mse: 0.0108 - mae: 0.0739 - mape: 22.9804 - val_loss: 5.0932 - val_mse: 0.0019 - val_mae: 0.0255 - val_mape: 47.2464\n",
      "\n",
      "Epoch 00313: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00313: ReduceLROnPlateau reducing learning rate to 9.888366366794799e-05.\n",
      "Epoch 314/2000\n",
      " - 3s - loss: 5.1355 - mse: 0.0096 - mae: 0.0688 - mape: 19.5748 - val_loss: 5.0896 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 45.7449\n",
      "\n",
      "Epoch 00314: val_mse did not improve from 0.00170\n",
      "Epoch 315/2000\n",
      " - 3s - loss: 5.1288 - mse: 0.0083 - mae: 0.0641 - mape: 19.6660 - val_loss: 5.0872 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5346\n",
      "\n",
      "Epoch 00315: val_mse did not improve from 0.00170\n",
      "Epoch 316/2000\n",
      " - 3s - loss: 5.1300 - mse: 0.0096 - mae: 0.0673 - mape: 20.6835 - val_loss: 5.0858 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 45.2013\n",
      "\n",
      "Epoch 00316: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00316: ReduceLROnPlateau reducing learning rate to 9.393947875651064e-05.\n",
      "Epoch 317/2000\n",
      " - 3s - loss: 5.1301 - mse: 0.0098 - mae: 0.0694 - mape: 19.3225 - val_loss: 5.0840 - val_mse: 0.0018 - val_mae: 0.0243 - val_mape: 45.1992\n",
      "\n",
      "Epoch 00317: val_mse did not improve from 0.00170\n",
      "Epoch 318/2000\n",
      " - 3s - loss: 5.1279 - mse: 0.0097 - mae: 0.0690 - mape: 22.5870 - val_loss: 5.0815 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 43.6951\n",
      "\n",
      "Epoch 00318: val_mse did not improve from 0.00170\n",
      "Epoch 319/2000\n",
      " - 3s - loss: 5.1261 - mse: 0.0096 - mae: 0.0692 - mape: 21.9988 - val_loss: 5.0795 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 44.2248\n",
      "\n",
      "Epoch 00319: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00319: ReduceLROnPlateau reducing learning rate to 8.924250723794102e-05.\n",
      "Epoch 320/2000\n",
      " - 3s - loss: 5.1220 - mse: 0.0093 - mae: 0.0669 - mape: 19.6586 - val_loss: 5.0782 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 45.1658\n",
      "\n",
      "Epoch 00320: val_mse did not improve from 0.00170\n",
      "Epoch 321/2000\n",
      " - 3s - loss: 5.1212 - mse: 0.0091 - mae: 0.0679 - mape: 21.5274 - val_loss: 5.0769 - val_mse: 0.0019 - val_mae: 0.0246 - val_mape: 44.8951\n",
      "\n",
      "Epoch 00321: val_mse did not improve from 0.00170\n",
      "Epoch 322/2000\n",
      " - 3s - loss: 5.1165 - mse: 0.0088 - mae: 0.0650 - mape: 17.9906 - val_loss: 5.0745 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 44.8218\n",
      "\n",
      "Epoch 00322: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00322: ReduceLROnPlateau reducing learning rate to 8.478038325847592e-05.\n",
      "Epoch 323/2000\n",
      " - 3s - loss: 5.1153 - mse: 0.0086 - mae: 0.0656 - mape: 19.6931 - val_loss: 5.0725 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.2861\n",
      "\n",
      "Epoch 00323: val_mse did not improve from 0.00170\n",
      "Epoch 324/2000\n",
      " - 3s - loss: 5.1198 - mse: 0.0102 - mae: 0.0718 - mape: 22.6264 - val_loss: 5.0710 - val_mse: 0.0018 - val_mae: 0.0239 - val_mape: 44.4105\n",
      "\n",
      "Epoch 00324: val_mse did not improve from 0.00170\n",
      "Epoch 325/2000\n",
      " - 3s - loss: 5.1167 - mse: 0.0099 - mae: 0.0704 - mape: 23.9175 - val_loss: 5.0693 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 44.6519\n",
      "\n",
      "Epoch 00325: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00325: ReduceLROnPlateau reducing learning rate to 8.054136305872817e-05.\n",
      "Epoch 326/2000\n",
      " - 3s - loss: 5.1206 - mse: 0.0119 - mae: 0.0760 - mape: 24.9730 - val_loss: 5.0677 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 44.4551\n",
      "\n",
      "Epoch 00326: val_mse did not improve from 0.00170\n",
      "Epoch 327/2000\n",
      " - 3s - loss: 5.1166 - mse: 0.0109 - mae: 0.0736 - mape: 24.8262 - val_loss: 5.0660 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 43.6719\n",
      "\n",
      "Epoch 00327: val_mse did not improve from 0.00170\n",
      "Epoch 328/2000\n",
      " - 3s - loss: 5.1144 - mse: 0.0102 - mae: 0.0731 - mape: 23.5479 - val_loss: 5.0644 - val_mse: 0.0018 - val_mae: 0.0239 - val_mape: 45.2283\n",
      "\n",
      "Epoch 00328: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00328: ReduceLROnPlateau reducing learning rate to 7.651429732504766e-05.\n",
      "Epoch 329/2000\n",
      " - 3s - loss: 5.1106 - mse: 0.0104 - mae: 0.0708 - mape: 22.1381 - val_loss: 5.0630 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 45.4747\n",
      "\n",
      "Epoch 00329: val_mse did not improve from 0.00170\n",
      "Epoch 330/2000\n",
      " - 3s - loss: 5.1085 - mse: 0.0097 - mae: 0.0703 - mape: 26.1745 - val_loss: 5.0611 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 44.7526\n",
      "\n",
      "Epoch 00330: val_mse did not improve from 0.00170\n",
      "Epoch 331/2000\n",
      " - 3s - loss: 5.1048 - mse: 0.0092 - mae: 0.0681 - mape: 20.7841 - val_loss: 5.0598 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 44.7231\n",
      "\n",
      "Epoch 00331: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00331: ReduceLROnPlateau reducing learning rate to 7.268858280440327e-05.\n",
      "Epoch 332/2000\n",
      " - 3s - loss: 5.1067 - mse: 0.0107 - mae: 0.0715 - mape: 21.8395 - val_loss: 5.0581 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.0980\n",
      "\n",
      "Epoch 00332: val_mse did not improve from 0.00170\n",
      "Epoch 333/2000\n",
      " - 3s - loss: 5.0996 - mse: 0.0088 - mae: 0.0659 - mape: 20.7694 - val_loss: 5.0565 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8764\n",
      "\n",
      "Epoch 00333: val_mse did not improve from 0.00170\n",
      "Epoch 334/2000\n",
      " - 3s - loss: 5.1065 - mse: 0.0115 - mae: 0.0743 - mape: 23.3818 - val_loss: 5.0550 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 44.8096\n",
      "\n",
      "Epoch 00334: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00334: ReduceLROnPlateau reducing learning rate to 6.905415539222304e-05.\n",
      "Epoch 335/2000\n",
      " - 3s - loss: 5.1052 - mse: 0.0110 - mae: 0.0744 - mape: 23.8714 - val_loss: 5.0539 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 43.9698\n",
      "\n",
      "Epoch 00335: val_mse did not improve from 0.00170\n",
      "Epoch 336/2000\n",
      " - 3s - loss: 5.1023 - mse: 0.0103 - mae: 0.0729 - mape: 28.0841 - val_loss: 5.0527 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 45.4315\n",
      "\n",
      "Epoch 00336: val_mse did not improve from 0.00170\n",
      "Epoch 337/2000\n",
      " - 3s - loss: 5.0935 - mse: 0.0085 - mae: 0.0655 - mape: 19.8087 - val_loss: 5.0513 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 44.2312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00337: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00337: ReduceLROnPlateau reducing learning rate to 6.560144865943584e-05.\n",
      "Epoch 338/2000\n",
      " - 3s - loss: 5.0932 - mse: 0.0086 - mae: 0.0665 - mape: 21.1548 - val_loss: 5.0498 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 44.5297\n",
      "\n",
      "Epoch 00338: val_mse did not improve from 0.00170\n",
      "Epoch 339/2000\n",
      " - 3s - loss: 5.0960 - mse: 0.0098 - mae: 0.0707 - mape: 21.7764 - val_loss: 5.0485 - val_mse: 0.0018 - val_mae: 0.0239 - val_mape: 45.4305\n",
      "\n",
      "Epoch 00339: val_mse did not improve from 0.00170\n",
      "Epoch 340/2000\n",
      " - 3s - loss: 5.0982 - mse: 0.0111 - mae: 0.0742 - mape: 26.5567 - val_loss: 5.0476 - val_mse: 0.0018 - val_mae: 0.0242 - val_mape: 45.6419\n",
      "\n",
      "Epoch 00340: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00340: ReduceLROnPlateau reducing learning rate to 6.232137311599217e-05.\n",
      "Epoch 341/2000\n",
      " - 3s - loss: 5.0916 - mse: 0.0091 - mae: 0.0689 - mape: 23.5416 - val_loss: 5.0462 - val_mse: 0.0018 - val_mae: 0.0242 - val_mape: 45.5814\n",
      "\n",
      "Epoch 00341: val_mse did not improve from 0.00170\n",
      "Epoch 342/2000\n",
      " - 3s - loss: 5.0897 - mse: 0.0093 - mae: 0.0683 - mape: 25.7191 - val_loss: 5.0447 - val_mse: 0.0017 - val_mae: 0.0239 - val_mape: 45.7313\n",
      "\n",
      "Epoch 00342: val_mse did not improve from 0.00170\n",
      "Epoch 343/2000\n",
      " - 3s - loss: 5.0854 - mse: 0.0084 - mae: 0.0652 - mape: 20.5029 - val_loss: 5.0437 - val_mse: 0.0018 - val_mae: 0.0242 - val_mape: 45.5331\n",
      "\n",
      "Epoch 00343: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00343: ReduceLROnPlateau reducing learning rate to 5.9205302386544643e-05.\n",
      "Epoch 344/2000\n",
      " - 3s - loss: 5.0934 - mse: 0.0107 - mae: 0.0744 - mape: 27.3328 - val_loss: 5.0421 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 43.9460\n",
      "\n",
      "Epoch 00344: val_mse did not improve from 0.00170\n",
      "Epoch 345/2000\n",
      " - 3s - loss: 5.0936 - mse: 0.0112 - mae: 0.0758 - mape: 23.4876 - val_loss: 5.0409 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 43.5001\n",
      "\n",
      "Epoch 00345: val_mse did not improve from 0.00170\n",
      "Epoch 346/2000\n",
      " - 3s - loss: 5.0918 - mse: 0.0112 - mae: 0.0751 - mape: 25.5789 - val_loss: 5.0400 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 43.8320\n",
      "\n",
      "Epoch 00346: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00346: ReduceLROnPlateau reducing learning rate to 5.624503864964936e-05.\n",
      "Epoch 347/2000\n",
      " - 3s - loss: 5.0812 - mse: 0.0091 - mae: 0.0658 - mape: 20.9146 - val_loss: 5.0385 - val_mse: 0.0018 - val_mae: 0.0236 - val_mape: 44.5554\n",
      "\n",
      "Epoch 00347: val_mse did not improve from 0.00170\n",
      "Epoch 348/2000\n",
      " - 3s - loss: 5.0829 - mse: 0.0094 - mae: 0.0686 - mape: 22.8631 - val_loss: 5.0375 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 45.6997\n",
      "\n",
      "Epoch 00348: val_mse did not improve from 0.00170\n",
      "Epoch 349/2000\n",
      " - 3s - loss: 5.0806 - mse: 0.0089 - mae: 0.0674 - mape: 20.4475 - val_loss: 5.0363 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 45.8063\n",
      "\n",
      "Epoch 00349: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00349: ReduceLROnPlateau reducing learning rate to 5.3432784989126957e-05.\n",
      "Epoch 350/2000\n",
      " - 3s - loss: 5.0837 - mse: 0.0100 - mae: 0.0716 - mape: 21.9687 - val_loss: 5.0351 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 45.4047\n",
      "\n",
      "Epoch 00350: val_mse did not improve from 0.00170\n",
      "Epoch 351/2000\n",
      " - 3s - loss: 5.0844 - mse: 0.0105 - mae: 0.0734 - mape: 25.0344 - val_loss: 5.0342 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 45.3501\n",
      "\n",
      "Epoch 00351: val_mse did not improve from 0.00170\n",
      "Epoch 352/2000\n",
      " - 3s - loss: 5.0752 - mse: 0.0085 - mae: 0.0653 - mape: 19.3326 - val_loss: 5.0332 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 45.2129\n",
      "\n",
      "Epoch 00352: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00352: ReduceLROnPlateau reducing learning rate to 5.076114539406262e-05.\n",
      "Epoch 353/2000\n",
      " - 3s - loss: 5.0859 - mse: 0.0111 - mae: 0.0770 - mape: 24.1618 - val_loss: 5.0320 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.1879\n",
      "\n",
      "Epoch 00353: val_mse did not improve from 0.00170\n",
      "Epoch 354/2000\n",
      " - 3s - loss: 5.0737 - mse: 0.0087 - mae: 0.0658 - mape: 21.1284 - val_loss: 5.0312 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 45.4185\n",
      "\n",
      "Epoch 00354: val_mse did not improve from 0.00170\n",
      "Epoch 355/2000\n",
      " - 3s - loss: 5.0810 - mse: 0.0107 - mae: 0.0742 - mape: 24.8342 - val_loss: 5.0301 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 45.3726\n",
      "\n",
      "Epoch 00355: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00355: ReduceLROnPlateau reducing learning rate to 4.822308674192754e-05.\n",
      "Epoch 356/2000\n",
      " - 3s - loss: 5.0736 - mse: 0.0093 - mae: 0.0678 - mape: 20.7626 - val_loss: 5.0291 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.2993\n",
      "\n",
      "Epoch 00356: val_mse did not improve from 0.00170\n",
      "Epoch 357/2000\n",
      " - 3s - loss: 5.0744 - mse: 0.0094 - mae: 0.0695 - mape: 23.9856 - val_loss: 5.0281 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.4421\n",
      "\n",
      "Epoch 00357: val_mse did not improve from 0.00170\n",
      "Epoch 358/2000\n",
      " - 3s - loss: 5.0807 - mse: 0.0114 - mae: 0.0767 - mape: 23.7561 - val_loss: 5.0279 - val_mse: 0.0018 - val_mae: 0.0245 - val_mape: 47.4966\n",
      "\n",
      "Epoch 00358: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00358: ReduceLROnPlateau reducing learning rate to 4.5811931886419186e-05.\n",
      "Epoch 359/2000\n",
      " - 3s - loss: 5.0704 - mse: 0.0092 - mae: 0.0675 - mape: 21.2357 - val_loss: 5.0262 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.0726\n",
      "\n",
      "Epoch 00359: val_mse did not improve from 0.00170\n",
      "Epoch 360/2000\n",
      " - 3s - loss: 5.0711 - mse: 0.0094 - mae: 0.0690 - mape: 20.1099 - val_loss: 5.0251 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6984\n",
      "\n",
      "Epoch 00360: val_mse did not improve from 0.00170\n",
      "Epoch 361/2000\n",
      " - 3s - loss: 5.0667 - mse: 0.0087 - mae: 0.0656 - mape: 20.3436 - val_loss: 5.0243 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8179\n",
      "\n",
      "Epoch 00361: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00361: ReduceLROnPlateau reducing learning rate to 4.352133546490222e-05.\n",
      "Epoch 362/2000\n",
      " - 3s - loss: 5.0665 - mse: 0.0086 - mae: 0.0663 - mape: 21.4981 - val_loss: 5.0236 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 47.3295\n",
      "\n",
      "Epoch 00362: val_mse did not improve from 0.00170\n",
      "Epoch 363/2000\n",
      " - 3s - loss: 5.0683 - mse: 0.0096 - mae: 0.0689 - mape: 22.7874 - val_loss: 5.0224 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.0907\n",
      "\n",
      "Epoch 00363: val_mse did not improve from 0.00170\n",
      "Epoch 364/2000\n",
      " - 3s - loss: 5.0679 - mse: 0.0096 - mae: 0.0694 - mape: 24.7936 - val_loss: 5.0216 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.2444\n",
      "\n",
      "Epoch 00364: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00364: ReduceLROnPlateau reducing learning rate to 4.1345270074089054e-05.\n",
      "Epoch 365/2000\n",
      " - 3s - loss: 5.0651 - mse: 0.0092 - mae: 0.0675 - mape: 19.2958 - val_loss: 5.0207 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.6218\n",
      "\n",
      "Epoch 00365: val_mse did not improve from 0.00170\n",
      "Epoch 366/2000\n",
      " - 3s - loss: 5.0600 - mse: 0.0079 - mae: 0.0631 - mape: 20.7782 - val_loss: 5.0201 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.5092\n",
      "\n",
      "Epoch 00366: val_mse did not improve from 0.00170\n",
      "Epoch 367/2000\n",
      " - 3s - loss: 5.0686 - mse: 0.0108 - mae: 0.0726 - mape: 27.4472 - val_loss: 5.0196 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 47.0097\n",
      "\n",
      "Epoch 00367: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00367: ReduceLROnPlateau reducing learning rate to 3.927800553356064e-05.\n",
      "Epoch 368/2000\n",
      " - 3s - loss: 5.0652 - mse: 0.0098 - mae: 0.0701 - mape: 21.9373 - val_loss: 5.0188 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 47.3753\n",
      "\n",
      "Epoch 00368: val_mse did not improve from 0.00170\n",
      "Epoch 369/2000\n",
      " - 3s - loss: 5.0634 - mse: 0.0094 - mae: 0.0690 - mape: 21.7205 - val_loss: 5.0179 - val_mse: 0.0018 - val_mae: 0.0239 - val_mape: 46.7106\n",
      "\n",
      "Epoch 00369: val_mse did not improve from 0.00170\n",
      "Epoch 370/2000\n",
      " - 3s - loss: 5.0649 - mse: 0.0103 - mae: 0.0712 - mape: 23.7735 - val_loss: 5.0168 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.7192\n",
      "\n",
      "Epoch 00370: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00370: ReduceLROnPlateau reducing learning rate to 3.73141054296866e-05.\n",
      "Epoch 371/2000\n",
      " - 3s - loss: 5.0621 - mse: 0.0095 - mae: 0.0692 - mape: 22.5444 - val_loss: 5.0159 - val_mse: 0.0017 - val_mae: 0.0234 - val_mape: 47.5919\n",
      "\n",
      "Epoch 00371: val_mse did not improve from 0.00170\n",
      "Epoch 372/2000\n",
      " - 3s - loss: 5.0583 - mse: 0.0092 - mae: 0.0662 - mape: 19.4883 - val_loss: 5.0153 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.4280\n",
      "\n",
      "Epoch 00372: val_mse did not improve from 0.00170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 373/2000\n",
      " - 3s - loss: 5.0574 - mse: 0.0087 - mae: 0.0660 - mape: 23.2634 - val_loss: 5.0149 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 47.7176\n",
      "\n",
      "Epoch 00373: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00373: ReduceLROnPlateau reducing learning rate to 3.54483994669863e-05.\n",
      "Epoch 374/2000\n",
      " - 3s - loss: 5.0601 - mse: 0.0097 - mae: 0.0695 - mape: 22.4515 - val_loss: 5.0143 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 47.8203\n",
      "\n",
      "Epoch 00374: val_mse did not improve from 0.00170\n",
      "Epoch 375/2000\n",
      " - 3s - loss: 5.0581 - mse: 0.0093 - mae: 0.0682 - mape: 19.2073 - val_loss: 5.0131 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.9381\n",
      "\n",
      "Epoch 00375: val_mse did not improve from 0.00170\n",
      "Epoch 376/2000\n",
      " - 3s - loss: 5.0620 - mse: 0.0102 - mae: 0.0728 - mape: 23.9371 - val_loss: 5.0126 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 48.2250\n",
      "\n",
      "Epoch 00376: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00376: ReduceLROnPlateau reducing learning rate to 3.367598001204897e-05.\n",
      "Epoch 377/2000\n",
      " - 3s - loss: 5.0560 - mse: 0.0092 - mae: 0.0674 - mape: 22.4443 - val_loss: 5.0117 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.4503\n",
      "\n",
      "Epoch 00377: val_mse did not improve from 0.00170\n",
      "Epoch 378/2000\n",
      " - 3s - loss: 5.0576 - mse: 0.0099 - mae: 0.0698 - mape: 22.2576 - val_loss: 5.0113 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 46.3312\n",
      "\n",
      "Epoch 00378: val_mse did not improve from 0.00170\n",
      "Epoch 379/2000\n",
      " - 3s - loss: 5.0552 - mse: 0.0094 - mae: 0.0680 - mape: 21.4528 - val_loss: 5.0105 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 46.3766\n",
      "\n",
      "Epoch 00379: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00379: ReduceLROnPlateau reducing learning rate to 3.1992181357054504e-05.\n",
      "Epoch 380/2000\n",
      " - 3s - loss: 5.0538 - mse: 0.0089 - mae: 0.0673 - mape: 22.3444 - val_loss: 5.0099 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 46.6962\n",
      "\n",
      "Epoch 00380: val_mse did not improve from 0.00170\n",
      "Epoch 381/2000\n",
      " - 3s - loss: 5.0611 - mse: 0.0108 - mae: 0.0752 - mape: 25.0420 - val_loss: 5.0095 - val_mse: 0.0018 - val_mae: 0.0240 - val_mape: 46.1525\n",
      "\n",
      "Epoch 00381: val_mse did not improve from 0.00170\n",
      "Epoch 382/2000\n",
      " - 3s - loss: 5.0525 - mse: 0.0094 - mae: 0.0673 - mape: 21.7974 - val_loss: 5.0087 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 46.3113\n",
      "\n",
      "Epoch 00382: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00382: ReduceLROnPlateau reducing learning rate to 3.039257280761376e-05.\n",
      "Epoch 383/2000\n",
      " - 3s - loss: 5.0542 - mse: 0.0095 - mae: 0.0696 - mape: 20.3986 - val_loss: 5.0081 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.9134\n",
      "\n",
      "Epoch 00383: val_mse did not improve from 0.00170\n",
      "Epoch 384/2000\n",
      " - 3s - loss: 5.0570 - mse: 0.0105 - mae: 0.0730 - mape: 23.6164 - val_loss: 5.0078 - val_mse: 0.0018 - val_mae: 0.0241 - val_mape: 46.9683\n",
      "\n",
      "Epoch 00384: val_mse did not improve from 0.00170\n",
      "Epoch 385/2000\n",
      " - 3s - loss: 5.0557 - mse: 0.0104 - mae: 0.0724 - mape: 28.3375 - val_loss: 5.0069 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 47.1808\n",
      "\n",
      "Epoch 00385: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00385: ReduceLROnPlateau reducing learning rate to 2.8872944858449044e-05.\n",
      "Epoch 386/2000\n",
      " - 3s - loss: 5.0540 - mse: 0.0102 - mae: 0.0712 - mape: 20.8798 - val_loss: 5.0062 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.8730\n",
      "\n",
      "Epoch 00386: val_mse did not improve from 0.00170\n",
      "Epoch 387/2000\n",
      " - 3s - loss: 5.0520 - mse: 0.0097 - mae: 0.0698 - mape: 20.5658 - val_loss: 5.0055 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6800\n",
      "\n",
      "Epoch 00387: val_mse did not improve from 0.00170\n",
      "Epoch 388/2000\n",
      " - 3s - loss: 5.0531 - mse: 0.0105 - mae: 0.0715 - mape: 24.6338 - val_loss: 5.0049 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5694\n",
      "\n",
      "Epoch 00388: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00388: ReduceLROnPlateau reducing learning rate to 2.742929709711461e-05.\n",
      "Epoch 389/2000\n",
      " - 3s - loss: 5.0501 - mse: 0.0099 - mae: 0.0690 - mape: 22.0153 - val_loss: 5.0045 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.5231\n",
      "\n",
      "Epoch 00389: val_mse did not improve from 0.00170\n",
      "Epoch 390/2000\n",
      " - 3s - loss: 5.0513 - mse: 0.0098 - mae: 0.0708 - mape: 22.6093 - val_loss: 5.0040 - val_mse: 0.0018 - val_mae: 0.0238 - val_mape: 46.7328\n",
      "\n",
      "Epoch 00390: val_mse did not improve from 0.00170\n",
      "Epoch 391/2000\n",
      " - 3s - loss: 5.0482 - mse: 0.0095 - mae: 0.0682 - mape: 21.9650 - val_loss: 5.0033 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4267\n",
      "\n",
      "Epoch 00391: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00391: ReduceLROnPlateau reducing learning rate to 2.605783301987685e-05.\n",
      "Epoch 392/2000\n",
      " - 3s - loss: 5.0442 - mse: 0.0087 - mae: 0.0648 - mape: 20.8609 - val_loss: 5.0030 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.8755\n",
      "\n",
      "Epoch 00392: val_mse did not improve from 0.00170\n",
      "Epoch 393/2000\n",
      " - 3s - loss: 5.0472 - mse: 0.0094 - mae: 0.0682 - mape: 21.1047 - val_loss: 5.0023 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.0907\n",
      "\n",
      "Epoch 00393: val_mse did not improve from 0.00170\n",
      "Epoch 394/2000\n",
      " - 3s - loss: 5.0413 - mse: 0.0081 - mae: 0.0629 - mape: 17.1330 - val_loss: 5.0017 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.8220\n",
      "\n",
      "Epoch 00394: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00394: ReduceLROnPlateau reducing learning rate to 2.4754941023275023e-05.\n",
      "Epoch 395/2000\n",
      " - 3s - loss: 5.0434 - mse: 0.0086 - mae: 0.0655 - mape: 20.9003 - val_loss: 5.0012 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6369\n",
      "\n",
      "Epoch 00395: val_mse did not improve from 0.00170\n",
      "Epoch 396/2000\n",
      " - 3s - loss: 5.0492 - mse: 0.0100 - mae: 0.0718 - mape: 24.0376 - val_loss: 5.0007 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.2275\n",
      "\n",
      "Epoch 00396: val_mse did not improve from 0.00170\n",
      "Epoch 397/2000\n",
      " - 3s - loss: 5.0445 - mse: 0.0092 - mae: 0.0676 - mape: 19.5553 - val_loss: 5.0003 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.0619\n",
      "\n",
      "Epoch 00397: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00397: ReduceLROnPlateau reducing learning rate to 2.3517194404121255e-05.\n",
      "Epoch 398/2000\n",
      " - 3s - loss: 5.0456 - mse: 0.0096 - mae: 0.0692 - mape: 22.3374 - val_loss: 4.9997 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.2731\n",
      "\n",
      "Epoch 00398: val_mse did not improve from 0.00170\n",
      "Epoch 399/2000\n",
      " - 3s - loss: 5.0449 - mse: 0.0096 - mae: 0.0689 - mape: 22.0662 - val_loss: 4.9992 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.3537\n",
      "\n",
      "Epoch 00399: val_mse did not improve from 0.00170\n",
      "Epoch 400/2000\n",
      " - 3s - loss: 5.0457 - mse: 0.0101 - mae: 0.0702 - mape: 23.4495 - val_loss: 4.9987 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.2919\n",
      "\n",
      "Epoch 00400: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00400: ReduceLROnPlateau reducing learning rate to 2.2341334079101214e-05.\n",
      "Epoch 401/2000\n",
      " - 3s - loss: 5.0463 - mse: 0.0104 - mae: 0.0713 - mape: 21.0800 - val_loss: 4.9983 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.6195\n",
      "\n",
      "Epoch 00401: val_mse did not improve from 0.00170\n",
      "Epoch 402/2000\n",
      " - 3s - loss: 5.0438 - mse: 0.0097 - mae: 0.0692 - mape: 21.0679 - val_loss: 4.9978 - val_mse: 0.0017 - val_mae: 0.0234 - val_mape: 48.0509\n",
      "\n",
      "Epoch 00402: val_mse did not improve from 0.00170\n",
      "Epoch 403/2000\n",
      " - 3s - loss: 5.0453 - mse: 0.0103 - mae: 0.0711 - mape: 23.5970 - val_loss: 4.9973 - val_mse: 0.0017 - val_mae: 0.0234 - val_mape: 47.8011\n",
      "\n",
      "Epoch 00403: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00403: ReduceLROnPlateau reducing learning rate to 2.1224266856734175e-05.\n",
      "Epoch 404/2000\n",
      " - 3s - loss: 5.0477 - mse: 0.0103 - mae: 0.0740 - mape: 26.6463 - val_loss: 4.9970 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.4213\n",
      "\n",
      "Epoch 00404: val_mse did not improve from 0.00170\n",
      "Epoch 405/2000\n",
      " - 3s - loss: 5.0433 - mse: 0.0099 - mae: 0.0700 - mape: 20.5042 - val_loss: 4.9966 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.3206\n",
      "\n",
      "Epoch 00405: val_mse did not improve from 0.00170\n",
      "Epoch 406/2000\n",
      " - 3s - loss: 5.0394 - mse: 0.0091 - mae: 0.0666 - mape: 21.0061 - val_loss: 4.9963 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.0911\n",
      "\n",
      "Epoch 00406: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00406: ReduceLROnPlateau reducing learning rate to 2.016305334109347e-05.\n",
      "Epoch 407/2000\n",
      " - 3s - loss: 5.0446 - mse: 0.0101 - mae: 0.0721 - mape: 22.6904 - val_loss: 4.9959 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.8967\n",
      "\n",
      "Epoch 00407: val_mse did not improve from 0.00170\n",
      "Epoch 408/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 5.0386 - mse: 0.0085 - mae: 0.0665 - mape: 22.1911 - val_loss: 4.9957 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 47.4538\n",
      "\n",
      "Epoch 00408: val_mse did not improve from 0.00170\n",
      "Epoch 409/2000\n",
      " - 3s - loss: 5.0453 - mse: 0.0107 - mae: 0.0737 - mape: 21.6199 - val_loss: 4.9952 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.2949\n",
      "\n",
      "Epoch 00409: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00409: ReduceLROnPlateau reducing learning rate to 1.9154901019646785e-05.\n",
      "Epoch 410/2000\n",
      " - 3s - loss: 5.0397 - mse: 0.0093 - mae: 0.0684 - mape: 23.6059 - val_loss: 4.9949 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 47.2407\n",
      "\n",
      "Epoch 00410: val_mse did not improve from 0.00170\n",
      "Epoch 411/2000\n",
      " - 3s - loss: 5.0380 - mse: 0.0091 - mae: 0.0671 - mape: 23.3707 - val_loss: 4.9942 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.2402\n",
      "\n",
      "Epoch 00411: val_mse did not improve from 0.00170\n",
      "Epoch 412/2000\n",
      " - 3s - loss: 5.0386 - mse: 0.0093 - mae: 0.0681 - mape: 21.2048 - val_loss: 4.9937 - val_mse: 0.0017 - val_mae: 0.0234 - val_mape: 46.2321\n",
      "\n",
      "Epoch 00412: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00412: ReduceLROnPlateau reducing learning rate to 1.819715562305646e-05.\n",
      "Epoch 413/2000\n",
      " - 3s - loss: 5.0385 - mse: 0.0093 - mae: 0.0684 - mape: 19.3535 - val_loss: 4.9934 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.3525\n",
      "\n",
      "Epoch 00413: val_mse did not improve from 0.00170\n",
      "Epoch 414/2000\n",
      " - 3s - loss: 5.0362 - mse: 0.0085 - mae: 0.0664 - mape: 23.3031 - val_loss: 4.9931 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7167\n",
      "\n",
      "Epoch 00414: val_mse did not improve from 0.00170\n",
      "Epoch 415/2000\n",
      " - 3s - loss: 5.0343 - mse: 0.0083 - mae: 0.0649 - mape: 21.2403 - val_loss: 4.9927 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.2048\n",
      "\n",
      "Epoch 00415: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00415: ReduceLROnPlateau reducing learning rate to 1.7287297669099645e-05.\n",
      "Epoch 416/2000\n",
      " - 3s - loss: 5.0403 - mse: 0.0102 - mae: 0.0713 - mape: 23.0234 - val_loss: 4.9925 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8883\n",
      "\n",
      "Epoch 00416: val_mse did not improve from 0.00170\n",
      "Epoch 417/2000\n",
      " - 3s - loss: 5.0358 - mse: 0.0092 - mae: 0.0671 - mape: 23.3160 - val_loss: 4.9922 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.1119\n",
      "\n",
      "Epoch 00417: val_mse did not improve from 0.00170\n",
      "Epoch 418/2000\n",
      " - 3s - loss: 5.0354 - mse: 0.0090 - mae: 0.0670 - mape: 19.5560 - val_loss: 4.9917 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.0120\n",
      "\n",
      "Epoch 00418: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00418: ReduceLROnPlateau reducing learning rate to 1.642293209442869e-05.\n",
      "Epoch 419/2000\n",
      " - 3s - loss: 5.0350 - mse: 0.0088 - mae: 0.0670 - mape: 23.7100 - val_loss: 4.9915 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.9816\n",
      "\n",
      "Epoch 00419: val_mse did not improve from 0.00170\n",
      "Epoch 420/2000\n",
      " - 3s - loss: 5.0365 - mse: 0.0098 - mae: 0.0688 - mape: 19.8533 - val_loss: 4.9911 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8995\n",
      "\n",
      "Epoch 00420: val_mse did not improve from 0.00170\n",
      "Epoch 421/2000\n",
      " - 3s - loss: 5.0350 - mse: 0.0095 - mae: 0.0677 - mape: 21.6468 - val_loss: 4.9907 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.1300\n",
      "\n",
      "Epoch 00421: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00421: ReduceLROnPlateau reducing learning rate to 1.560178479849128e-05.\n",
      "Epoch 422/2000\n",
      " - 3s - loss: 5.0394 - mse: 0.0103 - mae: 0.0724 - mape: 24.6095 - val_loss: 4.9904 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.9085\n",
      "\n",
      "Epoch 00422: val_mse did not improve from 0.00170\n",
      "Epoch 423/2000\n",
      " - 3s - loss: 5.0354 - mse: 0.0096 - mae: 0.0686 - mape: 26.2655 - val_loss: 4.9902 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7021\n",
      "\n",
      "Epoch 00423: val_mse did not improve from 0.00170\n",
      "Epoch 424/2000\n",
      " - 3s - loss: 5.0314 - mse: 0.0087 - mae: 0.0650 - mape: 19.2980 - val_loss: 4.9898 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5885\n",
      "\n",
      "Epoch 00424: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00424: ReduceLROnPlateau reducing learning rate to 1.4821695731370709e-05.\n",
      "Epoch 425/2000\n",
      " - 3s - loss: 5.0349 - mse: 0.0095 - mae: 0.0688 - mape: 20.9810 - val_loss: 4.9896 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8831\n",
      "\n",
      "Epoch 00425: val_mse did not improve from 0.00170\n",
      "Epoch 426/2000\n",
      " - 3s - loss: 5.0346 - mse: 0.0098 - mae: 0.0688 - mape: 20.6422 - val_loss: 4.9893 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.1801\n",
      "\n",
      "Epoch 00426: val_mse did not improve from 0.00170\n",
      "Epoch 427/2000\n",
      " - 3s - loss: 5.0340 - mse: 0.0098 - mae: 0.0685 - mape: 22.8971 - val_loss: 4.9890 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7024\n",
      "\n",
      "Epoch 00427: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00427: ReduceLROnPlateau reducing learning rate to 1.4080611117606166e-05.\n",
      "Epoch 428/2000\n",
      " - 3s - loss: 5.0362 - mse: 0.0100 - mae: 0.0709 - mape: 23.8510 - val_loss: 4.9887 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6979\n",
      "\n",
      "Epoch 00428: val_mse did not improve from 0.00170\n",
      "Epoch 429/2000\n",
      " - 3s - loss: 5.0338 - mse: 0.0096 - mae: 0.0689 - mape: 20.6297 - val_loss: 4.9883 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.7105\n",
      "\n",
      "Epoch 00429: val_mse did not improve from 0.00170\n",
      "Epoch 430/2000\n",
      " - 3s - loss: 5.0332 - mse: 0.0094 - mae: 0.0685 - mape: 19.6085 - val_loss: 4.9881 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.4610\n",
      "\n",
      "Epoch 00430: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00430: ReduceLROnPlateau reducing learning rate to 1.3376580864132847e-05.\n",
      "Epoch 431/2000\n",
      " - 3s - loss: 5.0352 - mse: 0.0102 - mae: 0.0708 - mape: 23.5018 - val_loss: 4.9878 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.9470\n",
      "\n",
      "Epoch 00431: val_mse did not improve from 0.00170\n",
      "Epoch 432/2000\n",
      " - 3s - loss: 5.0285 - mse: 0.0082 - mae: 0.0643 - mape: 21.9795 - val_loss: 4.9876 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.1020\n",
      "\n",
      "Epoch 00432: val_mse did not improve from 0.00170\n",
      "Epoch 433/2000\n",
      " - 3s - loss: 5.0290 - mse: 0.0087 - mae: 0.0652 - mape: 22.2150 - val_loss: 4.9874 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.9527\n",
      "\n",
      "Epoch 00433: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00433: ReduceLROnPlateau reducing learning rate to 1.2707751648122211e-05.\n",
      "Epoch 434/2000\n",
      " - 3s - loss: 5.0328 - mse: 0.0097 - mae: 0.0692 - mape: 22.9191 - val_loss: 4.9871 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8763\n",
      "\n",
      "Epoch 00434: val_mse did not improve from 0.00170\n",
      "Epoch 435/2000\n",
      " - 3s - loss: 5.0314 - mse: 0.0092 - mae: 0.0681 - mape: 18.9410 - val_loss: 4.9868 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7428\n",
      "\n",
      "Epoch 00435: val_mse did not improve from 0.00170\n",
      "Epoch 436/2000\n",
      " - 3s - loss: 5.0310 - mse: 0.0095 - mae: 0.0679 - mape: 20.2825 - val_loss: 4.9865 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3581\n",
      "\n",
      "Epoch 00436: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00436: ReduceLROnPlateau reducing learning rate to 1.207236432492209e-05.\n",
      "Epoch 437/2000\n",
      " - 3s - loss: 5.0332 - mse: 0.0100 - mae: 0.0703 - mape: 23.9116 - val_loss: 4.9863 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2355\n",
      "\n",
      "Epoch 00437: val_mse did not improve from 0.00170\n",
      "Epoch 438/2000\n",
      " - 3s - loss: 5.0381 - mse: 0.0113 - mae: 0.0755 - mape: 26.2429 - val_loss: 4.9861 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2814\n",
      "\n",
      "Epoch 00438: val_mse did not improve from 0.00170\n",
      "Epoch 439/2000\n",
      " - 3s - loss: 5.0300 - mse: 0.0092 - mae: 0.0677 - mape: 24.7991 - val_loss: 4.9858 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5089\n",
      "\n",
      "Epoch 00439: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00439: ReduceLROnPlateau reducing learning rate to 1.1468746151876985e-05.\n",
      "Epoch 440/2000\n",
      " - 3s - loss: 5.0313 - mse: 0.0100 - mae: 0.0691 - mape: 20.1037 - val_loss: 4.9856 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.5389\n",
      "\n",
      "Epoch 00440: val_mse did not improve from 0.00170\n",
      "Epoch 441/2000\n",
      " - 3s - loss: 5.0320 - mse: 0.0098 - mae: 0.0701 - mape: 24.1288 - val_loss: 4.9853 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.6485\n",
      "\n",
      "Epoch 00441: val_mse did not improve from 0.00170\n",
      "Epoch 442/2000\n",
      " - 3s - loss: 5.0320 - mse: 0.0098 - mae: 0.0703 - mape: 21.9826 - val_loss: 4.9851 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5412\n",
      "\n",
      "Epoch 00442: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00442: ReduceLROnPlateau reducing learning rate to 1.0895309060288128e-05.\n",
      "Epoch 443/2000\n",
      " - 3s - loss: 5.0356 - mse: 0.0107 - mae: 0.0742 - mape: 23.8443 - val_loss: 4.9850 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00443: val_mse did not improve from 0.00170\n",
      "Epoch 444/2000\n",
      " - 3s - loss: 5.0287 - mse: 0.0092 - mae: 0.0674 - mape: 20.6730 - val_loss: 4.9847 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.9151\n",
      "\n",
      "Epoch 00444: val_mse did not improve from 0.00170\n",
      "Epoch 445/2000\n",
      " - 3s - loss: 5.0328 - mse: 0.0100 - mae: 0.0718 - mape: 20.6147 - val_loss: 4.9845 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.2931\n",
      "\n",
      "Epoch 00445: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00445: ReduceLROnPlateau reducing learning rate to 1.035054360727372e-05.\n",
      "Epoch 446/2000\n",
      " - 3s - loss: 5.0360 - mse: 0.0114 - mae: 0.0752 - mape: 21.6732 - val_loss: 4.9843 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.2224\n",
      "\n",
      "Epoch 00446: val_mse did not improve from 0.00170\n",
      "Epoch 447/2000\n",
      " - 3s - loss: 5.0361 - mse: 0.0112 - mae: 0.0755 - mape: 23.7998 - val_loss: 4.9841 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.4487\n",
      "\n",
      "Epoch 00447: val_mse did not improve from 0.00170\n",
      "Epoch 448/2000\n",
      " - 3s - loss: 5.0283 - mse: 0.0087 - mae: 0.0679 - mape: 22.8986 - val_loss: 4.9838 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 47.2695\n",
      "\n",
      "Epoch 00448: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00448: ReduceLROnPlateau reducing learning rate to 9.833016383709036e-06.\n",
      "Epoch 449/2000\n",
      " - 3s - loss: 5.0299 - mse: 0.0093 - mae: 0.0697 - mape: 25.1079 - val_loss: 4.9836 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.8371\n",
      "\n",
      "Epoch 00449: val_mse did not improve from 0.00170\n",
      "Epoch 450/2000\n",
      " - 3s - loss: 5.0299 - mse: 0.0099 - mae: 0.0698 - mape: 22.0338 - val_loss: 4.9834 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.6827\n",
      "\n",
      "Epoch 00450: val_mse did not improve from 0.00170\n",
      "Epoch 451/2000\n",
      " - 3s - loss: 5.0298 - mse: 0.0098 - mae: 0.0700 - mape: 21.1768 - val_loss: 4.9833 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5205\n",
      "\n",
      "Epoch 00451: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00451: ReduceLROnPlateau reducing learning rate to 9.34136569412658e-06.\n",
      "Epoch 452/2000\n",
      " - 3s - loss: 5.0309 - mse: 0.0099 - mae: 0.0713 - mape: 23.7735 - val_loss: 4.9831 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.0282\n",
      "\n",
      "Epoch 00452: val_mse did not improve from 0.00170\n",
      "Epoch 453/2000\n",
      " - 3s - loss: 5.0278 - mse: 0.0093 - mae: 0.0684 - mape: 25.3720 - val_loss: 4.9829 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.2885\n",
      "\n",
      "Epoch 00453: val_mse did not improve from 0.00170\n",
      "Epoch 454/2000\n",
      " - 3s - loss: 5.0292 - mse: 0.0096 - mae: 0.0699 - mape: 26.4575 - val_loss: 4.9827 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.9622\n",
      "\n",
      "Epoch 00454: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00454: ReduceLROnPlateau reducing learning rate to 8.874297236616258e-06.\n",
      "Epoch 455/2000\n",
      " - 3s - loss: 5.0271 - mse: 0.0099 - mae: 0.0680 - mape: 20.1414 - val_loss: 4.9825 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.7237\n",
      "\n",
      "Epoch 00455: val_mse did not improve from 0.00170\n",
      "Epoch 456/2000\n",
      " - 3s - loss: 5.0305 - mse: 0.0102 - mae: 0.0716 - mape: 21.0758 - val_loss: 4.9823 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.6024\n",
      "\n",
      "Epoch 00456: val_mse did not improve from 0.00170\n",
      "Epoch 457/2000\n",
      " - 3s - loss: 5.0303 - mse: 0.0103 - mae: 0.0716 - mape: 24.0007 - val_loss: 4.9822 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.4228\n",
      "\n",
      "Epoch 00457: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00457: ReduceLROnPlateau reducing learning rate to 8.430582374785444e-06.\n",
      "Epoch 458/2000\n",
      " - 3s - loss: 5.0299 - mse: 0.0102 - mae: 0.0714 - mape: 23.5270 - val_loss: 4.9821 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4386\n",
      "\n",
      "Epoch 00458: val_mse did not improve from 0.00170\n",
      "Epoch 459/2000\n",
      " - 3s - loss: 5.0290 - mse: 0.0104 - mae: 0.0706 - mape: 22.4799 - val_loss: 4.9819 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5188\n",
      "\n",
      "Epoch 00459: val_mse did not improve from 0.00170\n",
      "Epoch 460/2000\n",
      " - 3s - loss: 5.0274 - mse: 0.0098 - mae: 0.0692 - mape: 21.0995 - val_loss: 4.9817 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5712\n",
      "\n",
      "Epoch 00460: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00460: ReduceLROnPlateau reducing learning rate to 8.009052953639184e-06.\n",
      "Epoch 461/2000\n",
      " - 3s - loss: 5.0305 - mse: 0.0103 - mae: 0.0725 - mape: 21.5507 - val_loss: 4.9815 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5893\n",
      "\n",
      "Epoch 00461: val_mse did not improve from 0.00170\n",
      "Epoch 462/2000\n",
      " - 3s - loss: 5.0293 - mse: 0.0102 - mae: 0.0714 - mape: 24.4889 - val_loss: 4.9814 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6027\n",
      "\n",
      "Epoch 00462: val_mse did not improve from 0.00170\n",
      "Epoch 463/2000\n",
      " - 3s - loss: 5.0262 - mse: 0.0094 - mae: 0.0684 - mape: 19.7307 - val_loss: 4.9812 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6692\n",
      "\n",
      "Epoch 00463: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00463: ReduceLROnPlateau reducing learning rate to 7.608600435560219e-06.\n",
      "Epoch 464/2000\n",
      " - 3s - loss: 5.0282 - mse: 0.0097 - mae: 0.0706 - mape: 24.7455 - val_loss: 4.9812 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.8224\n",
      "\n",
      "Epoch 00464: val_mse did not improve from 0.00170\n",
      "Epoch 465/2000\n",
      " - 3s - loss: 5.0351 - mse: 0.0120 - mae: 0.0776 - mape: 26.8593 - val_loss: 4.9811 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.6594\n",
      "\n",
      "Epoch 00465: val_mse did not improve from 0.00170\n",
      "Epoch 466/2000\n",
      " - 3s - loss: 5.0223 - mse: 0.0086 - mae: 0.0650 - mape: 19.1108 - val_loss: 4.9809 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.5472\n",
      "\n",
      "Epoch 00466: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00466: ReduceLROnPlateau reducing learning rate to 7.228170284179213e-06.\n",
      "Epoch 467/2000\n",
      " - 3s - loss: 5.0257 - mse: 0.0097 - mae: 0.0686 - mape: 19.8107 - val_loss: 4.9807 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.5725\n",
      "\n",
      "Epoch 00467: val_mse did not improve from 0.00170\n",
      "Epoch 468/2000\n",
      " - 3s - loss: 5.0264 - mse: 0.0097 - mae: 0.0695 - mape: 21.0863 - val_loss: 4.9806 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1335\n",
      "\n",
      "Epoch 00468: val_mse did not improve from 0.00170\n",
      "Epoch 469/2000\n",
      " - 3s - loss: 5.0217 - mse: 0.0084 - mae: 0.0649 - mape: 19.7471 - val_loss: 4.9805 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1059\n",
      "\n",
      "Epoch 00469: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00469: ReduceLROnPlateau reducing learning rate to 6.866761964374746e-06.\n",
      "Epoch 470/2000\n",
      " - 3s - loss: 5.0277 - mse: 0.0101 - mae: 0.0710 - mape: 21.9400 - val_loss: 4.9803 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.3889\n",
      "\n",
      "Epoch 00470: val_mse did not improve from 0.00170\n",
      "Epoch 471/2000\n",
      " - 3s - loss: 5.0270 - mse: 0.0100 - mae: 0.0704 - mape: 22.5609 - val_loss: 4.9802 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.3203\n",
      "\n",
      "Epoch 00471: val_mse did not improve from 0.00170\n",
      "Epoch 472/2000\n",
      " - 3s - loss: 5.0280 - mse: 0.0107 - mae: 0.0716 - mape: 22.1717 - val_loss: 4.9801 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.2208\n",
      "\n",
      "Epoch 00472: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00472: ReduceLROnPlateau reducing learning rate to 6.523423758153512e-06.\n",
      "Epoch 473/2000\n",
      " - 3s - loss: 5.0318 - mse: 0.0110 - mae: 0.0755 - mape: 25.0406 - val_loss: 4.9800 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.4535\n",
      "\n",
      "Epoch 00473: val_mse did not improve from 0.00170\n",
      "Epoch 474/2000\n",
      " - 3s - loss: 5.0278 - mse: 0.0107 - mae: 0.0716 - mape: 23.1448 - val_loss: 4.9799 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.4223\n",
      "\n",
      "Epoch 00474: val_mse did not improve from 0.00170\n",
      "Epoch 475/2000\n",
      " - 3s - loss: 5.0268 - mse: 0.0095 - mae: 0.0707 - mape: 23.2627 - val_loss: 4.9797 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.6024\n",
      "\n",
      "Epoch 00475: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00475: ReduceLROnPlateau reducing learning rate to 6.197252764650329e-06.\n",
      "Epoch 476/2000\n",
      " - 3s - loss: 5.0264 - mse: 0.0097 - mae: 0.0705 - mape: 25.2371 - val_loss: 4.9796 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.3874\n",
      "\n",
      "Epoch 00476: val_mse did not improve from 0.00170\n",
      "Epoch 477/2000\n",
      " - 3s - loss: 5.0218 - mse: 0.0086 - mae: 0.0660 - mape: 21.4032 - val_loss: 4.9795 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.4536\n",
      "\n",
      "Epoch 00477: val_mse did not improve from 0.00170\n",
      "Epoch 478/2000\n",
      " - 3s - loss: 5.0243 - mse: 0.0096 - mae: 0.0686 - mape: 21.4832 - val_loss: 4.9794 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.4499\n",
      "\n",
      "Epoch 00478: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00478: ReduceLROnPlateau reducing learning rate to 5.887390148018312e-06.\n",
      "Epoch 479/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 5.0232 - mse: 0.0092 - mae: 0.0677 - mape: 22.8794 - val_loss: 4.9793 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.6642\n",
      "\n",
      "Epoch 00479: val_mse did not improve from 0.00170\n",
      "Epoch 480/2000\n",
      " - 3s - loss: 5.0249 - mse: 0.0096 - mae: 0.0695 - mape: 22.2855 - val_loss: 4.9792 - val_mse: 0.0017 - val_mae: 0.0238 - val_mape: 46.8763\n",
      "\n",
      "Epoch 00480: val_mse did not improve from 0.00170\n",
      "Epoch 481/2000\n",
      " - 3s - loss: 5.0234 - mse: 0.0092 - mae: 0.0681 - mape: 22.1836 - val_loss: 4.9790 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.6112\n",
      "\n",
      "Epoch 00481: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00481: ReduceLROnPlateau reducing learning rate to 5.593020705418894e-06.\n",
      "Epoch 482/2000\n",
      " - 3s - loss: 5.0273 - mse: 0.0104 - mae: 0.0721 - mape: 21.6890 - val_loss: 4.9789 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.8387\n",
      "\n",
      "Epoch 00482: val_mse did not improve from 0.00170\n",
      "Epoch 483/2000\n",
      " - 3s - loss: 5.0238 - mse: 0.0094 - mae: 0.0687 - mape: 27.2652 - val_loss: 4.9787 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.2387\n",
      "\n",
      "Epoch 00483: val_mse did not improve from 0.00170\n",
      "Epoch 484/2000\n",
      " - 3s - loss: 5.0232 - mse: 0.0097 - mae: 0.0682 - mape: 23.6638 - val_loss: 4.9786 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 47.1825\n",
      "\n",
      "Epoch 00484: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00484: ReduceLROnPlateau reducing learning rate to 5.3133698429519425e-06.\n",
      "Epoch 485/2000\n",
      " - 3s - loss: 5.0245 - mse: 0.0093 - mae: 0.0696 - mape: 26.0791 - val_loss: 4.9785 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.0750\n",
      "\n",
      "Epoch 00485: val_mse did not improve from 0.00170\n",
      "Epoch 486/2000\n",
      " - 3s - loss: 5.0255 - mse: 0.0100 - mae: 0.0708 - mape: 22.8069 - val_loss: 4.9784 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.8475\n",
      "\n",
      "Epoch 00486: val_mse did not improve from 0.00170\n",
      "Epoch 487/2000\n",
      " - 3s - loss: 5.0206 - mse: 0.0088 - mae: 0.0659 - mape: 21.3110 - val_loss: 4.9783 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.9233\n",
      "\n",
      "Epoch 00487: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00487: ReduceLROnPlateau reducing learning rate to 5.0477014156058426e-06.\n",
      "Epoch 488/2000\n",
      " - 3s - loss: 5.0239 - mse: 0.0096 - mae: 0.0693 - mape: 21.1053 - val_loss: 4.9781 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5998\n",
      "\n",
      "Epoch 00488: val_mse did not improve from 0.00170\n",
      "Epoch 489/2000\n",
      " - 3s - loss: 5.0205 - mse: 0.0088 - mae: 0.0660 - mape: 19.8624 - val_loss: 4.9780 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4604\n",
      "\n",
      "Epoch 00489: val_mse did not improve from 0.00170\n",
      "Epoch 490/2000\n",
      " - 3s - loss: 5.0359 - mse: 0.0125 - mae: 0.0816 - mape: 27.1465 - val_loss: 4.9780 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.4893\n",
      "\n",
      "Epoch 00490: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00490: ReduceLROnPlateau reducing learning rate to 4.795316431227547e-06.\n",
      "Epoch 491/2000\n",
      " - 3s - loss: 5.0334 - mse: 0.0118 - mae: 0.0792 - mape: 26.6514 - val_loss: 4.9778 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4560\n",
      "\n",
      "Epoch 00491: val_mse did not improve from 0.00170\n",
      "Epoch 492/2000\n",
      " - 3s - loss: 5.0226 - mse: 0.0097 - mae: 0.0684 - mape: 21.0642 - val_loss: 4.9777 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4778\n",
      "\n",
      "Epoch 00492: val_mse did not improve from 0.00170\n",
      "Epoch 493/2000\n",
      " - 3s - loss: 5.0219 - mse: 0.0091 - mae: 0.0678 - mape: 21.7551 - val_loss: 4.9776 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3301\n",
      "\n",
      "Epoch 00493: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00493: ReduceLROnPlateau reducing learning rate to 4.555550458462676e-06.\n",
      "Epoch 494/2000\n",
      " - 3s - loss: 5.0188 - mse: 0.0082 - mae: 0.0648 - mape: 19.9984 - val_loss: 4.9775 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3520\n",
      "\n",
      "Epoch 00494: val_mse did not improve from 0.00170\n",
      "Epoch 495/2000\n",
      " - 3s - loss: 5.0247 - mse: 0.0102 - mae: 0.0708 - mape: 23.4925 - val_loss: 4.9774 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3997\n",
      "\n",
      "Epoch 00495: val_mse did not improve from 0.00170\n",
      "Epoch 496/2000\n",
      " - 3s - loss: 5.0264 - mse: 0.0103 - mae: 0.0726 - mape: 24.9696 - val_loss: 4.9773 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2463\n",
      "\n",
      "Epoch 00496: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00496: ReduceLROnPlateau reducing learning rate to 4.3277727627355486e-06.\n",
      "Epoch 497/2000\n",
      " - 3s - loss: 5.0215 - mse: 0.0090 - mae: 0.0678 - mape: 24.6744 - val_loss: 4.9772 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4230\n",
      "\n",
      "Epoch 00497: val_mse did not improve from 0.00170\n",
      "Epoch 498/2000\n",
      " - 3s - loss: 5.0242 - mse: 0.0095 - mae: 0.0706 - mape: 22.7123 - val_loss: 4.9772 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5644\n",
      "\n",
      "Epoch 00498: val_mse did not improve from 0.00170\n",
      "Epoch 499/2000\n",
      " - 3s - loss: 5.0206 - mse: 0.0090 - mae: 0.0671 - mape: 19.3062 - val_loss: 4.9771 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8660\n",
      "\n",
      "Epoch 00499: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00499: ReduceLROnPlateau reducing learning rate to 4.1113841461992704e-06.\n",
      "Epoch 500/2000\n",
      " - 3s - loss: 5.0237 - mse: 0.0100 - mae: 0.0703 - mape: 22.5073 - val_loss: 4.9770 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7074\n",
      "\n",
      "Epoch 00500: val_mse did not improve from 0.00170\n",
      "Epoch 501/2000\n",
      " - 3s - loss: 5.0257 - mse: 0.0099 - mae: 0.0724 - mape: 23.2536 - val_loss: 4.9769 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7022\n",
      "\n",
      "Epoch 00501: val_mse did not improve from 0.00170\n",
      "Epoch 502/2000\n",
      " - 3s - loss: 5.0203 - mse: 0.0087 - mae: 0.0670 - mape: 23.6404 - val_loss: 4.9768 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7980\n",
      "\n",
      "Epoch 00502: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00502: ReduceLROnPlateau reducing learning rate to 3.905814787685812e-06.\n",
      "Epoch 503/2000\n",
      " - 3s - loss: 5.0190 - mse: 0.0082 - mae: 0.0658 - mape: 22.2332 - val_loss: 4.9768 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5740\n",
      "\n",
      "Epoch 00503: val_mse did not improve from 0.00170\n",
      "Epoch 504/2000\n",
      " - 3s - loss: 5.0272 - mse: 0.0106 - mae: 0.0741 - mape: 28.1827 - val_loss: 4.9767 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3909\n",
      "\n",
      "Epoch 00504: val_mse did not improve from 0.00170\n",
      "Epoch 505/2000\n",
      " - 3s - loss: 5.0216 - mse: 0.0097 - mae: 0.0686 - mape: 22.5230 - val_loss: 4.9766 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5321\n",
      "\n",
      "Epoch 00505: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00505: ReduceLROnPlateau reducing learning rate to 3.7105242427060146e-06.\n",
      "Epoch 506/2000\n",
      " - 3s - loss: 5.0182 - mse: 0.0087 - mae: 0.0652 - mape: 18.3089 - val_loss: 4.9765 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6365\n",
      "\n",
      "Epoch 00506: val_mse did not improve from 0.00170\n",
      "Epoch 507/2000\n",
      " - 3s - loss: 5.0266 - mse: 0.0106 - mae: 0.0737 - mape: 23.6590 - val_loss: 4.9764 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.7681\n",
      "\n",
      "Epoch 00507: val_mse did not improve from 0.00170\n",
      "Epoch 508/2000\n",
      " - 3s - loss: 5.0231 - mse: 0.0104 - mae: 0.0703 - mape: 23.3387 - val_loss: 4.9764 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6744\n",
      "\n",
      "Epoch 00508: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00508: ReduceLROnPlateau reducing learning rate to 3.5249979873697156e-06.\n",
      "Epoch 509/2000\n",
      " - 3s - loss: 5.0201 - mse: 0.0092 - mae: 0.0673 - mape: 21.3696 - val_loss: 4.9762 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.6550\n",
      "\n",
      "Epoch 00509: val_mse did not improve from 0.00170\n",
      "Epoch 510/2000\n",
      " - 3s - loss: 5.0248 - mse: 0.0103 - mae: 0.0721 - mape: 24.5262 - val_loss: 4.9762 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.2495\n",
      "\n",
      "Epoch 00510: val_mse did not improve from 0.00170\n",
      "Epoch 511/2000\n",
      " - 3s - loss: 5.0254 - mse: 0.0106 - mae: 0.0728 - mape: 23.5725 - val_loss: 4.9761 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.2971\n",
      "\n",
      "Epoch 00511: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00511: ReduceLROnPlateau reducing learning rate to 3.3487480664007307e-06.\n",
      "Epoch 512/2000\n",
      " - 3s - loss: 5.0219 - mse: 0.0097 - mae: 0.0694 - mape: 22.9237 - val_loss: 4.9760 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.1211\n",
      "\n",
      "Epoch 00512: val_mse did not improve from 0.00170\n",
      "Epoch 513/2000\n",
      " - 3s - loss: 5.0186 - mse: 0.0086 - mae: 0.0661 - mape: 20.6288 - val_loss: 4.9759 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.0492\n",
      "\n",
      "Epoch 00513: val_mse did not improve from 0.00170\n",
      "Epoch 514/2000\n",
      " - 3s - loss: 5.0178 - mse: 0.0085 - mae: 0.0654 - mape: 21.0498 - val_loss: 4.9759 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.0159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00514: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00514: ReduceLROnPlateau reducing learning rate to 3.181310717081942e-06.\n",
      "Epoch 515/2000\n",
      " - 3s - loss: 5.0250 - mse: 0.0106 - mae: 0.0727 - mape: 22.0496 - val_loss: 4.9758 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.0338\n",
      "\n",
      "Epoch 00515: val_mse did not improve from 0.00170\n",
      "Epoch 516/2000\n",
      " - 3s - loss: 5.0185 - mse: 0.0090 - mae: 0.0662 - mape: 18.8610 - val_loss: 4.9758 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.1940\n",
      "\n",
      "Epoch 00516: val_mse did not improve from 0.00170\n",
      "Epoch 517/2000\n",
      " - 3s - loss: 5.0206 - mse: 0.0092 - mae: 0.0684 - mape: 25.0323 - val_loss: 4.9757 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.5009\n",
      "\n",
      "Epoch 00517: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00517: ReduceLROnPlateau reducing learning rate to 3.022245073225349e-06.\n",
      "Epoch 518/2000\n",
      " - 3s - loss: 5.0174 - mse: 0.0086 - mae: 0.0653 - mape: 19.3781 - val_loss: 4.9756 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.4632\n",
      "\n",
      "Epoch 00518: val_mse did not improve from 0.00170\n",
      "Epoch 519/2000\n",
      " - 3s - loss: 5.0241 - mse: 0.0100 - mae: 0.0720 - mape: 21.4785 - val_loss: 4.9755 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.6417\n",
      "\n",
      "Epoch 00519: val_mse did not improve from 0.00170\n",
      "Epoch 520/2000\n",
      " - 3s - loss: 5.0152 - mse: 0.0079 - mae: 0.0631 - mape: 20.0555 - val_loss: 4.9755 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.4125\n",
      "\n",
      "Epoch 00520: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00520: ReduceLROnPlateau reducing learning rate to 2.871132733162085e-06.\n",
      "Epoch 521/2000\n",
      " - 3s - loss: 5.0196 - mse: 0.0090 - mae: 0.0676 - mape: 19.1400 - val_loss: 4.9755 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.7174\n",
      "\n",
      "Epoch 00521: val_mse did not improve from 0.00170\n",
      "Epoch 522/2000\n",
      " - 3s - loss: 5.0228 - mse: 0.0098 - mae: 0.0709 - mape: 23.3710 - val_loss: 4.9754 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.6713\n",
      "\n",
      "Epoch 00522: val_mse did not improve from 0.00170\n",
      "Epoch 523/2000\n",
      " - 3s - loss: 5.0188 - mse: 0.0087 - mae: 0.0669 - mape: 21.2149 - val_loss: 4.9753 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.8101\n",
      "\n",
      "Epoch 00523: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00523: ReduceLROnPlateau reducing learning rate to 2.7275760317024833e-06.\n",
      "Epoch 524/2000\n",
      " - 3s - loss: 5.0293 - mse: 0.0117 - mae: 0.0775 - mape: 24.2293 - val_loss: 4.9753 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.8332\n",
      "\n",
      "Epoch 00524: val_mse did not improve from 0.00170\n",
      "Epoch 525/2000\n",
      " - 3s - loss: 5.0216 - mse: 0.0096 - mae: 0.0699 - mape: 24.2381 - val_loss: 4.9752 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.7076\n",
      "\n",
      "Epoch 00525: val_mse did not improve from 0.00170\n",
      "Epoch 526/2000\n",
      " - 3s - loss: 5.0184 - mse: 0.0091 - mae: 0.0667 - mape: 20.3399 - val_loss: 4.9752 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.8197\n",
      "\n",
      "Epoch 00526: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00526: ReduceLROnPlateau reducing learning rate to 2.591197176116111e-06.\n",
      "Epoch 527/2000\n",
      " - 3s - loss: 5.0227 - mse: 0.0097 - mae: 0.0711 - mape: 25.2331 - val_loss: 4.9752 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.0363\n",
      "\n",
      "Epoch 00527: val_mse did not improve from 0.00170\n",
      "Epoch 528/2000\n",
      " - 3s - loss: 5.0163 - mse: 0.0085 - mae: 0.0647 - mape: 19.5386 - val_loss: 4.9751 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.8263\n",
      "\n",
      "Epoch 00528: val_mse did not improve from 0.00170\n",
      "Epoch 529/2000\n",
      " - 3s - loss: 5.0197 - mse: 0.0095 - mae: 0.0682 - mape: 23.2827 - val_loss: 4.9751 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.6812\n",
      "\n",
      "Epoch 00529: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00529: ReduceLROnPlateau reducing learning rate to 2.461637382111803e-06.\n",
      "Epoch 530/2000\n",
      " - 3s - loss: 5.0229 - mse: 0.0101 - mae: 0.0714 - mape: 21.8294 - val_loss: 4.9751 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.9938\n",
      "\n",
      "Epoch 00530: val_mse did not improve from 0.00170\n",
      "Epoch 531/2000\n",
      " - 3s - loss: 5.0246 - mse: 0.0105 - mae: 0.0731 - mape: 23.4060 - val_loss: 4.9750 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8879\n",
      "\n",
      "Epoch 00531: val_mse did not improve from 0.00170\n",
      "Epoch 532/2000\n",
      " - 3s - loss: 5.0270 - mse: 0.0113 - mae: 0.0756 - mape: 24.5523 - val_loss: 4.9750 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.0486\n",
      "\n",
      "Epoch 00532: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00532: ReduceLROnPlateau reducing learning rate to 2.3385555778077103e-06.\n",
      "Epoch 533/2000\n",
      " - 3s - loss: 5.0231 - mse: 0.0102 - mae: 0.0718 - mape: 25.9894 - val_loss: 4.9750 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 47.0020\n",
      "\n",
      "Epoch 00533: val_mse did not improve from 0.00170\n",
      "Epoch 534/2000\n",
      " - 3s - loss: 5.0241 - mse: 0.0109 - mae: 0.0728 - mape: 23.4399 - val_loss: 4.9749 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.8007\n",
      "\n",
      "Epoch 00534: val_mse did not improve from 0.00170\n",
      "Epoch 535/2000\n",
      " - 3s - loss: 5.0210 - mse: 0.0093 - mae: 0.0698 - mape: 20.6240 - val_loss: 4.9748 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6676\n",
      "\n",
      "Epoch 00535: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00535: ReduceLROnPlateau reducing learning rate to 2.2216277557163265e-06.\n",
      "Epoch 536/2000\n",
      " - 3s - loss: 5.0248 - mse: 0.0102 - mae: 0.0735 - mape: 22.8708 - val_loss: 4.9748 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5471\n",
      "\n",
      "Epoch 00536: val_mse did not improve from 0.00170\n",
      "Epoch 537/2000\n",
      " - 3s - loss: 5.0260 - mse: 0.0114 - mae: 0.0748 - mape: 21.4263 - val_loss: 4.9748 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.6614\n",
      "\n",
      "Epoch 00537: val_mse did not improve from 0.00170\n",
      "Epoch 538/2000\n",
      " - 3s - loss: 5.0206 - mse: 0.0096 - mae: 0.0695 - mape: 21.5468 - val_loss: 4.9748 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5352\n",
      "\n",
      "Epoch 00538: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00538: ReduceLROnPlateau reducing learning rate to 2.110546324729512e-06.\n",
      "Epoch 539/2000\n",
      " - 3s - loss: 5.0155 - mse: 0.0084 - mae: 0.0644 - mape: 19.7085 - val_loss: 4.9747 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0884\n",
      "\n",
      "Epoch 00539: val_mse did not improve from 0.00170\n",
      "Epoch 540/2000\n",
      " - 3s - loss: 5.0259 - mse: 0.0116 - mae: 0.0749 - mape: 24.4187 - val_loss: 4.9746 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0858\n",
      "\n",
      "Epoch 00540: val_mse did not improve from 0.00170\n",
      "Epoch 541/2000\n",
      " - 3s - loss: 5.0233 - mse: 0.0104 - mae: 0.0723 - mape: 23.4076 - val_loss: 4.9746 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1973\n",
      "\n",
      "Epoch 00541: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00541: ReduceLROnPlateau reducing learning rate to 2.0050190300935354e-06.\n",
      "Epoch 542/2000\n",
      " - 3s - loss: 5.0243 - mse: 0.0104 - mae: 0.0734 - mape: 23.7359 - val_loss: 4.9745 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1846\n",
      "\n",
      "Epoch 00542: val_mse did not improve from 0.00170\n",
      "Epoch 543/2000\n",
      " - 3s - loss: 5.0178 - mse: 0.0091 - mae: 0.0669 - mape: 19.1134 - val_loss: 4.9744 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.1775\n",
      "\n",
      "Epoch 00543: val_mse did not improve from 0.00170\n",
      "Epoch 544/2000\n",
      " - 3s - loss: 5.0223 - mse: 0.0106 - mae: 0.0714 - mape: 21.2984 - val_loss: 4.9744 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3511\n",
      "\n",
      "Epoch 00544: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00544: ReduceLROnPlateau reducing learning rate to 1.9047680893891082e-06.\n",
      "Epoch 545/2000\n",
      " - 3s - loss: 5.0206 - mse: 0.0097 - mae: 0.0697 - mape: 22.9438 - val_loss: 4.9744 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.2059\n",
      "\n",
      "Epoch 00545: val_mse did not improve from 0.00170\n",
      "Epoch 546/2000\n",
      " - 3s - loss: 5.0168 - mse: 0.0090 - mae: 0.0660 - mape: 22.4538 - val_loss: 4.9743 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3127\n",
      "\n",
      "Epoch 00546: val_mse did not improve from 0.00170\n",
      "Epoch 547/2000\n",
      " - 3s - loss: 5.0188 - mse: 0.0091 - mae: 0.0680 - mape: 21.2788 - val_loss: 4.9743 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3224\n",
      "\n",
      "Epoch 00547: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00547: ReduceLROnPlateau reducing learning rate to 1.809529652518904e-06.\n",
      "Epoch 548/2000\n",
      " - 3s - loss: 5.0221 - mse: 0.0103 - mae: 0.0714 - mape: 24.0951 - val_loss: 4.9743 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4147\n",
      "\n",
      "Epoch 00548: val_mse did not improve from 0.00170\n",
      "Epoch 549/2000\n",
      " - 3s - loss: 5.0232 - mse: 0.0101 - mae: 0.0725 - mape: 19.9033 - val_loss: 4.9742 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3450\n",
      "\n",
      "Epoch 00549: val_mse did not improve from 0.00170\n",
      "Epoch 550/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 5.0254 - mse: 0.0108 - mae: 0.0747 - mape: 22.5683 - val_loss: 4.9742 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2042\n",
      "\n",
      "Epoch 00550: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00550: ReduceLROnPlateau reducing learning rate to 1.7190531536925845e-06.\n",
      "Epoch 551/2000\n",
      " - 3s - loss: 5.0155 - mse: 0.0085 - mae: 0.0649 - mape: 21.2774 - val_loss: 4.9741 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.0597\n",
      "\n",
      "Epoch 00551: val_mse did not improve from 0.00170\n",
      "Epoch 552/2000\n",
      " - 3s - loss: 5.0202 - mse: 0.0098 - mae: 0.0696 - mape: 21.8073 - val_loss: 4.9741 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9446\n",
      "\n",
      "Epoch 00552: val_mse did not improve from 0.00170\n",
      "Epoch 553/2000\n",
      " - 3s - loss: 5.0200 - mse: 0.0096 - mae: 0.0695 - mape: 23.7900 - val_loss: 4.9741 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0168\n",
      "\n",
      "Epoch 00553: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00553: ReduceLROnPlateau reducing learning rate to 1.6331004474068322e-06.\n",
      "Epoch 554/2000\n",
      " - 3s - loss: 5.0258 - mse: 0.0109 - mae: 0.0753 - mape: 24.3162 - val_loss: 4.9741 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7538\n",
      "\n",
      "Epoch 00554: val_mse did not improve from 0.00170\n",
      "Epoch 555/2000\n",
      " - 3s - loss: 5.0197 - mse: 0.0094 - mae: 0.0692 - mape: 21.0426 - val_loss: 4.9741 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9207\n",
      "\n",
      "Epoch 00555: val_mse did not improve from 0.00170\n",
      "Epoch 556/2000\n",
      " - 3s - loss: 5.0130 - mse: 0.0082 - mae: 0.0625 - mape: 18.6080 - val_loss: 4.9740 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7567\n",
      "\n",
      "Epoch 00556: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00556: ReduceLROnPlateau reducing learning rate to 1.5514453764353675e-06.\n",
      "Epoch 557/2000\n",
      " - 3s - loss: 5.0217 - mse: 0.0100 - mae: 0.0713 - mape: 21.5386 - val_loss: 4.9740 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9717\n",
      "\n",
      "Epoch 00557: val_mse did not improve from 0.00170\n",
      "Epoch 558/2000\n",
      " - 3s - loss: 5.0173 - mse: 0.0090 - mae: 0.0669 - mape: 20.8967 - val_loss: 4.9739 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9765\n",
      "\n",
      "Epoch 00558: val_mse did not improve from 0.00170\n",
      "Epoch 559/2000\n",
      " - 3s - loss: 5.0200 - mse: 0.0098 - mae: 0.0697 - mape: 22.2335 - val_loss: 4.9739 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8210\n",
      "\n",
      "Epoch 00559: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00559: ReduceLROnPlateau reducing learning rate to 1.4738731238139735e-06.\n",
      "Epoch 560/2000\n",
      " - 3s - loss: 5.0153 - mse: 0.0087 - mae: 0.0650 - mape: 19.9472 - val_loss: 4.9739 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 45.9391\n",
      "\n",
      "Epoch 00560: val_mse did not improve from 0.00170\n",
      "Epoch 561/2000\n",
      " - 3s - loss: 5.0174 - mse: 0.0087 - mae: 0.0671 - mape: 24.6859 - val_loss: 4.9738 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.0219\n",
      "\n",
      "Epoch 00561: val_mse did not improve from 0.00170\n",
      "Epoch 562/2000\n",
      " - 3s - loss: 5.0206 - mse: 0.0099 - mae: 0.0704 - mape: 22.3274 - val_loss: 4.9738 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9944\n",
      "\n",
      "Epoch 00562: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00562: ReduceLROnPlateau reducing learning rate to 1.4001794568230252e-06.\n",
      "Epoch 563/2000\n",
      " - 3s - loss: 5.0230 - mse: 0.0102 - mae: 0.0728 - mape: 22.0065 - val_loss: 4.9738 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 45.9098\n",
      "\n",
      "Epoch 00563: val_mse did not improve from 0.00170\n",
      "Epoch 564/2000\n",
      " - 3s - loss: 5.0226 - mse: 0.0102 - mae: 0.0724 - mape: 24.4628 - val_loss: 4.9738 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1910\n",
      "\n",
      "Epoch 00564: val_mse did not improve from 0.00170\n",
      "Epoch 565/2000\n",
      " - 3s - loss: 5.0169 - mse: 0.0091 - mae: 0.0667 - mape: 21.6396 - val_loss: 4.9738 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1489\n",
      "\n",
      "Epoch 00565: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00565: ReduceLROnPlateau reducing learning rate to 1.3301705109824978e-06.\n",
      "Epoch 566/2000\n",
      " - 3s - loss: 5.0168 - mse: 0.0087 - mae: 0.0666 - mape: 20.8720 - val_loss: 4.9738 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3662\n",
      "\n",
      "Epoch 00566: val_mse did not improve from 0.00170\n",
      "Epoch 567/2000\n",
      " - 3s - loss: 5.0213 - mse: 0.0107 - mae: 0.0712 - mape: 25.8732 - val_loss: 4.9738 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2447\n",
      "\n",
      "Epoch 00567: val_mse did not improve from 0.00170\n",
      "Epoch 568/2000\n",
      " - 3s - loss: 5.0229 - mse: 0.0107 - mae: 0.0728 - mape: 21.1971 - val_loss: 4.9737 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8805\n",
      "\n",
      "Epoch 00568: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00568: ReduceLROnPlateau reducing learning rate to 1.263662034034496e-06.\n",
      "Epoch 569/2000\n",
      " - 3s - loss: 5.0179 - mse: 0.0092 - mae: 0.0678 - mape: 21.4407 - val_loss: 4.9737 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0940\n",
      "\n",
      "Epoch 00569: val_mse did not improve from 0.00170\n",
      "Epoch 570/2000\n",
      " - 3s - loss: 5.0212 - mse: 0.0099 - mae: 0.0712 - mape: 23.7021 - val_loss: 4.9737 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9646\n",
      "\n",
      "Epoch 00570: val_mse did not improve from 0.00170\n",
      "Epoch 571/2000\n",
      " - 3s - loss: 5.0192 - mse: 0.0097 - mae: 0.0692 - mape: 22.8607 - val_loss: 4.9737 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1735\n",
      "\n",
      "Epoch 00571: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00571: ReduceLROnPlateau reducing learning rate to 1.2004789539332704e-06.\n",
      "Epoch 572/2000\n",
      " - 3s - loss: 5.0178 - mse: 0.0092 - mae: 0.0678 - mape: 22.2701 - val_loss: 4.9736 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1693\n",
      "\n",
      "Epoch 00572: val_mse did not improve from 0.00170\n",
      "Epoch 573/2000\n",
      " - 3s - loss: 5.0164 - mse: 0.0089 - mae: 0.0664 - mape: 21.5717 - val_loss: 4.9735 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9812\n",
      "\n",
      "Epoch 00573: val_mse did not improve from 0.00170\n",
      "Epoch 574/2000\n",
      " - 3s - loss: 5.0200 - mse: 0.0098 - mae: 0.0701 - mape: 22.0238 - val_loss: 4.9735 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1601\n",
      "\n",
      "Epoch 00574: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00574: ReduceLROnPlateau reducing learning rate to 1.1404550548377301e-06.\n",
      "Epoch 575/2000\n",
      " - 3s - loss: 5.0183 - mse: 0.0095 - mae: 0.0684 - mape: 20.1967 - val_loss: 4.9735 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3147\n",
      "\n",
      "Epoch 00575: val_mse did not improve from 0.00170\n",
      "Epoch 576/2000\n",
      " - 3s - loss: 5.0210 - mse: 0.0102 - mae: 0.0710 - mape: 23.5334 - val_loss: 4.9735 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2335\n",
      "\n",
      "Epoch 00576: val_mse did not improve from 0.00170\n",
      "Epoch 577/2000\n",
      " - 3s - loss: 5.0220 - mse: 0.0101 - mae: 0.0721 - mape: 21.7353 - val_loss: 4.9735 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1173\n",
      "\n",
      "Epoch 00577: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00577: ReduceLROnPlateau reducing learning rate to 1.0834323290964675e-06.\n",
      "Epoch 578/2000\n",
      " - 3s - loss: 5.0192 - mse: 0.0097 - mae: 0.0693 - mape: 21.5098 - val_loss: 4.9735 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0635\n",
      "\n",
      "Epoch 00578: val_mse did not improve from 0.00170\n",
      "Epoch 579/2000\n",
      " - 3s - loss: 5.0158 - mse: 0.0088 - mae: 0.0660 - mape: 24.9063 - val_loss: 4.9735 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0749\n",
      "\n",
      "Epoch 00579: val_mse did not improve from 0.00170\n",
      "Epoch 580/2000\n",
      " - 3s - loss: 5.0169 - mse: 0.0093 - mae: 0.0670 - mape: 21.5115 - val_loss: 4.9734 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8859\n",
      "\n",
      "Epoch 00580: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00580: ReduceLROnPlateau reducing learning rate to 1.0292607612427674e-06.\n",
      "Epoch 581/2000\n",
      " - 3s - loss: 5.0189 - mse: 0.0099 - mae: 0.0690 - mape: 22.6418 - val_loss: 4.9734 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6904\n",
      "\n",
      "Epoch 00581: val_mse did not improve from 0.00170\n",
      "Epoch 582/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0088 - mae: 0.0668 - mape: 20.2247 - val_loss: 4.9734 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6747\n",
      "\n",
      "Epoch 00582: val_mse did not improve from 0.00170\n",
      "Epoch 583/2000\n",
      " - 3s - loss: 5.0302 - mse: 0.0126 - mae: 0.0804 - mape: 25.6088 - val_loss: 4.9734 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7925\n",
      "\n",
      "Epoch 00583: val_mse did not improve from 0.00170\n",
      "\n",
      "Epoch 00583: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "Epoch 584/2000\n",
      " - 3s - loss: 5.0196 - mse: 0.0101 - mae: 0.0699 - mape: 20.5984 - val_loss: 4.9734 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.8562\n",
      "\n",
      "Epoch 00584: val_mse did not improve from 0.00170\n",
      "Epoch 585/2000\n",
      " - 3s - loss: 5.0173 - mse: 0.0091 - mae: 0.0676 - mape: 24.4886 - val_loss: 4.9733 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00585: val_mse did not improve from 0.00170\n",
      "Epoch 586/2000\n",
      " - 3s - loss: 5.0157 - mse: 0.0089 - mae: 0.0660 - mape: 18.6127 - val_loss: 4.9733 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0603\n",
      "\n",
      "Epoch 00586: val_mse did not improve from 0.00170\n",
      "Epoch 587/2000\n",
      " - 3s - loss: 5.0210 - mse: 0.0100 - mae: 0.0713 - mape: 24.5351 - val_loss: 4.9733 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0182\n",
      "\n",
      "Epoch 00587: val_mse did not improve from 0.00170\n",
      "Epoch 588/2000\n",
      " - 3s - loss: 5.0155 - mse: 0.0087 - mae: 0.0658 - mape: 20.7821 - val_loss: 4.9733 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1254\n",
      "\n",
      "Epoch 00588: val_mse did not improve from 0.00170\n",
      "Epoch 589/2000\n",
      " - 3s - loss: 5.0161 - mse: 0.0091 - mae: 0.0665 - mape: 19.2984 - val_loss: 4.9733 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0743\n",
      "\n",
      "Epoch 00589: val_mse did not improve from 0.00170\n",
      "Epoch 590/2000\n",
      " - 3s - loss: 5.0152 - mse: 0.0088 - mae: 0.0656 - mape: 19.2162 - val_loss: 4.9732 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9944\n",
      "\n",
      "Epoch 00590: val_mse did not improve from 0.00170\n",
      "Epoch 591/2000\n",
      " - 3s - loss: 5.0172 - mse: 0.0095 - mae: 0.0676 - mape: 20.2549 - val_loss: 4.9732 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9442\n",
      "\n",
      "Epoch 00591: val_mse did not improve from 0.00170\n",
      "Epoch 592/2000\n",
      " - 3s - loss: 5.0220 - mse: 0.0105 - mae: 0.0724 - mape: 22.0912 - val_loss: 4.9732 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0635\n",
      "\n",
      "Epoch 00592: val_mse did not improve from 0.00170\n",
      "Epoch 593/2000\n",
      " - 3s - loss: 5.0151 - mse: 0.0088 - mae: 0.0655 - mape: 18.5391 - val_loss: 4.9732 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1856\n",
      "\n",
      "Epoch 00593: val_mse did not improve from 0.00170\n",
      "Epoch 594/2000\n",
      " - 3s - loss: 5.0200 - mse: 0.0098 - mae: 0.0705 - mape: 21.9114 - val_loss: 4.9732 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3065\n",
      "\n",
      "Epoch 00594: val_mse did not improve from 0.00170\n",
      "Epoch 595/2000\n",
      " - 3s - loss: 5.0219 - mse: 0.0100 - mae: 0.0723 - mape: 23.8734 - val_loss: 4.9732 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0924\n",
      "\n",
      "Epoch 00595: val_mse did not improve from 0.00170\n",
      "Epoch 596/2000\n",
      " - 3s - loss: 5.0186 - mse: 0.0093 - mae: 0.0690 - mape: 21.4333 - val_loss: 4.9731 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0389\n",
      "\n",
      "Epoch 00596: val_mse did not improve from 0.00170\n",
      "Epoch 597/2000\n",
      " - 3s - loss: 5.0150 - mse: 0.0086 - mae: 0.0655 - mape: 20.4573 - val_loss: 4.9731 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0894\n",
      "\n",
      "Epoch 00597: val_mse did not improve from 0.00170\n",
      "Epoch 598/2000\n",
      " - 3s - loss: 5.0207 - mse: 0.0098 - mae: 0.0712 - mape: 24.3681 - val_loss: 4.9731 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2090\n",
      "\n",
      "Epoch 00598: val_mse did not improve from 0.00170\n",
      "Epoch 599/2000\n",
      " - 3s - loss: 5.0179 - mse: 0.0094 - mae: 0.0685 - mape: 23.7212 - val_loss: 4.9731 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2252\n",
      "\n",
      "Epoch 00599: val_mse did not improve from 0.00170\n",
      "Epoch 600/2000\n",
      " - 3s - loss: 5.0211 - mse: 0.0101 - mae: 0.0717 - mape: 26.1803 - val_loss: 4.9731 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8062\n",
      "\n",
      "Epoch 00600: val_mse did not improve from 0.00170\n",
      "Epoch 601/2000\n",
      " - 3s - loss: 5.0172 - mse: 0.0094 - mae: 0.0678 - mape: 20.8178 - val_loss: 4.9730 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7743\n",
      "\n",
      "Epoch 00601: val_mse did not improve from 0.00170\n",
      "Epoch 602/2000\n",
      " - 3s - loss: 5.0154 - mse: 0.0088 - mae: 0.0660 - mape: 20.8788 - val_loss: 4.9730 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6636\n",
      "\n",
      "Epoch 00602: val_mse did not improve from 0.00170\n",
      "Epoch 603/2000\n",
      " - 3s - loss: 5.0212 - mse: 0.0102 - mae: 0.0719 - mape: 23.6341 - val_loss: 4.9730 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8292\n",
      "\n",
      "Epoch 00603: val_mse did not improve from 0.00170\n",
      "Epoch 604/2000\n",
      " - 3s - loss: 5.0180 - mse: 0.0095 - mae: 0.0686 - mape: 20.5487 - val_loss: 4.9730 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.8058\n",
      "\n",
      "Epoch 00604: val_mse did not improve from 0.00170\n",
      "Epoch 605/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0090 - mae: 0.0672 - mape: 18.8839 - val_loss: 4.9729 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7573\n",
      "\n",
      "Epoch 00605: val_mse did not improve from 0.00170\n",
      "Epoch 606/2000\n",
      " - 3s - loss: 5.0239 - mse: 0.0111 - mae: 0.0746 - mape: 25.3396 - val_loss: 4.9729 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0262\n",
      "\n",
      "Epoch 00606: val_mse did not improve from 0.00170\n",
      "Epoch 607/2000\n",
      " - 3s - loss: 5.0160 - mse: 0.0093 - mae: 0.0667 - mape: 20.1816 - val_loss: 4.9729 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7714\n",
      "\n",
      "Epoch 00607: val_mse did not improve from 0.00170\n",
      "Epoch 608/2000\n",
      " - 3s - loss: 5.0196 - mse: 0.0101 - mae: 0.0703 - mape: 22.3727 - val_loss: 4.9729 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8361\n",
      "\n",
      "Epoch 00608: val_mse did not improve from 0.00170\n",
      "Epoch 609/2000\n",
      " - 3s - loss: 5.0153 - mse: 0.0087 - mae: 0.0660 - mape: 22.5214 - val_loss: 4.9729 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0569\n",
      "\n",
      "Epoch 00609: val_mse did not improve from 0.00170\n",
      "Epoch 610/2000\n",
      " - 3s - loss: 5.0176 - mse: 0.0092 - mae: 0.0683 - mape: 21.6263 - val_loss: 4.9729 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9492\n",
      "\n",
      "Epoch 00610: val_mse did not improve from 0.00170\n",
      "Epoch 611/2000\n",
      " - 3s - loss: 5.0218 - mse: 0.0100 - mae: 0.0725 - mape: 25.1704 - val_loss: 4.9729 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9678\n",
      "\n",
      "Epoch 00611: val_mse did not improve from 0.00170\n",
      "Epoch 612/2000\n",
      " - 3s - loss: 5.0289 - mse: 0.0122 - mae: 0.0797 - mape: 24.3792 - val_loss: 4.9728 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8468\n",
      "\n",
      "Epoch 00612: val_mse did not improve from 0.00170\n",
      "Epoch 613/2000\n",
      " - 3s - loss: 5.0194 - mse: 0.0101 - mae: 0.0702 - mape: 20.3183 - val_loss: 4.9728 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0936\n",
      "\n",
      "Epoch 00613: val_mse did not improve from 0.00170\n",
      "Epoch 614/2000\n",
      " - 3s - loss: 5.0222 - mse: 0.0107 - mae: 0.0730 - mape: 22.8480 - val_loss: 4.9728 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9951\n",
      "\n",
      "Epoch 00614: val_mse did not improve from 0.00170\n",
      "Epoch 615/2000\n",
      " - 3s - loss: 5.0172 - mse: 0.0098 - mae: 0.0681 - mape: 22.6803 - val_loss: 4.9728 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.7988\n",
      "\n",
      "Epoch 00615: val_mse did not improve from 0.00170\n",
      "Epoch 616/2000\n",
      " - 3s - loss: 5.0150 - mse: 0.0085 - mae: 0.0658 - mape: 22.9109 - val_loss: 4.9728 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.8564\n",
      "\n",
      "Epoch 00616: val_mse did not improve from 0.00170\n",
      "Epoch 617/2000\n",
      " - 3s - loss: 5.0199 - mse: 0.0102 - mae: 0.0708 - mape: 20.2157 - val_loss: 4.9727 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8039\n",
      "\n",
      "Epoch 00617: val_mse did not improve from 0.00170\n",
      "Epoch 618/2000\n",
      " - 3s - loss: 5.0241 - mse: 0.0115 - mae: 0.0750 - mape: 25.5196 - val_loss: 4.9727 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0330\n",
      "\n",
      "Epoch 00618: val_mse did not improve from 0.00170\n",
      "Epoch 619/2000\n",
      " - 3s - loss: 5.0277 - mse: 0.0125 - mae: 0.0787 - mape: 23.4101 - val_loss: 4.9728 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9165\n",
      "\n",
      "Epoch 00619: val_mse did not improve from 0.00170\n",
      "Epoch 620/2000\n",
      " - 3s - loss: 5.0145 - mse: 0.0087 - mae: 0.0654 - mape: 19.6046 - val_loss: 4.9727 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0362\n",
      "\n",
      "Epoch 00620: val_mse did not improve from 0.00170\n",
      "Epoch 621/2000\n",
      " - 3s - loss: 5.0183 - mse: 0.0099 - mae: 0.0692 - mape: 20.0507 - val_loss: 4.9727 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9698\n",
      "\n",
      "Epoch 00621: val_mse did not improve from 0.00170\n",
      "Epoch 622/2000\n",
      " - 3s - loss: 5.0137 - mse: 0.0086 - mae: 0.0647 - mape: 20.5766 - val_loss: 4.9726 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9422\n",
      "\n",
      "Epoch 00622: val_mse did not improve from 0.00170\n",
      "Epoch 623/2000\n",
      " - 3s - loss: 5.0173 - mse: 0.0096 - mae: 0.0683 - mape: 20.2044 - val_loss: 4.9726 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0484\n",
      "\n",
      "Epoch 00623: val_mse did not improve from 0.00170\n",
      "Epoch 624/2000\n",
      " - 3s - loss: 5.0188 - mse: 0.0097 - mae: 0.0698 - mape: 21.0494 - val_loss: 4.9726 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0108\n",
      "\n",
      "Epoch 00624: val_mse did not improve from 0.00170\n",
      "Epoch 625/2000\n",
      " - 3s - loss: 5.0185 - mse: 0.0096 - mae: 0.0695 - mape: 21.6555 - val_loss: 4.9726 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00625: val_mse did not improve from 0.00170\n",
      "Epoch 626/2000\n",
      " - 3s - loss: 5.0159 - mse: 0.0090 - mae: 0.0670 - mape: 21.8576 - val_loss: 4.9726 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9897\n",
      "\n",
      "Epoch 00626: val_mse did not improve from 0.00170\n",
      "Epoch 627/2000\n",
      " - 3s - loss: 5.0176 - mse: 0.0094 - mae: 0.0687 - mape: 23.5455 - val_loss: 4.9725 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1261\n",
      "\n",
      "Epoch 00627: val_mse did not improve from 0.00170\n",
      "Epoch 628/2000\n",
      " - 3s - loss: 5.0149 - mse: 0.0084 - mae: 0.0660 - mape: 22.9990 - val_loss: 4.9725 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3015\n",
      "\n",
      "Epoch 00628: val_mse did not improve from 0.00170\n",
      "Epoch 629/2000\n",
      " - 3s - loss: 5.0166 - mse: 0.0091 - mae: 0.0678 - mape: 23.7775 - val_loss: 4.9725 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1898\n",
      "\n",
      "Epoch 00629: val_mse did not improve from 0.00170\n",
      "Epoch 630/2000\n",
      " - 3s - loss: 5.0264 - mse: 0.0113 - mae: 0.0775 - mape: 29.3669 - val_loss: 4.9725 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8340\n",
      "\n",
      "Epoch 00630: val_mse did not improve from 0.00170\n",
      "Epoch 631/2000\n",
      " - 3s - loss: 5.0184 - mse: 0.0100 - mae: 0.0696 - mape: 20.0617 - val_loss: 4.9725 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7853\n",
      "\n",
      "Epoch 00631: val_mse did not improve from 0.00170\n",
      "Epoch 632/2000\n",
      " - 3s - loss: 5.0206 - mse: 0.0105 - mae: 0.0718 - mape: 25.1560 - val_loss: 4.9725 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7112\n",
      "\n",
      "Epoch 00632: val_mse did not improve from 0.00170\n",
      "Epoch 633/2000\n",
      " - 3s - loss: 5.0181 - mse: 0.0092 - mae: 0.0693 - mape: 20.4020 - val_loss: 4.9724 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8728\n",
      "\n",
      "Epoch 00633: val_mse did not improve from 0.00170\n",
      "Epoch 634/2000\n",
      " - 3s - loss: 5.0182 - mse: 0.0095 - mae: 0.0694 - mape: 20.4053 - val_loss: 4.9724 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0245\n",
      "\n",
      "Epoch 00634: val_mse did not improve from 0.00170\n",
      "Epoch 635/2000\n",
      " - 3s - loss: 5.0207 - mse: 0.0102 - mae: 0.0720 - mape: 22.9538 - val_loss: 4.9724 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2841\n",
      "\n",
      "Epoch 00635: val_mse did not improve from 0.00170\n",
      "Epoch 636/2000\n",
      " - 3s - loss: 5.0159 - mse: 0.0093 - mae: 0.0672 - mape: 21.3588 - val_loss: 4.9723 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2220\n",
      "\n",
      "Epoch 00636: val_mse did not improve from 0.00170\n",
      "Epoch 637/2000\n",
      " - 3s - loss: 5.0193 - mse: 0.0100 - mae: 0.0706 - mape: 26.6033 - val_loss: 4.9724 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8144\n",
      "\n",
      "Epoch 00637: val_mse did not improve from 0.00170\n",
      "Epoch 638/2000\n",
      " - 3s - loss: 5.0168 - mse: 0.0098 - mae: 0.0681 - mape: 20.7456 - val_loss: 4.9723 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0097\n",
      "\n",
      "Epoch 00638: val_mse did not improve from 0.00170\n",
      "Epoch 639/2000\n",
      " - 3s - loss: 5.0154 - mse: 0.0091 - mae: 0.0667 - mape: 19.9465 - val_loss: 4.9723 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9508\n",
      "\n",
      "Epoch 00639: val_mse did not improve from 0.00170\n",
      "Epoch 640/2000\n",
      " - 3s - loss: 5.0200 - mse: 0.0101 - mae: 0.0713 - mape: 22.8698 - val_loss: 4.9723 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7074\n",
      "\n",
      "Epoch 00640: val_mse did not improve from 0.00170\n",
      "Epoch 641/2000\n",
      " - 3s - loss: 5.0137 - mse: 0.0085 - mae: 0.0651 - mape: 21.5800 - val_loss: 4.9722 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6353\n",
      "\n",
      "Epoch 00641: val_mse did not improve from 0.00170\n",
      "Epoch 642/2000\n",
      " - 3s - loss: 5.0127 - mse: 0.0080 - mae: 0.0641 - mape: 18.7552 - val_loss: 4.9722 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6485\n",
      "\n",
      "Epoch 00642: val_mse did not improve from 0.00170\n",
      "Epoch 643/2000\n",
      " - 3s - loss: 5.0172 - mse: 0.0093 - mae: 0.0686 - mape: 21.2084 - val_loss: 4.9722 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6080\n",
      "\n",
      "Epoch 00643: val_mse did not improve from 0.00170\n",
      "Epoch 644/2000\n",
      " - 3s - loss: 5.0148 - mse: 0.0088 - mae: 0.0662 - mape: 20.8636 - val_loss: 4.9722 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6483\n",
      "\n",
      "Epoch 00644: val_mse did not improve from 0.00170\n",
      "Epoch 645/2000\n",
      " - 3s - loss: 5.0130 - mse: 0.0088 - mae: 0.0645 - mape: 18.6785 - val_loss: 4.9722 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5739\n",
      "\n",
      "Epoch 00645: val_mse did not improve from 0.00170\n",
      "Epoch 646/2000\n",
      " - 3s - loss: 5.0194 - mse: 0.0104 - mae: 0.0709 - mape: 24.1388 - val_loss: 4.9722 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5410\n",
      "\n",
      "Epoch 00646: val_mse did not improve from 0.00170\n",
      "Epoch 647/2000\n",
      " - 3s - loss: 5.0224 - mse: 0.0104 - mae: 0.0739 - mape: 24.8529 - val_loss: 4.9722 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.6320\n",
      "\n",
      "Epoch 00647: val_mse did not improve from 0.00170\n",
      "Epoch 648/2000\n",
      " - 3s - loss: 5.0186 - mse: 0.0101 - mae: 0.0700 - mape: 21.0707 - val_loss: 4.9721 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7388\n",
      "\n",
      "Epoch 00648: val_mse did not improve from 0.00170\n",
      "Epoch 649/2000\n",
      " - 3s - loss: 5.0189 - mse: 0.0098 - mae: 0.0704 - mape: 19.1706 - val_loss: 4.9721 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9750\n",
      "\n",
      "Epoch 00649: val_mse did not improve from 0.00170\n",
      "Epoch 650/2000\n",
      " - 3s - loss: 5.0228 - mse: 0.0117 - mae: 0.0743 - mape: 27.4625 - val_loss: 4.9721 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9440\n",
      "\n",
      "Epoch 00650: val_mse did not improve from 0.00170\n",
      "Epoch 651/2000\n",
      " - 3s - loss: 5.0181 - mse: 0.0100 - mae: 0.0696 - mape: 21.7917 - val_loss: 4.9721 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8824\n",
      "\n",
      "Epoch 00651: val_mse did not improve from 0.00170\n",
      "Epoch 652/2000\n",
      " - 3s - loss: 5.0130 - mse: 0.0082 - mae: 0.0646 - mape: 19.5188 - val_loss: 4.9721 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.8178\n",
      "\n",
      "Epoch 00652: val_mse did not improve from 0.00170\n",
      "Epoch 653/2000\n",
      " - 3s - loss: 5.0189 - mse: 0.0095 - mae: 0.0705 - mape: 20.8025 - val_loss: 4.9721 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9138\n",
      "\n",
      "Epoch 00653: val_mse did not improve from 0.00170\n",
      "Epoch 654/2000\n",
      " - 3s - loss: 5.0192 - mse: 0.0101 - mae: 0.0708 - mape: 22.5837 - val_loss: 4.9720 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.7479\n",
      "\n",
      "Epoch 00654: val_mse did not improve from 0.00170\n",
      "Epoch 655/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0096 - mae: 0.0681 - mape: 19.6797 - val_loss: 4.9720 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8431\n",
      "\n",
      "Epoch 00655: val_mse did not improve from 0.00170\n",
      "Epoch 656/2000\n",
      " - 3s - loss: 5.0189 - mse: 0.0098 - mae: 0.0705 - mape: 22.0048 - val_loss: 4.9719 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1398\n",
      "\n",
      "Epoch 00656: val_mse did not improve from 0.00170\n",
      "Epoch 657/2000\n",
      " - 3s - loss: 5.0161 - mse: 0.0095 - mae: 0.0678 - mape: 20.9113 - val_loss: 4.9720 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1246\n",
      "\n",
      "Epoch 00657: val_mse did not improve from 0.00170\n",
      "Epoch 658/2000\n",
      " - 3s - loss: 5.0208 - mse: 0.0104 - mae: 0.0725 - mape: 23.7815 - val_loss: 4.9720 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1916\n",
      "\n",
      "Epoch 00658: val_mse did not improve from 0.00170\n",
      "Epoch 659/2000\n",
      " - 3s - loss: 5.0220 - mse: 0.0104 - mae: 0.0737 - mape: 23.2174 - val_loss: 4.9720 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.4100\n",
      "\n",
      "Epoch 00659: val_mse did not improve from 0.00170\n",
      "Epoch 660/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0097 - mae: 0.0682 - mape: 19.0808 - val_loss: 4.9719 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.2514\n",
      "\n",
      "Epoch 00660: val_mse did not improve from 0.00170\n",
      "Epoch 661/2000\n",
      " - 3s - loss: 5.0147 - mse: 0.0090 - mae: 0.0665 - mape: 22.4719 - val_loss: 4.9719 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1444\n",
      "\n",
      "Epoch 00661: val_mse did not improve from 0.00170\n",
      "Epoch 662/2000\n",
      " - 3s - loss: 5.0144 - mse: 0.0085 - mae: 0.0661 - mape: 20.9808 - val_loss: 4.9719 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0098\n",
      "\n",
      "Epoch 00662: val_mse did not improve from 0.00170\n",
      "Epoch 663/2000\n",
      " - 3s - loss: 5.0178 - mse: 0.0096 - mae: 0.0696 - mape: 20.8151 - val_loss: 4.9719 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0934\n",
      "\n",
      "Epoch 00663: val_mse did not improve from 0.00170\n",
      "Epoch 664/2000\n",
      " - 3s - loss: 5.0097 - mse: 0.0079 - mae: 0.0615 - mape: 20.4352 - val_loss: 4.9718 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0472\n",
      "\n",
      "Epoch 00664: val_mse did not improve from 0.00170\n",
      "Epoch 665/2000\n",
      " - 3s - loss: 5.0244 - mse: 0.0114 - mae: 0.0763 - mape: 24.2514 - val_loss: 4.9718 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00665: val_mse did not improve from 0.00170\n",
      "Epoch 666/2000\n",
      " - 3s - loss: 5.0225 - mse: 0.0109 - mae: 0.0744 - mape: 25.7390 - val_loss: 4.9718 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1401\n",
      "\n",
      "Epoch 00666: val_mse did not improve from 0.00170\n",
      "Epoch 667/2000\n",
      " - 3s - loss: 5.0170 - mse: 0.0091 - mae: 0.0688 - mape: 24.6180 - val_loss: 4.9718 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1207\n",
      "\n",
      "Epoch 00667: val_mse did not improve from 0.00170\n",
      "Epoch 668/2000\n",
      " - 3s - loss: 5.0147 - mse: 0.0086 - mae: 0.0666 - mape: 18.9453 - val_loss: 4.9718 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9859\n",
      "\n",
      "Epoch 00668: val_mse did not improve from 0.00170\n",
      "Epoch 669/2000\n",
      " - 3s - loss: 5.0183 - mse: 0.0101 - mae: 0.0702 - mape: 21.6686 - val_loss: 4.9717 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8812\n",
      "\n",
      "Epoch 00669: val_mse did not improve from 0.00170\n",
      "Epoch 670/2000\n",
      " - 3s - loss: 5.0169 - mse: 0.0094 - mae: 0.0688 - mape: 24.2075 - val_loss: 4.9717 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7306\n",
      "\n",
      "Epoch 00670: val_mse did not improve from 0.00170\n",
      "Epoch 671/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0094 - mae: 0.0684 - mape: 20.9290 - val_loss: 4.9717 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6548\n",
      "\n",
      "Epoch 00671: val_mse did not improve from 0.00170\n",
      "Epoch 672/2000\n",
      " - 3s - loss: 5.0193 - mse: 0.0104 - mae: 0.0713 - mape: 22.2100 - val_loss: 4.9717 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9679\n",
      "\n",
      "Epoch 00672: val_mse did not improve from 0.00170\n",
      "Epoch 673/2000\n",
      " - 3s - loss: 5.0169 - mse: 0.0097 - mae: 0.0688 - mape: 23.6937 - val_loss: 4.9717 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8663\n",
      "\n",
      "Epoch 00673: val_mse did not improve from 0.00170\n",
      "Epoch 674/2000\n",
      " - 3s - loss: 5.0184 - mse: 0.0099 - mae: 0.0704 - mape: 23.4165 - val_loss: 4.9717 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0596\n",
      "\n",
      "Epoch 00674: val_mse did not improve from 0.00170\n",
      "Epoch 675/2000\n",
      " - 3s - loss: 5.0154 - mse: 0.0091 - mae: 0.0674 - mape: 21.2896 - val_loss: 4.9716 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2619\n",
      "\n",
      "Epoch 00675: val_mse did not improve from 0.00170\n",
      "Epoch 676/2000\n",
      " - 3s - loss: 5.0186 - mse: 0.0100 - mae: 0.0707 - mape: 22.1499 - val_loss: 4.9716 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3488\n",
      "\n",
      "Epoch 00676: val_mse did not improve from 0.00170\n",
      "Epoch 677/2000\n",
      " - 3s - loss: 5.0138 - mse: 0.0087 - mae: 0.0659 - mape: 20.1442 - val_loss: 4.9715 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2795\n",
      "\n",
      "Epoch 00677: val_mse did not improve from 0.00170\n",
      "Epoch 678/2000\n",
      " - 3s - loss: 5.0204 - mse: 0.0104 - mae: 0.0725 - mape: 22.8187 - val_loss: 4.9715 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2657\n",
      "\n",
      "Epoch 00678: val_mse did not improve from 0.00170\n",
      "Epoch 679/2000\n",
      " - 3s - loss: 5.0212 - mse: 0.0103 - mae: 0.0733 - mape: 21.9759 - val_loss: 4.9715 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3958\n",
      "\n",
      "Epoch 00679: val_mse did not improve from 0.00170\n",
      "Epoch 680/2000\n",
      " - 3s - loss: 5.0127 - mse: 0.0089 - mae: 0.0648 - mape: 20.8137 - val_loss: 4.9715 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2463\n",
      "\n",
      "Epoch 00680: val_mse did not improve from 0.00170\n",
      "Epoch 681/2000\n",
      " - 3s - loss: 5.0200 - mse: 0.0099 - mae: 0.0721 - mape: 24.3447 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1853\n",
      "\n",
      "Epoch 00681: val_mse did not improve from 0.00170\n",
      "Epoch 682/2000\n",
      " - 3s - loss: 5.0132 - mse: 0.0085 - mae: 0.0654 - mape: 23.2224 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1401\n",
      "\n",
      "Epoch 00682: val_mse did not improve from 0.00170\n",
      "Epoch 683/2000\n",
      " - 3s - loss: 5.0146 - mse: 0.0092 - mae: 0.0668 - mape: 22.0611 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0235 - val_mape: 46.0008\n",
      "\n",
      "Epoch 00683: val_mse did not improve from 0.00170\n",
      "Epoch 684/2000\n",
      " - 3s - loss: 5.0200 - mse: 0.0103 - mae: 0.0722 - mape: 22.1888 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7336\n",
      "\n",
      "Epoch 00684: val_mse did not improve from 0.00170\n",
      "Epoch 685/2000\n",
      " - 3s - loss: 5.0227 - mse: 0.0116 - mae: 0.0749 - mape: 21.8369 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5900\n",
      "\n",
      "Epoch 00685: val_mse did not improve from 0.00170\n",
      "Epoch 686/2000\n",
      " - 3s - loss: 5.0248 - mse: 0.0115 - mae: 0.0770 - mape: 26.6695 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8994\n",
      "\n",
      "Epoch 00686: val_mse did not improve from 0.00170\n",
      "Epoch 687/2000\n",
      " - 3s - loss: 5.0125 - mse: 0.0085 - mae: 0.0648 - mape: 22.5861 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1639\n",
      "\n",
      "Epoch 00687: val_mse did not improve from 0.00170\n",
      "Epoch 688/2000\n",
      " - 3s - loss: 5.0156 - mse: 0.0092 - mae: 0.0678 - mape: 19.4167 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9366\n",
      "\n",
      "Epoch 00688: val_mse did not improve from 0.00170\n",
      "Epoch 689/2000\n",
      " - 3s - loss: 5.0210 - mse: 0.0109 - mae: 0.0733 - mape: 19.5849 - val_loss: 4.9714 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0321\n",
      "\n",
      "Epoch 00689: val_mse did not improve from 0.00170\n",
      "Epoch 690/2000\n",
      " - 3s - loss: 5.0237 - mse: 0.0112 - mae: 0.0760 - mape: 26.3512 - val_loss: 4.9713 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.6977\n",
      "\n",
      "Epoch 00690: val_mse did not improve from 0.00170\n",
      "Epoch 691/2000\n",
      " - 3s - loss: 5.0150 - mse: 0.0090 - mae: 0.0673 - mape: 20.9949 - val_loss: 4.9713 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.7508\n",
      "\n",
      "Epoch 00691: val_mse did not improve from 0.00170\n",
      "Epoch 692/2000\n",
      " - 3s - loss: 5.0162 - mse: 0.0093 - mae: 0.0686 - mape: 22.6760 - val_loss: 4.9713 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.8455\n",
      "\n",
      "Epoch 00692: val_mse did not improve from 0.00170\n",
      "Epoch 693/2000\n",
      " - 3s - loss: 5.0159 - mse: 0.0091 - mae: 0.0683 - mape: 20.1535 - val_loss: 4.9713 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9438\n",
      "\n",
      "Epoch 00693: val_mse did not improve from 0.00170\n",
      "Epoch 694/2000\n",
      " - 3s - loss: 5.0109 - mse: 0.0081 - mae: 0.0633 - mape: 19.0345 - val_loss: 4.9712 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0493\n",
      "\n",
      "Epoch 00694: val_mse did not improve from 0.00170\n",
      "Epoch 695/2000\n",
      " - 3s - loss: 5.0169 - mse: 0.0095 - mae: 0.0693 - mape: 22.1121 - val_loss: 4.9712 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2565\n",
      "\n",
      "Epoch 00695: val_mse did not improve from 0.00170\n",
      "Epoch 696/2000\n",
      " - 3s - loss: 5.0213 - mse: 0.0105 - mae: 0.0737 - mape: 21.4737 - val_loss: 4.9712 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0587\n",
      "\n",
      "Epoch 00696: val_mse did not improve from 0.00170\n",
      "Epoch 697/2000\n",
      " - 3s - loss: 5.0178 - mse: 0.0099 - mae: 0.0702 - mape: 22.1003 - val_loss: 4.9712 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0184\n",
      "\n",
      "Epoch 00697: val_mse did not improve from 0.00170\n",
      "Epoch 698/2000\n",
      " - 3s - loss: 5.0114 - mse: 0.0081 - mae: 0.0638 - mape: 20.3204 - val_loss: 4.9712 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8767\n",
      "\n",
      "Epoch 00698: val_mse did not improve from 0.00170\n",
      "Epoch 699/2000\n",
      " - 3s - loss: 5.0131 - mse: 0.0087 - mae: 0.0655 - mape: 18.4696 - val_loss: 4.9711 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8733\n",
      "\n",
      "Epoch 00699: val_mse did not improve from 0.00170\n",
      "Epoch 700/2000\n",
      " - 3s - loss: 5.0144 - mse: 0.0088 - mae: 0.0669 - mape: 20.6181 - val_loss: 4.9711 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9292\n",
      "\n",
      "Epoch 00700: val_mse did not improve from 0.00170\n",
      "Epoch 701/2000\n",
      " - 3s - loss: 5.0204 - mse: 0.0109 - mae: 0.0729 - mape: 23.9805 - val_loss: 4.9711 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8212\n",
      "\n",
      "Epoch 00701: val_mse did not improve from 0.00170\n",
      "Epoch 702/2000\n",
      " - 3s - loss: 5.0139 - mse: 0.0090 - mae: 0.0664 - mape: 19.3623 - val_loss: 4.9711 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9450\n",
      "\n",
      "Epoch 00702: val_mse did not improve from 0.00170\n",
      "Epoch 703/2000\n",
      " - 3s - loss: 5.0163 - mse: 0.0095 - mae: 0.0689 - mape: 21.6543 - val_loss: 4.9711 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9819\n",
      "\n",
      "Epoch 00703: val_mse did not improve from 0.00170\n",
      "Epoch 704/2000\n",
      " - 3s - loss: 5.0182 - mse: 0.0100 - mae: 0.0708 - mape: 24.1826 - val_loss: 4.9711 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9591\n",
      "\n",
      "Epoch 00704: val_mse did not improve from 0.00170\n",
      "Epoch 705/2000\n",
      " - 3s - loss: 5.0127 - mse: 0.0084 - mae: 0.0653 - mape: 21.9681 - val_loss: 4.9710 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00705: val_mse did not improve from 0.00170\n",
      "Epoch 706/2000\n",
      " - 3s - loss: 5.0215 - mse: 0.0110 - mae: 0.0741 - mape: 21.1684 - val_loss: 4.9710 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1581\n",
      "\n",
      "Epoch 00706: val_mse did not improve from 0.00170\n",
      "Epoch 707/2000\n",
      " - 3s - loss: 5.0195 - mse: 0.0104 - mae: 0.0722 - mape: 21.7469 - val_loss: 4.9710 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.2208\n",
      "\n",
      "Epoch 00707: val_mse did not improve from 0.00170\n",
      "Epoch 708/2000\n",
      " - 3s - loss: 5.0124 - mse: 0.0085 - mae: 0.0651 - mape: 19.9472 - val_loss: 4.9710 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9538\n",
      "\n",
      "Epoch 00708: val_mse did not improve from 0.00170\n",
      "Epoch 709/2000\n",
      " - 3s - loss: 5.0170 - mse: 0.0095 - mae: 0.0697 - mape: 22.5584 - val_loss: 4.9709 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0176\n",
      "\n",
      "Epoch 00709: val_mse did not improve from 0.00170\n",
      "Epoch 710/2000\n",
      " - 3s - loss: 5.0182 - mse: 0.0098 - mae: 0.0709 - mape: 23.0218 - val_loss: 4.9709 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1561\n",
      "\n",
      "Epoch 00710: val_mse did not improve from 0.00170\n",
      "Epoch 711/2000\n",
      " - 3s - loss: 5.0155 - mse: 0.0098 - mae: 0.0682 - mape: 21.2159 - val_loss: 4.9709 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2362\n",
      "\n",
      "Epoch 00711: val_mse did not improve from 0.00170\n",
      "Epoch 712/2000\n",
      " - 3s - loss: 5.0183 - mse: 0.0097 - mae: 0.0710 - mape: 22.3439 - val_loss: 4.9709 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1876\n",
      "\n",
      "Epoch 00712: val_mse did not improve from 0.00170\n",
      "Epoch 713/2000\n",
      " - 3s - loss: 5.0221 - mse: 0.0112 - mae: 0.0748 - mape: 23.9343 - val_loss: 4.9709 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0823\n",
      "\n",
      "Epoch 00713: val_mse did not improve from 0.00170\n",
      "Epoch 714/2000\n",
      " - 3s - loss: 5.0150 - mse: 0.0095 - mae: 0.0678 - mape: 18.5793 - val_loss: 4.9709 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1189\n",
      "\n",
      "Epoch 00714: val_mse did not improve from 0.00170\n",
      "Epoch 715/2000\n",
      " - 3s - loss: 5.0162 - mse: 0.0097 - mae: 0.0690 - mape: 21.3490 - val_loss: 4.9709 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 46.1992\n",
      "\n",
      "Epoch 00715: val_mse did not improve from 0.00170\n",
      "Epoch 716/2000\n",
      " - 3s - loss: 5.0183 - mse: 0.0099 - mae: 0.0711 - mape: 23.3876 - val_loss: 4.9709 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 46.2059\n",
      "\n",
      "Epoch 00716: val_mse did not improve from 0.00170\n",
      "Epoch 717/2000\n",
      " - 3s - loss: 5.0156 - mse: 0.0093 - mae: 0.0684 - mape: 21.7810 - val_loss: 4.9709 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 46.3311\n",
      "\n",
      "Epoch 00717: val_mse did not improve from 0.00170\n",
      "Epoch 718/2000\n",
      " - 3s - loss: 5.0164 - mse: 0.0093 - mae: 0.0692 - mape: 22.9031 - val_loss: 4.9708 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.3340\n",
      "\n",
      "Epoch 00718: val_mse did not improve from 0.00170\n",
      "Epoch 719/2000\n",
      " - 3s - loss: 5.0135 - mse: 0.0091 - mae: 0.0663 - mape: 18.2866 - val_loss: 4.9708 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1085\n",
      "\n",
      "Epoch 00719: val_mse did not improve from 0.00170\n",
      "Epoch 720/2000\n",
      " - 3s - loss: 5.0169 - mse: 0.0098 - mae: 0.0697 - mape: 21.3905 - val_loss: 4.9707 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2436\n",
      "\n",
      "Epoch 00720: val_mse did not improve from 0.00170\n",
      "Epoch 721/2000\n",
      " - 3s - loss: 5.0147 - mse: 0.0093 - mae: 0.0676 - mape: 21.0360 - val_loss: 4.9707 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3213\n",
      "\n",
      "Epoch 00721: val_mse did not improve from 0.00170\n",
      "Epoch 722/2000\n",
      " - 3s - loss: 5.0132 - mse: 0.0089 - mae: 0.0662 - mape: 20.7235 - val_loss: 4.9707 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.2094\n",
      "\n",
      "Epoch 00722: val_mse did not improve from 0.00170\n",
      "Epoch 723/2000\n",
      " - 3s - loss: 5.0158 - mse: 0.0095 - mae: 0.0688 - mape: 22.0704 - val_loss: 4.9707 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.9640\n",
      "\n",
      "Epoch 00723: val_mse did not improve from 0.00170\n",
      "Epoch 724/2000\n",
      " - 3s - loss: 5.0129 - mse: 0.0086 - mae: 0.0659 - mape: 21.5597 - val_loss: 4.9707 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.0615\n",
      "\n",
      "Epoch 00724: val_mse did not improve from 0.00170\n",
      "Epoch 725/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0099 - mae: 0.0695 - mape: 22.7255 - val_loss: 4.9707 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.8911\n",
      "\n",
      "Epoch 00725: val_mse did not improve from 0.00170\n",
      "Epoch 726/2000\n",
      " - 3s - loss: 5.0189 - mse: 0.0103 - mae: 0.0719 - mape: 25.1578 - val_loss: 4.9706 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8317\n",
      "\n",
      "Epoch 00726: val_mse did not improve from 0.00170\n",
      "Epoch 727/2000\n",
      " - 3s - loss: 5.0143 - mse: 0.0088 - mae: 0.0673 - mape: 19.8108 - val_loss: 4.9706 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9331\n",
      "\n",
      "Epoch 00727: val_mse did not improve from 0.00170\n",
      "Epoch 728/2000\n",
      " - 3s - loss: 5.0135 - mse: 0.0086 - mae: 0.0665 - mape: 21.6187 - val_loss: 4.9705 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6681\n",
      "\n",
      "Epoch 00728: val_mse did not improve from 0.00170\n",
      "Epoch 729/2000\n",
      " - 3s - loss: 5.0144 - mse: 0.0092 - mae: 0.0674 - mape: 19.5479 - val_loss: 4.9705 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5538\n",
      "\n",
      "Epoch 00729: val_mse did not improve from 0.00170\n",
      "Epoch 730/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0096 - mae: 0.0696 - mape: 23.5302 - val_loss: 4.9705 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7263\n",
      "\n",
      "Epoch 00730: val_mse did not improve from 0.00170\n",
      "Epoch 731/2000\n",
      " - 3s - loss: 5.0175 - mse: 0.0098 - mae: 0.0706 - mape: 24.4666 - val_loss: 4.9705 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9129\n",
      "\n",
      "Epoch 00731: val_mse did not improve from 0.00170\n",
      "Epoch 732/2000\n",
      " - 3s - loss: 5.0222 - mse: 0.0117 - mae: 0.0754 - mape: 23.9489 - val_loss: 4.9705 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5726\n",
      "\n",
      "Epoch 00732: val_mse did not improve from 0.00170\n",
      "Epoch 733/2000\n",
      " - 3s - loss: 5.0209 - mse: 0.0107 - mae: 0.0740 - mape: 21.8192 - val_loss: 4.9705 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6557\n",
      "\n",
      "Epoch 00733: val_mse did not improve from 0.00170\n",
      "Epoch 734/2000\n",
      " - 3s - loss: 5.0212 - mse: 0.0107 - mae: 0.0743 - mape: 26.0151 - val_loss: 4.9704 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8458\n",
      "\n",
      "Epoch 00734: val_mse did not improve from 0.00170\n",
      "Epoch 735/2000\n",
      " - 3s - loss: 5.0225 - mse: 0.0110 - mae: 0.0757 - mape: 26.7498 - val_loss: 4.9704 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7028\n",
      "\n",
      "Epoch 00735: val_mse did not improve from 0.00170\n",
      "Epoch 736/2000\n",
      " - 3s - loss: 5.0131 - mse: 0.0089 - mae: 0.0663 - mape: 20.3773 - val_loss: 4.9704 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7549\n",
      "\n",
      "Epoch 00736: val_mse did not improve from 0.00170\n",
      "Epoch 737/2000\n",
      " - 3s - loss: 5.0171 - mse: 0.0099 - mae: 0.0703 - mape: 23.9797 - val_loss: 4.9703 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6755\n",
      "\n",
      "Epoch 00737: val_mse did not improve from 0.00170\n",
      "Epoch 738/2000\n",
      " - 3s - loss: 5.0199 - mse: 0.0108 - mae: 0.0731 - mape: 22.3938 - val_loss: 4.9703 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5657\n",
      "\n",
      "Epoch 00738: val_mse did not improve from 0.00170\n",
      "Epoch 739/2000\n",
      " - 3s - loss: 5.0179 - mse: 0.0101 - mae: 0.0712 - mape: 21.8371 - val_loss: 4.9704 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.5139\n",
      "\n",
      "Epoch 00739: val_mse did not improve from 0.00170\n",
      "Epoch 740/2000\n",
      " - 3s - loss: 5.0125 - mse: 0.0088 - mae: 0.0658 - mape: 23.1522 - val_loss: 4.9703 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.3750\n",
      "\n",
      "Epoch 00740: val_mse did not improve from 0.00170\n",
      "Epoch 741/2000\n",
      " - 3s - loss: 5.0210 - mse: 0.0108 - mae: 0.0743 - mape: 22.9637 - val_loss: 4.9703 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.2059\n",
      "\n",
      "Epoch 00741: val_mse did not improve from 0.00170\n",
      "Epoch 742/2000\n",
      " - 3s - loss: 5.0159 - mse: 0.0097 - mae: 0.0692 - mape: 22.5748 - val_loss: 4.9703 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.4576\n",
      "\n",
      "Epoch 00742: val_mse did not improve from 0.00170\n",
      "Epoch 743/2000\n",
      " - 3s - loss: 5.0144 - mse: 0.0093 - mae: 0.0677 - mape: 21.4214 - val_loss: 4.9703 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.3162\n",
      "\n",
      "Epoch 00743: val_mse did not improve from 0.00170\n",
      "Epoch 744/2000\n",
      " - 3s - loss: 5.0151 - mse: 0.0094 - mae: 0.0685 - mape: 20.2102 - val_loss: 4.9702 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6039\n",
      "\n",
      "Epoch 00744: val_mse did not improve from 0.00170\n",
      "Epoch 745/2000\n",
      " - 3s - loss: 5.0184 - mse: 0.0104 - mae: 0.0718 - mape: 22.8304 - val_loss: 4.9702 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.4862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00745: val_mse did not improve from 0.00170\n",
      "Epoch 746/2000\n",
      " - 3s - loss: 5.0176 - mse: 0.0105 - mae: 0.0710 - mape: 21.4721 - val_loss: 4.9702 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6773\n",
      "\n",
      "Epoch 00746: val_mse did not improve from 0.00170\n",
      "Epoch 747/2000\n",
      " - 3s - loss: 5.0219 - mse: 0.0117 - mae: 0.0753 - mape: 23.0038 - val_loss: 4.9702 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7823\n",
      "\n",
      "Epoch 00747: val_mse did not improve from 0.00170\n",
      "Epoch 748/2000\n",
      " - 3s - loss: 5.0134 - mse: 0.0090 - mae: 0.0668 - mape: 23.4479 - val_loss: 4.9702 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7946\n",
      "\n",
      "Epoch 00748: val_mse did not improve from 0.00170\n",
      "Epoch 749/2000\n",
      " - 3s - loss: 5.0127 - mse: 0.0088 - mae: 0.0662 - mape: 20.7193 - val_loss: 4.9701 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9797\n",
      "\n",
      "Epoch 00749: val_mse did not improve from 0.00170\n",
      "Epoch 750/2000\n",
      " - 3s - loss: 5.0169 - mse: 0.0098 - mae: 0.0704 - mape: 20.6498 - val_loss: 4.9701 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0758\n",
      "\n",
      "Epoch 00750: val_mse did not improve from 0.00170\n",
      "Epoch 751/2000\n",
      " - 3s - loss: 5.0181 - mse: 0.0102 - mae: 0.0716 - mape: 23.4023 - val_loss: 4.9701 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2626\n",
      "\n",
      "Epoch 00751: val_mse did not improve from 0.00170\n",
      "Epoch 752/2000\n",
      " - 3s - loss: 5.0178 - mse: 0.0099 - mae: 0.0713 - mape: 25.7893 - val_loss: 4.9701 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1574\n",
      "\n",
      "Epoch 00752: val_mse did not improve from 0.00170\n",
      "Epoch 753/2000\n",
      " - 3s - loss: 5.0188 - mse: 0.0107 - mae: 0.0723 - mape: 23.3782 - val_loss: 4.9701 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1611\n",
      "\n",
      "Epoch 00753: val_mse did not improve from 0.00170\n",
      "Epoch 754/2000\n",
      " - 3s - loss: 5.0188 - mse: 0.0104 - mae: 0.0724 - mape: 21.1957 - val_loss: 4.9700 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1979\n",
      "\n",
      "Epoch 00754: val_mse did not improve from 0.00170\n",
      "Epoch 755/2000\n",
      " - 3s - loss: 5.0147 - mse: 0.0091 - mae: 0.0683 - mape: 22.3712 - val_loss: 4.9700 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1902\n",
      "\n",
      "Epoch 00755: val_mse did not improve from 0.00170\n",
      "Epoch 756/2000\n",
      " - 3s - loss: 5.0155 - mse: 0.0095 - mae: 0.0690 - mape: 22.5412 - val_loss: 4.9700 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2032\n",
      "\n",
      "Epoch 00756: val_mse did not improve from 0.00170\n",
      "Epoch 757/2000\n",
      " - 3s - loss: 5.0181 - mse: 0.0099 - mae: 0.0717 - mape: 25.8129 - val_loss: 4.9700 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7564\n",
      "\n",
      "Epoch 00757: val_mse did not improve from 0.00170\n",
      "Epoch 758/2000\n",
      " - 3s - loss: 5.0192 - mse: 0.0108 - mae: 0.0728 - mape: 23.9770 - val_loss: 4.9700 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.7346\n",
      "\n",
      "Epoch 00758: val_mse did not improve from 0.00170\n",
      "Epoch 759/2000\n",
      " - 3s - loss: 5.0162 - mse: 0.0100 - mae: 0.0698 - mape: 20.6342 - val_loss: 4.9699 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1298\n",
      "\n",
      "Epoch 00759: val_mse did not improve from 0.00170\n",
      "Epoch 760/2000\n",
      " - 3s - loss: 5.0143 - mse: 0.0091 - mae: 0.0679 - mape: 19.5896 - val_loss: 4.9699 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2395\n",
      "\n",
      "Epoch 00760: val_mse did not improve from 0.00170\n",
      "Epoch 761/2000\n",
      " - 3s - loss: 5.0151 - mse: 0.0095 - mae: 0.0687 - mape: 21.9074 - val_loss: 4.9699 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2913\n",
      "\n",
      "Epoch 00761: val_mse did not improve from 0.00170\n",
      "Epoch 762/2000\n",
      " - 3s - loss: 5.0150 - mse: 0.0096 - mae: 0.0687 - mape: 21.5339 - val_loss: 4.9699 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3027\n",
      "\n",
      "Epoch 00762: val_mse did not improve from 0.00170\n",
      "Epoch 763/2000\n",
      " - 3s - loss: 5.0194 - mse: 0.0109 - mae: 0.0732 - mape: 23.2017 - val_loss: 4.9699 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8694\n",
      "\n",
      "Epoch 00763: val_mse did not improve from 0.00170\n",
      "Epoch 764/2000\n",
      " - 3s - loss: 5.0177 - mse: 0.0098 - mae: 0.0715 - mape: 26.8430 - val_loss: 4.9698 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2487\n",
      "\n",
      "Epoch 00764: val_mse did not improve from 0.00170\n",
      "Epoch 765/2000\n",
      " - 3s - loss: 5.0152 - mse: 0.0093 - mae: 0.0689 - mape: 23.8160 - val_loss: 4.9698 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2632\n",
      "\n",
      "Epoch 00765: val_mse did not improve from 0.00170\n",
      "Epoch 766/2000\n",
      " - 3s - loss: 5.0129 - mse: 0.0088 - mae: 0.0667 - mape: 23.9345 - val_loss: 4.9698 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3121\n",
      "\n",
      "Epoch 00766: val_mse did not improve from 0.00170\n",
      "Epoch 767/2000\n",
      " - 3s - loss: 5.0144 - mse: 0.0092 - mae: 0.0682 - mape: 20.1226 - val_loss: 4.9698 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3045\n",
      "\n",
      "Epoch 00767: val_mse did not improve from 0.00170\n",
      "Epoch 768/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0097 - mae: 0.0703 - mape: 22.5118 - val_loss: 4.9698 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0835\n",
      "\n",
      "Epoch 00768: val_mse did not improve from 0.00170\n",
      "Epoch 769/2000\n",
      " - 3s - loss: 5.0154 - mse: 0.0093 - mae: 0.0693 - mape: 22.5675 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3194\n",
      "\n",
      "Epoch 00769: val_mse did not improve from 0.00170\n",
      "Epoch 770/2000\n",
      " - 3s - loss: 5.0146 - mse: 0.0092 - mae: 0.0685 - mape: 21.7880 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0680\n",
      "\n",
      "Epoch 00770: val_mse did not improve from 0.00170\n",
      "Epoch 771/2000\n",
      " - 3s - loss: 5.0254 - mse: 0.0126 - mae: 0.0793 - mape: 26.6894 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.1105\n",
      "\n",
      "Epoch 00771: val_mse did not improve from 0.00170\n",
      "Epoch 772/2000\n",
      " - 3s - loss: 5.0204 - mse: 0.0111 - mae: 0.0743 - mape: 23.7815 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.2810\n",
      "\n",
      "Epoch 00772: val_mse did not improve from 0.00170\n",
      "Epoch 773/2000\n",
      " - 3s - loss: 5.0152 - mse: 0.0094 - mae: 0.0691 - mape: 21.0699 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.5649\n",
      "\n",
      "Epoch 00773: val_mse did not improve from 0.00170\n",
      "Epoch 774/2000\n",
      " - 3s - loss: 5.0155 - mse: 0.0096 - mae: 0.0694 - mape: 24.6645 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.3452\n",
      "\n",
      "Epoch 00774: val_mse did not improve from 0.00170\n",
      "Epoch 775/2000\n",
      " - 3s - loss: 5.0156 - mse: 0.0099 - mae: 0.0696 - mape: 21.2945 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4181\n",
      "\n",
      "Epoch 00775: val_mse did not improve from 0.00170\n",
      "Epoch 776/2000\n",
      " - 3s - loss: 5.0150 - mse: 0.0097 - mae: 0.0689 - mape: 20.6680 - val_loss: 4.9696 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.4198\n",
      "\n",
      "Epoch 00776: val_mse did not improve from 0.00170\n",
      "Epoch 777/2000\n",
      " - 3s - loss: 5.0164 - mse: 0.0098 - mae: 0.0704 - mape: 21.8589 - val_loss: 4.9697 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.3902\n",
      "\n",
      "Epoch 00777: val_mse did not improve from 0.00170\n",
      "Epoch 778/2000\n",
      " - 3s - loss: 5.0187 - mse: 0.0106 - mae: 0.0727 - mape: 24.7431 - val_loss: 4.9696 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 46.5923\n",
      "\n",
      "Epoch 00778: val_mse did not improve from 0.00170\n",
      "Epoch 779/2000\n",
      " - 3s - loss: 5.0148 - mse: 0.0096 - mae: 0.0689 - mape: 21.4617 - val_loss: 4.9696 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.5925\n",
      "\n",
      "Epoch 00779: val_mse did not improve from 0.00170\n",
      "Epoch 780/2000\n",
      " - 3s - loss: 5.0115 - mse: 0.0089 - mae: 0.0655 - mape: 21.0070 - val_loss: 4.9696 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 46.6848\n",
      "\n",
      "Epoch 00780: val_mse did not improve from 0.00170\n",
      "Epoch 781/2000\n",
      " - 3s - loss: 5.0172 - mse: 0.0100 - mae: 0.0713 - mape: 21.1719 - val_loss: 4.9696 - val_mse: 0.0018 - val_mae: 0.0237 - val_mape: 46.7021\n",
      "\n",
      "Epoch 00781: val_mse did not improve from 0.00170\n",
      "Epoch 782/2000\n",
      " - 3s - loss: 5.0194 - mse: 0.0105 - mae: 0.0735 - mape: 23.8424 - val_loss: 4.9696 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.5345\n",
      "\n",
      "Epoch 00782: val_mse did not improve from 0.00170\n",
      "Epoch 783/2000\n",
      " - 3s - loss: 5.0204 - mse: 0.0112 - mae: 0.0745 - mape: 24.7656 - val_loss: 4.9696 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1674\n",
      "\n",
      "Epoch 00783: val_mse did not improve from 0.00170\n",
      "Epoch 784/2000\n",
      " - 3s - loss: 5.0155 - mse: 0.0091 - mae: 0.0696 - mape: 21.8202 - val_loss: 4.9695 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1192\n",
      "\n",
      "Epoch 00784: val_mse did not improve from 0.00170\n",
      "Epoch 785/2000\n",
      " - 3s - loss: 5.0165 - mse: 0.0098 - mae: 0.0707 - mape: 25.1353 - val_loss: 4.9695 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00785: val_mse did not improve from 0.00170\n",
      "Epoch 786/2000\n",
      " - 3s - loss: 5.0173 - mse: 0.0104 - mae: 0.0715 - mape: 25.3929 - val_loss: 4.9694 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9355\n",
      "\n",
      "Epoch 00786: val_mse did not improve from 0.00170\n",
      "Epoch 787/2000\n",
      " - 3s - loss: 5.0139 - mse: 0.0094 - mae: 0.0681 - mape: 22.1061 - val_loss: 4.9694 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9723\n",
      "\n",
      "Epoch 00787: val_mse did not improve from 0.00170\n",
      "Epoch 788/2000\n",
      " - 3s - loss: 5.0186 - mse: 0.0109 - mae: 0.0728 - mape: 22.0773 - val_loss: 4.9694 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8341\n",
      "\n",
      "Epoch 00788: val_mse did not improve from 0.00170\n",
      "Epoch 789/2000\n",
      " - 3s - loss: 5.0134 - mse: 0.0089 - mae: 0.0676 - mape: 19.7564 - val_loss: 4.9694 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0318\n",
      "\n",
      "Epoch 00789: val_mse did not improve from 0.00170\n",
      "Epoch 790/2000\n",
      " - 3s - loss: 5.0166 - mse: 0.0097 - mae: 0.0708 - mape: 25.5047 - val_loss: 4.9693 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.9295\n",
      "\n",
      "Epoch 00790: val_mse did not improve from 0.00170\n",
      "Epoch 791/2000\n",
      " - 3s - loss: 5.0218 - mse: 0.0112 - mae: 0.0761 - mape: 23.5177 - val_loss: 4.9694 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8993\n",
      "\n",
      "Epoch 00791: val_mse did not improve from 0.00170\n",
      "Epoch 792/2000\n",
      " - 3s - loss: 5.0157 - mse: 0.0097 - mae: 0.0700 - mape: 21.8647 - val_loss: 4.9693 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 46.0899\n",
      "\n",
      "Epoch 00792: val_mse did not improve from 0.00170\n",
      "Epoch 793/2000\n",
      " - 3s - loss: 5.0180 - mse: 0.0102 - mae: 0.0723 - mape: 22.6406 - val_loss: 4.9693 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 46.1217\n",
      "\n",
      "Epoch 00793: val_mse did not improve from 0.00170\n",
      "Epoch 794/2000\n",
      " - 3s - loss: 5.0192 - mse: 0.0110 - mae: 0.0735 - mape: 21.7823 - val_loss: 4.9693 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.8222\n",
      "\n",
      "Epoch 00794: val_mse did not improve from 0.00170\n",
      "Epoch 795/2000\n",
      " - 3s - loss: 5.0152 - mse: 0.0098 - mae: 0.0696 - mape: 21.8818 - val_loss: 4.9693 - val_mse: 0.0017 - val_mae: 0.0237 - val_mape: 45.7152\n",
      "\n",
      "Epoch 00795: val_mse did not improve from 0.00170\n",
      "Epoch 796/2000\n",
      " - 3s - loss: 5.0180 - mse: 0.0107 - mae: 0.0724 - mape: 26.2167 - val_loss: 4.9693 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.6379\n",
      "\n",
      "Epoch 00796: val_mse did not improve from 0.00170\n",
      "Epoch 797/2000\n",
      " - 3s - loss: 5.0226 - mse: 0.0116 - mae: 0.0769 - mape: 24.2988 - val_loss: 4.9692 - val_mse: 0.0017 - val_mae: 0.0236 - val_mape: 45.8698\n",
      "\n",
      "Epoch 00797: val_mse did not improve from 0.00170\n",
      "1536/1536 [==============================] - 1s 736us/step\n",
      "val_loss:  5.129209995269775  val_mse:  0.0016797560965642333  val_mae:  0.023405471816658974  val_mape:  43.79740905761719\n",
      "dict_keys(['val_loss', 'val_mse', 'val_mae', 'val_mape', 'loss', 'mse', 'mae', 'mape', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\connection.py\", line 160, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\util\\connection.py\", line 61, in create_connection\n",
      "    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\socket.py\", line 752, in getaddrinfo\n",
      "    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n",
      "socket.gaierror: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\connectionpool.py\", line 677, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\connectionpool.py\", line 381, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\connectionpool.py\", line 978, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\connection.py\", line 309, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\connection.py\", line 172, in _new_conn\n",
      "    self, \"Failed to establish a new connection: %s\" % e\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x0000024516711B48>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\requests\\adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\connectionpool.py\", line 727, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\urllib3\\util\\retry.py\", line 439, in increment\n",
      "    raise MaxRetryError(_pool, url, error or ResponseError(cause))\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='query1.finance.yahoo.com', port=443): Max retries exceeded with url: /v8/finance/chart/%5EGSPC?period1=1587009600&period2=1587960000&interval=1d&includePrePost=False&events=div%2Csplits (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000024516711B48>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\multitasking\\__init__.py\", line 102, in _run_via_pool\n",
      "    return callee(*args, **kwargs)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\yfinance\\multi.py\", line 167, in _download_one_threaded\n",
      "    actions, period, interval, prepost, proxy, rounding)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\yfinance\\multi.py\", line 182, in _download_one\n",
      "    rounding=rounding, many=True)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\yfinance\\base.py\", line 150, in history\n",
      "    data = _requests.get(url=url, params=params, proxies=proxy)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\requests\\api.py\", line 76, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\requests\\api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\requests\\sessions.py\", line 530, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\requests\\sessions.py\", line 643, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\kenneth\\Anaconda3\\envs\\tf115\\lib\\site-packages\\requests\\adapters.py\", line 516, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: HTTPSConnectionPool(host='query1.finance.yahoo.com', port=443): Max retries exceeded with url: /v8/finance/chart/%5EGSPC?period1=1587009600&period2=1587960000&interval=1d&includePrePost=False&events=div%2Csplits (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000024516711B48>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if best_model_search:\n",
    "    if __name__ == '__main__':\n",
    "       \n",
    "        best_run, best_model = optim.minimize(model=create_cudnnlstm_model, data=data,functions = [data_split, data_params, feature_selection],algo=tpe.suggest,\n",
    "                                              max_evals=max_evals,trials=Trials(), notebook_name=model_name,rseed=random_state, eval_space=True, verbose=False)\n",
    "        \n",
    "               \n",
    "        \n",
    "        print(\"Evaluation of best performing model:\")\n",
    "        build_path('best_model')\n",
    "        best_model.save('best_model/'+model_name+'.h5')\n",
    "        print('\\n'+'*'*20+\"Best Model Configuration:\"+'*'*20)\n",
    "        print(best_model.get_config())\n",
    "        f = open('best_model/'+model_name+'_best_run.pickle', 'wb')# save best_run parameters in a pickle file\n",
    "        pickle.dump(best_run, f)\n",
    "        f.close()\n",
    "        \n",
    "        kfold, train_size, shuffle, split, fold_number,random_state = data_split()\n",
    "        batch_size, time_steps, rm_window, hyperas_epochs, forecast_period, x_scaler, y_scaler = data_params()           \n",
    "        x_train, y_train, x_test, y_test, batch_size, time_steps, x_predict, last_date, ticker, x_scaler, y_scaler = data()\n",
    "\n",
    "        val_loss, val_mse, val_mae, val_mape = best_model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "        print('val_loss: ', val_loss, ' val_mse: ',val_mse, ' val_mae: ',val_mae,  ' val_mape: ',val_mape)\n",
    "        print('\\n'+'*'*20+\"Best Run hyper-parameters:\"+'*'*20)\n",
    "        print(best_run)\n",
    "        del best_model, best_run, x_train, x_test, y_train, y_test, x_predict\n",
    "        gc.collect()\n",
    "        \n",
    "\n",
    "if  perform_training:\n",
    "    kfold, train_size, shuffle, split, fold_number,random_state = data_split()\n",
    "    batch_size, time_steps, rm_window, hyperas_epochs, forecast_period, x_scaler, y_scaler = data_params()           \n",
    "    x_train, y_train, x_test, y_test, batch_size, time_steps, x_predict, last_date, ticker, x_scaler, y_scaler = data()\n",
    "    training()\n",
    "    del x_train, x_test, y_train, y_test, x_predict\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('train_model/'+model_name+'.h5'):\n",
    "    print('Trained model doesnt exist, changing evaluate_all_folds to False')\n",
    "    evaluate_all_folds = False\n",
    "if evaluate_all_folds:\n",
    "    kfold, train_size, shuffle, split, fold_number,random_state = data_split()\n",
    "    batch_size, time_steps, rm_window, hyperas_epochs, forecast_period, x_scaler,y_scaler = data_params()\n",
    "    df, y, ticker = feature_selection()\n",
    "    model = load_model('train_model/'+model_name+'.h5')\n",
    "    print('loaded trained model')\n",
    "    \n",
    "    # add percent change\n",
    "    #df=df.pct_change()\n",
    "    #df=df.replace([np.inf, -np.inf],np.nan) \n",
    "    #df.fillna(0, inplace=True)\n",
    "    #df.isnull().any().mean()\n",
    "    y=y.shift(-forecast_period)\n",
    "    \n",
    "    # apply preprocessing \n",
    "    x = x_scaler.fit_transform(df)\n",
    "    y = y_scaler.fit_transform(y)\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    # apply time steps\n",
    "    def create_dataset(X, y, time_steps=1):\n",
    "        Xs, ys = [], []\n",
    "        for i in range(len(X) - time_steps):\n",
    "            v = X[i:(i + time_steps)]\n",
    "            Xs.append(v)\n",
    "            ys.append(y[i + time_steps])\n",
    "        return np.array(Xs), np.array(ys)\n",
    "    x, y = create_dataset(x, y, time_steps)\n",
    "    # forecast periods and removing last7 rows (for the target being NaN for 7 day model)\n",
    "    x = x[:-forecast_period]\n",
    "    y =  y[:-forecast_period]\n",
    "\n",
    "    #implementation of fold\n",
    "    print('started Kfold')\n",
    "\n",
    "    count = 0\n",
    "    fd = KFold(n_splits=split, random_state=random_state, shuffle=shuffle)\n",
    "\n",
    "    for train_index, test_index in fd.split(x):\n",
    "        count +=1\n",
    "        print('processing fold number ', count)\n",
    "\n",
    "        _, x_test,_, y_test  = x[train_index], x[test_index], y[train_index], y[test_index]\n",
    "        x_test = x_test.astype('float32')\n",
    "        y_test = y_test.astype('float32')\n",
    "    \n",
    "        # adjustment for batch_size\n",
    "        test_start = x_test.shape[0]%batch_size\n",
    "        x_test = x_test[test_start:]\n",
    "        y_test = y_test[test_start:]\n",
    "        val_loss, val_mse, val_mae, val_mape = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "        print('Fold Number ', count,'val_loss: ', val_loss, ' val_mse: ',val_mse, '\\n',' val_mae: ',val_mae,  ' val_mape: ',val_mape)\n",
    "        del x_test, y_test\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "GSPC_7days_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf115]",
   "language": "python",
   "name": "conda-env-tf115-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
